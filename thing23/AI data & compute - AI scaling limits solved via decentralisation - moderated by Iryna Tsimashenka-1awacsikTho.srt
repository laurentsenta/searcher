1
00:00:00,000 --> 00:00:12,800
So, hello everyone. Now, this is our last talk slash panel. And the subject is AI data


2
00:00:12,800 --> 00:00:23,100
and compute, AI scaling limits of AI decentralization. So, I want to welcome our guests, our panelists.


3
00:00:23,100 --> 00:00:37,100
Wes from Bakayau, Don from Nevermind, Zeeshan from Fusion, and Hari from Genesis AI. So, do you mind to quickly introduce yourself?


4
00:00:37,100 --> 00:00:45,600
Hey there, my name is Wes Floyd, a member of the Bakayau team, also helping run the compute over data working group at cod.cloud.


5
00:00:45,600 --> 00:00:54,100
And been in the space, crypto space for a few years, was working in enterprise before that, and really glad to participate in the panel today.


6
00:00:54,100 --> 00:01:07,600
Alright, Don Gossin, I've been in crypto for a while now. I co-founded a project called Ocean Protocol, where we created compute-to data.


7
00:01:07,600 --> 00:01:15,600
Yeah, totally different. More recently, I've spent the last three and a half years working on a project called Nevermind.


8
00:01:15,600 --> 00:01:33,100
So, we're quite data-centric, therefore we've got to, we skew towards the data side of things, but we have this thesis that the future of AI will depend on its access to data.


9
00:01:33,100 --> 00:01:47,100
So, we've created tools that enable AIs to discover and acquire data, as well as for the data publishers to control set access and enforce compensation.


10
00:01:47,100 --> 00:01:55,100
Hello, I'm Zeeshan. Yeah, I spoke, I'm at Fission, working on Homestar and IPVM. Homestar is our implementation of IPVM.


11
00:01:55,100 --> 00:02:04,100
I'm a staff researcher there, doing that among other things, and also one of the founders, organizers of Papers We Love.


12
00:02:04,100 --> 00:02:17,100
And so, obviously a lot of thoughts around data, compute, AI. Back in the day, I used to do, kind of before the deep learning boom, I did music recommendation systems with the old machine learning ways.


13
00:02:17,100 --> 00:02:22,100
But yeah.


14
00:02:22,100 --> 00:02:26,100
Harry, could you please introduce yourself quickly?


15
00:02:26,100 --> 00:02:42,100
Sure. I'm Harry Greave, co-founder of Jensen. We're a deep learning compute protocol. So we enable deep learning developers to train enormous scale neural networks across ultra low cost GPUs.


16
00:02:42,100 --> 00:03:03,100
Our kind of thesis is that the key bottleneck in the space is access to compute. It's something that I felt when I was leading a research team in an industry, essentially in the reinsurance catastrophe prediction marketplace, where we use more statistical machine learning methods to estimate risk.


17
00:03:03,100 --> 00:03:06,100
Happy to be here.


18
00:03:06,100 --> 00:03:18,100
So let's start with the first question. And question to us, what are real world applications of decentralized AI and how they differ from centralized AI system?


19
00:03:18,100 --> 00:03:28,100
Thank you. Yes, I think, fortunately, there's a lot of emerging use cases. I'll cover a handful, and then I know there's other folks in the panel that have a much more broad perspective.


20
00:03:28,100 --> 00:03:42,100
Two in particular that I think have a lot of potential. One is on the decentralized finance side. There's limitations in the way that smart contracts operate today where it's difficult to do large scale machine learning training or inference.


21
00:03:42,100 --> 00:03:59,100
And so if you could improve the machine learning you could do through a smart contract, you could look at more complicated use cases like hedging risk on complicated financial instruments. You could look at sort of derivatives contracts and sort of explore more sophisticated allocations of capital and defy.


22
00:03:59,100 --> 00:04:17,100
Also in the emerging decentralized social media space, I think this whole space could be good for humanity, just providing an alternative to the big tech social media space. If I have a nice new UI and it's blue sky or it's lens and I'm scrolling through my app, what content should appear there?


23
00:04:17,100 --> 00:04:31,100
Should it be a content based on an algorithm that I wrote or a content based on an algorithm that you wrote? Or how is it going to get fed? Because now we're sort of breaking away from the big tech monopoly on algorithms and recommendation.


24
00:04:31,100 --> 00:04:37,100
So I think there's a lot of opportunity for machine learning there as well, and looking forward to seeing the space as it grows.


25
00:04:37,100 --> 00:04:53,100
Nice. So a little variation on that, whether we're talking about like decentralized ML or AI or just the application of decentralized technology to those applications.


26
00:04:53,100 --> 00:05:11,100
I think I'm going to scoot more towards the latter for right now. So I think there's a real need, a real drive for proper attribution. You see that in particular with like text to image creation that's taking place right now.


27
00:05:11,100 --> 00:05:31,100
So I type in some words, I get a nice image out of stable diffusion. What is that based on? In all likelihood, it's based on images that were scraped, sourced from the public internet, but there was no consent provisioned from the publisher.


28
00:05:31,100 --> 00:05:45,100
So the question becomes, can you trace and track that source material from start to finish? And then with a blockchain, you've got an innate payment gateway.


29
00:05:45,100 --> 00:05:54,100
You could then potentially, if you can do that track and trace, prescribe proper attribution, compensation, royalties, residuals back to that publisher.


30
00:05:54,100 --> 00:06:08,100
So the specific use case that comes to mind right now is Getty Images suing Stability AI for copyright infringement. Can you avoid that problem set entirely?


31
00:06:08,100 --> 00:06:21,100
Can you incent a Getty Images to actually, I guess, modify or add to their business models and leverage this technology to do so?


32
00:06:21,100 --> 00:06:28,100
Cool. Yeah, actually, we'll add to that. I think at Fission, we do a lot of work obviously with UCANs and capabilities and authorization delegation.


33
00:06:28,100 --> 00:06:35,100
And we also built that into like WinFS, which is a concept of like having volumes that can be in directories that are private versus public.


34
00:06:35,100 --> 00:06:43,100
So knowing which data you're okay with sharing with these systems that build models versus not, I think is actually really crucial.


35
00:06:43,100 --> 00:06:50,100
So that's actually a huge one. And I think decentralization really lends toward data that can be private and things that you don't want to share.


36
00:06:50,100 --> 00:06:57,100
I mean, a lot of companies now, even with ChatGPT, I have friends at large companies who are basically getting notices to say,


37
00:06:57,100 --> 00:07:03,100
please don't put in any of our non-open source code into ChatGPT.


38
00:07:03,100 --> 00:07:08,100
We'll separately buy a license with OpenAI where they have a separate API for that kind of stuff.


39
00:07:08,100 --> 00:07:17,100
So the ability to kind of carve out what's for public and not public use within these models, especially because IP could come around here,


40
00:07:17,100 --> 00:07:22,100
I think is a big proponent for decentralization.


41
00:07:25,100 --> 00:07:28,100
Hari, do you want to contribute?


42
00:07:28,100 --> 00:07:35,100
Yeah. So very quickly from our side, the main kind of thesis we have for decentralization,


43
00:07:35,100 --> 00:07:40,100
helping of scaling is around the economics, primarily kind of two areas.


44
00:07:40,100 --> 00:07:47,100
So the first is by being a layer one or by creating layer ones, you remove the kind of large margins,


45
00:07:47,100 --> 00:07:52,100
which are placed on the entire system by the kind of current cloud oligopolists.


46
00:07:52,100 --> 00:07:57,100
So, for example, the computers, a 63 percent margin with EC2,


47
00:07:57,100 --> 00:08:04,100
and a crypto economic protocols don't need to charge margins because the value accrues to the token as opposed to being extracted by a margin.


48
00:08:04,100 --> 00:08:09,100
So you immediately kind of depress the prices, which increases the scale you can access.


49
00:08:09,100 --> 00:08:15,100
But on top of that, you can also kind of pull together globally all the resources,


50
00:08:15,100 --> 00:08:23,100
which are relevant for machine learning at various places in the kind of stack, be it on the kind of data side or more on the compute side.


51
00:08:23,100 --> 00:08:26,100
And that increase in supply also depresses the price.


52
00:08:26,100 --> 00:08:31,100
So as opposed to paying the kind of exorbitant prices you see in the EC2 instances,


53
00:08:31,100 --> 00:08:38,100
you can actually get a much kind of lower unit cost of compute, which means that more people can train large language models.


54
00:08:38,100 --> 00:08:43,100
Yeah, so and this is coming to my second question,


55
00:08:43,100 --> 00:08:52,100
how we can overcome these technical regulatory challenges of scaling decentralized AI?


56
00:08:52,100 --> 00:08:59,100
You know, what's interesting is a lot of the popular machine learning frameworks that we use today rely on centralized training.


57
00:08:59,100 --> 00:09:06,100
They expect that not only the hardware, but also the algorithms expect the data is close to the other data sets in the same data center.


58
00:09:06,100 --> 00:09:11,100
And most existing infrastructures in the cloud or centralized systems, they've been built around that.


59
00:09:11,100 --> 00:09:22,100
But there's a lot of good dialogue going on in the decentralized AI community about rebuilding the algorithms to operate in a more federated approach so that there is latency involved.


60
00:09:22,100 --> 00:09:27,100
But if we have plenty of scale and obviously like, you know, with Ethereum miners, we know that we can source scale.


61
00:09:27,100 --> 00:09:29,100
There are people in the world that want to contribute hardware.


62
00:09:29,100 --> 00:09:36,100
If you take a different paradigm, a different approach to the hardware, and you rethink the algorithms to a certain extent,


63
00:09:36,100 --> 00:09:39,100
you can really achieve some interesting decentralized outcomes.


64
00:09:39,100 --> 00:09:46,100
There's a project that we're working on with the Bacquiao team right now around a component called insulated jobs where we want to do federated machine learning.


65
00:09:46,100 --> 00:09:51,100
And so we're looking at projects like Flowers, which you brought up. It's a really good job.


66
00:09:51,100 --> 00:09:54,100
Bloom, pedals.ml is another good example.


67
00:09:54,100 --> 00:09:58,100
FedML.ai, flock.io is another good group in this space.


68
00:09:58,100 --> 00:10:07,100
So I think as this demand for these use cases grow, you'll see hardware and algorithms all kind of come together to make it more possible.


69
00:10:07,100 --> 00:10:12,100
Just to plug, nevermind. We had the Flower framework integrated like two and a half years ago.


70
00:10:12,100 --> 00:10:18,100
But my drop.


71
00:10:18,100 --> 00:10:22,100
Why not? I'm up here. Excuse me.


72
00:10:22,100 --> 00:10:34,100
I think education, but it goes without saying, I think what we really need to work towards is consent and in particular informed consent.


73
00:10:34,100 --> 00:10:37,100
And how can that informed consent actually manifest?


74
00:10:37,100 --> 00:10:44,100
I think what we're talking about from our point of view, blockchain also serves a purpose there.


75
00:10:44,100 --> 00:10:51,100
It's a global database where you can issue or burn informed consent.


76
00:10:51,100 --> 00:10:57,100
So you can actually distinguish what should or shouldn't be available in the context that you are discussing.


77
00:10:57,100 --> 00:11:12,100
And so from a regulatory point of view, if you can designate what's available versus what isn't, I think you can mitigate a lot of the risk that goes along with that.


78
00:11:12,100 --> 00:11:17,100
For me, regulation is kind of a manifestation of a lack of control.


79
00:11:17,100 --> 00:11:22,100
So how do you bring control to the publisher and also the user?


80
00:11:22,100 --> 00:11:25,100
Because it's kind of incumbent on both sides.


81
00:11:25,100 --> 00:11:30,100
I want to know what I can use and I also want to inform the ecosystem what they're allowed to use.


82
00:11:30,100 --> 00:11:35,100
So working on informed consent will go a long way.


83
00:11:35,100 --> 00:11:40,100
Yeah, I think to the point on scale, I think things...


84
00:11:40,100 --> 00:11:53,100
I look a lot back when I was working on networking stuff at Comcast, a lot of IoT edge device places we had compute at the home and compute at the hubs in between all the way to the centralized services.


85
00:11:53,100 --> 00:12:01,100
So the way I think of it is not so much as decentralization versus centralization, but thinking about what are the...


86
00:12:01,100 --> 00:12:05,100
There's actually tiers of where you want to do compute.


87
00:12:05,100 --> 00:12:12,100
Maybe you have to do training models at the large scale machines and you run those on your centralized cloud providers.


88
00:12:12,100 --> 00:12:20,100
But within a compute context, you know that certain parts like applying information from a model can happen on your device at your home.


89
00:12:20,100 --> 00:12:33,100
And then using the stuff we're working on with IPVM, which is also UCAN based, like what do I want to control within my set of devices versus what I have to go get or what I have to go compute on something at a different tier.


90
00:12:33,100 --> 00:12:35,100
So I think that's one part of it.


91
00:12:35,100 --> 00:12:54,100
The other one that we're super exploring right now to a lot of what Wes was saying was how do you do matchmaking to figure out, again, looking at tiers again, how do you have a set of work that you want to do and matchmake with who should run that based on locality, based on the kind of job.


92
00:12:54,100 --> 00:13:16,100
One thing that we're building into our implementation is bootstrapping nodes with information shared between what's your GPU, what's your CPU, all the process information that you have, using traces over time with receipts to actually look back over time and say these players haven't been as good or haven't been able to process to certain SLAs that you want to match.


93
00:13:16,100 --> 00:13:25,100
So using history and traceability also comes a long way to kind of build toward that scale that we all want.


94
00:13:25,100 --> 00:13:28,100
Haru, do you want to add?


95
00:13:28,100 --> 00:13:47,100
Yeah, I would just highlight that a lot of the kind of centralized training of the larger models that we've seen have been built with that paradigm as the only option. I think up until the kind of involvement of the kind of crypto networks we're talking about just now, there haven't really been alternatives.


96
00:13:47,100 --> 00:13:58,100
You've had to build these large clusters, they've had to be centralized, and they've typically kind of used a few types of reference hardware, notably like the A100s.


97
00:13:58,100 --> 00:14:09,100
What's interesting from our perspective is if you were to give AI researchers the option to actually use a kind of higher scale and cheaper alternative, it's a bit of a chicken and egg problem.


98
00:14:09,100 --> 00:14:27,100
It would give them the ability to actually design systems which didn't rely on decentralized clusters. I think that's just a kind of chicken and egg point that there's another option, but we just haven't seen it because there hasn't been the opportunity to do it yet.


99
00:14:27,100 --> 00:14:41,100
Yeah, so there are several times that you mentioned blockchain technology. Can you please tell me benefits and challenges of using blockchain technology for AI, data, and compute? Haru, do you want to start?


100
00:14:41,100 --> 00:15:05,100
Yeah, in terms of challenges around blockchain, I think the number one we think about is kind of around verification. So if you want to use decentralized technologies, if you want to be permissionless, you essentially have to verify what is exceptionally computationally expensive work in a way that is


101
00:15:05,100 --> 00:15:27,100
A, low overhead, and B, resistant to scaling problems. I think a lot of the kind of issues that we've seen in the space so far kind of come from people building systems that either don't scale well, so they rely on reputation systems, or they do scale, but the kind of cost to making the work verified is extremely expensive.


102
00:15:27,100 --> 00:15:40,100
So just a good example, the expensive part would be, if you wanted to train a neural network on Ethereum, like on chain, it would be completely cost prohibitive due to the kind of the way in which the verifications run.


103
00:15:40,100 --> 00:16:00,100
So, you know, almost the entirety of the work we've done so far at Jensen is focusing on how do you get that verification overhead as minimal as possible, whilst allowing that scale to essentially be uncapped. That's the kind of main kind of challenge that we've seen in the space of applying these technologies.


104
00:16:00,100 --> 00:16:13,100
Well said. I think I'll save some time for the other folks. I know you guys are very much involved in some of the challenges. I think the biggest benefit is just the opportunity to both democratize and also to reduce friction in terms of the economic components.


105
00:16:13,100 --> 00:16:29,100
I mean, we were just hearing about how Nevermind is focusing on enabling accreditation or credit given to the original data sources, and I think the ability to then pay folks and create those marketplaces is going to be really valuable.


106
00:16:29,100 --> 00:16:54,100
So, challenges, overcoming, broadly speaking, the mental gap of why you need it. You know, there's a lot of education that goes into this stuff, which is one of the reasons why in particular, you know, as opposed to going someplace that looks like it would be a great fit, like enterprise, the time to market there is just exorbitant.


107
00:16:54,100 --> 00:17:15,100
So, focus on where people understand it, and you know, you've got some control on that aspect. And I think, you know, the primary benefit from our point of view is attribution, right? But there's a lot that gets coupled to that.


108
00:17:15,100 --> 00:17:40,100
Again, regulation, and how you manage the control of that information, how you navigate different data policies, GDPR, CCPA, HIPAA, etc. And then how those manifest across likely a number of different blockchains, smart contract or not smart contract enabled.


109
00:17:40,100 --> 00:17:58,100
So, you know, there's a bunch of different permutations that and because this environment is, you know, that there's so much activity going on in the space, you know, that's also a hindrance to overall development.


110
00:17:58,100 --> 00:18:08,100
Because, you know, there's a lot of different choices, a lot of different distractions. So, you know, it's also maintaining focus. Anyway, that's a few.


111
00:18:08,100 --> 00:18:30,100
Yeah, I would say, I mean, one thing that, you know, we're also looking at and have been thinking about around this idea of delegation authorization in regards to the chain is, or in relation to changes, and Brooke actually was bringing this up in her talk that's happening like 10 minutes ago, whatever it was, I think fissions kicked around for a while called you can't, you can't pay payment channels.


112
00:18:30,100 --> 00:18:39,100
So also using the same kind of things that we have capabilities and being like, are you able to run this? Or can you run this and then tying that to attaching that to payments?


113
00:18:39,100 --> 00:19:01,100
I think another kind of positive note to this too, and why we're putting a lot of stock into wasm, of course, is that it comes essentially with metering. So you can actually gauge and and tie in, you know, cost with fuel that's run by a certain set of execution. That's much harder to do in other compute systems, right?


114
00:19:01,100 --> 00:19:17,100
Where you have to just attribute payment to some, some understanding of what's being used. But wasm, you can actually measure that using instructions. So I think there's a lot of good tie ins with all the stuff that we're moving toward and the tools that we have to find better ways to incorporate payment tokens.


115
00:19:17,100 --> 00:19:37,100
And of course, to something you mentioned, I think in the previous question, you know, using in a compute model like that we're looking at, there's also a lot of good examples of how, hey, certain parties don't want to participate or are not participating well in the network, you can kick them out, kind of fire and burn them, as you think you mentioned.


116
00:19:37,100 --> 00:19:53,100
And I think that's, you know, a thing that to do kind of global massive compute this way, that's going to be a major proponent, these, you know, that you don't want, you don't want bad actors, right? So you have to have ways to remove them.


117
00:19:53,100 --> 00:20:13,100
So how can decentralization improve the scalability of AI systems and overcome limitations?


118
00:20:00,000 --> 00:20:03,000
of centralized data storage and computation?


119
00:20:03,000 --> 00:20:04,000
Why don't you start?


120
00:20:04,000 --> 00:20:05,000
Yeah, I'd be happy to.


121
00:20:05,000 --> 00:20:14,000
Yeah, one other thing that I was just thinking about that we didn't mention is particularly decentralization is this issue of encrypted data sets and private data sets.


122
00:20:14,000 --> 00:20:23,000
Because obviously, if you're, you know, training on your own company's data, you have it locked in your own slice of your public cloud, and it doesn't become an issue.


123
00:20:23,000 --> 00:20:29,000
But I think when you go to decentralized solutions, there's folks like Ocean Protocol, who have been working on this for a long time,


124
00:20:29,000 --> 00:20:34,000
who are, you know, who are thinking about smart approaches to how do I maintain my private data set?


125
00:20:34,000 --> 00:20:37,000
How do I encrypt it? And then how do I sort of share the economic benefits?


126
00:20:37,000 --> 00:20:41,000
And there's a number of really good projects in the space like Lit Protocol.


127
00:20:41,000 --> 00:20:46,000
Also, the folks at nCloud, encloud, are doing a lot of work in this space.


128
00:20:46,000 --> 00:20:54,000
So that, you know, once you can say, OK, I have a health care data set that's relevant to my specific group, maybe I'm a nonprofit or research organization,


129
00:20:54,000 --> 00:21:02,000
then maybe you have some other data sets or folks want to sell their own personal health data or whatever purposes and then get, you know, remuneration for that.


130
00:21:02,000 --> 00:21:08,000
If you can still maintain privacy and have some level of compute over that private data or do some smart things with key management,


131
00:21:08,000 --> 00:21:11,000
then you open up things that you really could not do in a centralized system,


132
00:21:11,000 --> 00:21:16,000
because it's very difficult to get millions of people to grant HIPAA authorization over all their private data sets.


133
00:21:16,000 --> 00:21:19,000
But in this, you know, sort of crypto centric world, you can do it.


134
00:21:19,000 --> 00:21:23,000
So hopefully, with encrypted data, we'll get some other good applications.


135
00:21:23,000 --> 00:21:28,000
Yeah, I mean, obviously, that's a big part of the fission stuff as well.


136
00:21:28,000 --> 00:21:36,000
Encryption at rest, encryption on flight, you know, with dreams of grandeur toward homomorphic encryption and being able to compute there,


137
00:21:36,000 --> 00:21:40,000
which, you know, it gets closer and closer, even though it seems far away.


138
00:21:40,000 --> 00:21:47,000
And I think, yeah, I think actually to go on that, I think that that's the super really important part of it.


139
00:21:47,000 --> 00:21:54,000
You know, one thing we've even talked about when it comes to this idea of affinities and capabilities, well, having the ability to,


140
00:21:54,000 --> 00:21:59,000
if you have certain work that has to be signed, and you know, that's that you have to have that key.


141
00:21:59,000 --> 00:22:01,000
That is a capability that you have for certain kinds of work.


142
00:22:01,000 --> 00:22:05,000
If you don't have that capability, you know, you can't run that transaction.


143
00:22:05,000 --> 00:22:08,000
Right. So you know how to filter these kind of things out. Right.


144
00:22:08,000 --> 00:22:16,000
So obviously, baking, baking security into your point, Wes, I think is like a huge kind of boon for what we want to try to do.


145
00:22:16,000 --> 00:22:30,000
So just to add to both of these, I think, and you said it quite well, I think there's going to be this like blending of decentralized and centralized technology.


146
00:22:30,000 --> 00:22:43,000
So for the foreseeable future, the development of like LLM will probably be centralized because they take up ship piles of bandwidth and compute resources and all this stuff.


147
00:22:43,000 --> 00:22:52,000
However, transfer learning or augmented retrieval modeling, you know, that could really benefit across the board from decentralization,


148
00:22:52,000 --> 00:23:03,000
especially if you have privacy preservation at play, because now what you can do is get more fine-tuned inferences,


149
00:23:03,000 --> 00:23:16,000
but base that off of potentially multiple counterparties training that transfer learning model independently in a federated fashion across their data sets,


150
00:23:16,000 --> 00:23:19,000
but the whole or the collective getting the benefit of that.


151
00:23:19,000 --> 00:23:22,000
So that's something that's potentially very powerful.


152
00:23:22,000 --> 00:23:36,000
And then obviously, tagging on all the economic components to this and being able to like incentivize multiple counterparties to perform these types of activities is just, it's a superpower.


153
00:23:36,000 --> 00:23:38,000
Perfect. Thank you.


154
00:23:38,000 --> 00:23:47,000
So how can decentralized machine learning techniques can improve efficiency and accuracy of AI models? Who wants to start?


155
00:23:47,000 --> 00:23:54,000
Harry, do you want to start?


156
00:23:54,000 --> 00:24:02,000
Yeah, sure. I think our kind of house view is that, you know, more is better.


157
00:24:02,000 --> 00:24:07,000
So the more kind of scale you can apply, broadly speaking, the better results you get.


158
00:24:07,000 --> 00:24:15,000
The scaling laws have held relatively well over the kind of recent iterations of large models.


159
00:24:15,000 --> 00:24:22,000
There's kind of chinchilla scaling laws about whether or not you scale the parameters of the data, but broadly speaking, more is better.


160
00:24:22,000 --> 00:24:28,000
It's quite clear that, you know, the model, you know, that results in better model accuracy of various tasks.


161
00:24:28,000 --> 00:24:35,000
We saw that with like the GPT-4 increase in various kind of like academic benchmarks.


162
00:24:35,000 --> 00:24:46,000
When I kind of think like longer term, well, how do you kind of get accuracy improved further? It basically looks like hooking up more and more and more compute.


163
00:24:46,000 --> 00:24:52,000
And there reaches a point with centralized clusters, at least, where you just physically can't connect more.


164
00:24:52,000 --> 00:25:00,000
You know, if you think about like a kind of a physical building where you're hosting A100s, etc.


165
00:25:00,000 --> 00:25:05,000
You reach a point where you just can't fit anyone in the building.


166
00:25:05,000 --> 00:25:09,000
Or there's just sort of just electricity limits locally.


167
00:25:09,000 --> 00:25:19,000
Long term, medium term, having a global cluster, a universal cluster that you can access solves that problem and allows you to build ever larger models.


168
00:25:19,000 --> 00:25:27,000
So I think the benefit, the accuracy improvements at least comes from the scale.


169
00:25:27,000 --> 00:25:36,000
I think one interesting scenario that might emerge, especially, you know, with FEM launch, a lot of people are talking about data DAOs like Ocean Protocol and other Lagrange and some other folks.


170
00:25:36,000 --> 00:25:43,000
If you could have a hybrid scenario, there's a lot of work in this group at Stanford called the Center for Foundational Model Research.


171
00:25:43,000 --> 00:25:47,000
And they basically build these massive models that are trained on large data sets.


172
00:25:47,000 --> 00:25:49,000
And they're generic enough, but they're still effective.


173
00:25:49,000 --> 00:25:50,000
They're foundational model.


174
00:25:50,000 --> 00:25:55,000
If you could have a private company offer a foundational model that had the resources of centralized compute.


175
00:25:55,000 --> 00:26:01,000
But then you could tap into the unique data sets that people would buy and sell over decentralized systems and have the best of both.


176
00:26:01,000 --> 00:26:04,000
I think that would be a very interesting kind of combination.


177
00:26:04,000 --> 00:26:08,000
If you could have a DAO that was actually paying a private company to use its foundational model.


178
00:26:08,000 --> 00:26:15,000
And the DAO trained its data set on specialized things that it had access to, like weather data or whatever proprietary data set it's at.


179
00:26:15,000 --> 00:26:20,000
That would be kind of a fun and interesting way to go beyond the current status quo.


180
00:26:20,000 --> 00:26:28,000
So if you tie scale to relevance, basically we're looking at a state.


181
00:26:28,000 --> 00:26:29,000
I mean, there's a couple.


182
00:26:29,000 --> 00:26:32,000
Well, there's more than a couple, but there's a few ways of thinking about this.


183
00:26:32,000 --> 00:26:37,000
One is you can work on the parameters of the model.


184
00:26:37,000 --> 00:26:40,000
It requires a lot of knowledge and expertise.


185
00:26:40,000 --> 00:26:47,000
Alternatively, what's going on right now, you're just throwing shape hulls of information at the model and training it that way.


186
00:26:47,000 --> 00:26:59,000
If you align with that school of thought, there is a wall that you hit with the amount of data that's freely available from the public Internet.


187
00:26:59,000 --> 00:27:09,000
And what ultimately happens with models that are trained in these conditions is they basically converge to the same responses.


188
00:27:09,000 --> 00:27:11,000
So sure, that's scalable.


189
00:27:11,000 --> 00:27:15,000
But from a model point of view, from an AI relevance point of view, that's not really scalable.


190
00:27:15,000 --> 00:27:20,000
If Wes and I come up with the same response, who's better than the other?


191
00:27:20,000 --> 00:27:24,000
It's not really scalable from a business point of view anyway.


192
00:27:24,000 --> 00:27:39,000
So now the question becomes, okay, if scale is attached to relevance, then we need to access information that isn't necessarily publicly available or requires consent in order to avail itself to the model.


193
00:27:39,000 --> 00:27:51,000
So I think enabling that access is paramount and working towards that type of future.


194
00:27:51,000 --> 00:28:02,000
Yeah, I mean, I'll head on, I think, going back again to that multi-tier term, which I think is a really legit thing that a lot of people are working on, particularly in academia as well.


195
00:28:02,000 --> 00:28:13,000
But the ability to have it, to the point, you need more scale, but knowing that you can filter information that's not important for a model locally.


196
00:28:13,000 --> 00:28:21,000
Or when I began back at Comcast, we used to have how voice remote worked, which was really well done.


197
00:28:21,000 --> 00:28:25,000
And so obviously a lot of the modeling was built in lots of data centers, centralized servers.


198
00:28:25,000 --> 00:28:30,000
But the hardware on the set box was getting better and better and better.


199
00:28:30,000 --> 00:28:33,000
It could run WASM, it could run these kinds of things.


200
00:28:33,000 --> 00:28:43,000
And so you could do some small scales of compute for localized data there that then can be deemed important for the more aggregate, larger training models that you want to do.


201
00:28:43,000 --> 00:28:45,000
And again, there was a middle ground as well.


202
00:28:45,000 --> 00:29:02,000
At an ISP like Comcast, they would actually buy houses and put small data centers to be like node hubs in certain places before you have to go to, which for us at that time would have been data centers that we had or Amazon or whatever the case might be.


203
00:29:02,000 --> 00:29:21,000
So really understanding that there's these different levels where you can actually try to reduce the amount of data that you need to also make things more economic or augment data that you need closer before you have to go to the larger models actually will really help at scale at the large.


204
00:29:21,000 --> 00:29:32,000
Thank you. So the next question is, how do you ensure privacy and security of AI data in decentralized network?


205
00:29:32,000 --> 00:29:40,000
Yeah, I think there's been a really good history of that.


206
00:29:40,000 --> 00:29:52,000
Particularly if you look at some of the other social media technologies working on decentralized technology, a lot of times they prefer the NFT approach. So NFT is your way of owning and having a token for your data set.


207
00:29:52,000 --> 00:29:59,000
And then your data set can then reference data that lives in IPFS, potentially, as an example. It's a very popular technology.


208
00:29:59,000 --> 00:30:05,000
So your data lives in IPFS, you own it, it sits you, you can trade that and move it around.


209
00:30:05,000 --> 00:30:13,000
That seems to be a scalable mechanism, particularly if you're on either a mainnet chain or an L2 chain that can scale to the number of transactions and things like that.


210
00:30:13,000 --> 00:30:18,000
So that feels like kind of a natural emerging way to handle data management.


211
00:30:18,000 --> 00:30:24,000
I think like every other technical problem, it's use case dependent.


212
00:30:24,000 --> 00:30:28,000
So right now you see a lot of use cases where privacy isn't a concern.


213
00:30:28,000 --> 00:30:35,000
Everybody's throwing their data up to chat GPT, like, okay, cool, Microsoft's got it now.


214
00:30:35,000 --> 00:30:45,000
But there's lots of different techniques to use, pseudo-anonymization, trusted execution environments.


215
00:30:45,000 --> 00:30:51,000
It just depends on how accurate your information needs to be if you're training a model, right?


216
00:30:51,000 --> 00:30:57,000
How reliable that is if you are leveraging privacy preserving techniques.


217
00:30:57,000 --> 00:31:12,000
And then if you're using trusted execution environments or MPP, it's a question of what's the latency on those techniques.


218
00:31:12,000 --> 00:31:14,000
MPC, sorry, not MPP.


219
00:31:14,000 --> 00:31:25,000
Yeah, I mean to kind of go with that, and again, I was working at a company that's done a lot with UCANs and trying to really push that with execution in a compute graph.


220
00:31:25,000 --> 00:31:30,000
But also, I think actually you mentioned the trusted execution stuff like SGX and stuff like that.


221
00:31:30,000 --> 00:31:38,000
And when I think of the capability model, again, to me that's another form of like if I'm a node that has that and I know how to work on certain kinds of data,


222
00:31:38,000 --> 00:31:42,000
we'll be under the SGX umbrella that's going to run.


223
00:31:42,000 --> 00:31:48,000
I come up and I register with that so you can use me and then you can pay me because I'm running that, right?


224
00:31:48,000 --> 00:31:57,000
And so, and those are like very good positives that really come out of like understanding the space of registering and understanding who's available for what.


225
00:31:57,000 --> 00:32:03,000
And so, you know, if you're really good at execution with privacy, you'll probably get more money out of it, right?


226
00:32:03,000 --> 00:32:07,000
And that's a big one.


227
00:32:07,000 --> 00:32:10,000
Hari?


228
00:32:10,000 --> 00:32:20,000
Yeah, it's saying the concept is, as I admire what someone said about it being a use case dependent, when you think about the kind of large, the largest of the language models,


229
00:32:20,000 --> 00:32:24,000
currently they're trained, you know, exclusively on open data.


230
00:32:24,000 --> 00:32:33,000
So if the use case is kind of training those models, then the privacy concerns are less kind of salient.


231
00:32:33,000 --> 00:32:46,000
I think it was something that we saw when we interviewed kind of over 100 AI developers was that we were surprised by the kind of the percentage that actually had data privacy as a primary concern.


232
00:32:46,000 --> 00:32:54,000
It was definitely there, like there was over kind of roughly like 40% of them were, had that as a top concern.


233
00:32:54,000 --> 00:33:13,000
But breaking up by use case, there was a large percentage who actually didn't have it as something top of mind. I guess on top of that, there's a second consideration as well around, you know, there's the training data privacy, but then there's also the kind of model parameter data privacy considerations.


234
00:33:13,000 --> 00:33:27,000
So for example, you might not mind what the kind of, about the training data being public, but if you're, you know, building specific or you're tuning specific parameters, those can actually be quite important.


235
00:33:27,000 --> 00:33:37,000
And you don't want those to kind of be leaked to other sources so there's considerations on the kind of, I guess network parameter size as well.


236
00:33:37,000 --> 00:33:41,000
Okay, and last question of our panel.


237
00:33:41,000 --> 00:33:51,000
I would like to start with you. How can decentralized AI contribute to solving global challenges such as climate change and healthcare?


238
00:33:51,000 --> 00:34:00,000
Yeah, I think the single most important thing we can work on as a species is building artificial general intelligence.


239
00:34:00,000 --> 00:34:17,000
I think it occupies a kind of category of technology where there are just unknown unknowns about it in terms of what they can fix. I think a lot of people that's terrifying, because it kind of raises these existential questions around, you know, doom and alignment, etc.


240
00:34:17,000 --> 00:34:32,000
But having, you know, access to a tool which is legitimately smarter than us in ways that we can kind of fully comprehend I think is one of the most potentially beneficial things that we've ever done as a civilization.


241
00:34:32,000 --> 00:34:52,000
I think the most kind of immediate kind of short term impact of it is purely just in freeing up humans from, you know, labor which just isn't, you know, isn't kind of creative. So, so much of the kind of, you know, LLM tech generation that we've seen.


242
00:34:52,000 --> 00:35:10,000
It automates away a lot of administrative work, which isn't the kind of best use for kind of human time, it can be spent on much more creative endeavors. And I think yeah short term there's that, but long term the kind of unknown unknowns are the most exciting part there.


243
00:35:10,000 --> 00:35:22,000
Yeah, in general, the thing I like about crypto is that I think in particular like AI use cases for it is that it doesn't necessarily have to be a better solution than the current centralized solutions.


244
00:35:22,000 --> 00:35:42,000
But if it can provide a viable alternative that can make a massive impact for humanity in the sense that, you know, Bitcoin didn't need to be a better alternative than US dollar but once you have it now it's sort of it's an alternative and now eventually traditional fiat systems have Bitcoin to contend with.


245
00:35:42,000 --> 00:36:06,000
Ethereum didn't need to replace the financial system and financial contracts but now it's an alternative which takes away some, it provides sort of a check. So I think in the same sense, if users can have better privacy with decentralized AI and better trust and better representation of the societies that they operate in, then that's good because that's a check and then that means that traditional tech companies will have to contend and it'll be a healthy market force for other folks.


246
00:36:06,000 --> 00:36:17,000
So this is pretty specific but we spent a lot of time in Deci, another shameless plug, Nevermind was behind the first IP NFT.


247
00:36:17,000 --> 00:36:42,000
So anyway, when you look at that space, obviously the records that pertain to patients is quite important as it relates to the subset in Deci that's focused on healthcare and what emerges out of that is again this idea of informed consent both on the healthcare side and also on the general research side.


248
00:36:42,000 --> 00:37:02,000
So you can leverage AI to help with that informed consent process both from a cost displacement point of view where you don't need to have as many experts in the loop helping to solve that challenge of keeping the patient informed but also potentially scaling that capability.


249
00:37:02,000 --> 00:37:14,000
So you can actually potentially include more patients in your research by scaling that informed consent process.


250
00:37:14,000 --> 00:37:32,000
I was going to say too that study you were mentioning, that's not published anywhere, right? The developer study you did, AI developers, Harry?


251
00:37:32,000 --> 00:37:45,000
Tim?


252
00:37:45,000 --> 00:37:51,400
You want to include that for certain kinds of models, but you want people not to be able to have to give away their information.


253
00:37:51,720 --> 00:37:54,800
That's super key.


254
00:37:55,000 --> 00:38:06,000
I think the other thing too, with hardware changing as well, as we know, my MacBook now has 10 GPU.


255
00:38:06,120 --> 00:38:09,640
It's pretty good. It can run stable diffusion.


256
00:38:09,840 --> 00:38:13,920
I forget, I was rereading the OG stable diffusion paper.


257
00:38:13,920 --> 00:38:21,120
As much as it is about a lot of the great work they've done around working with images and the learning model,


258
00:38:21,320 --> 00:38:30,520
a lot of that paper is how well engineered the algorithm is to actually run on less powerful devices.


259
00:38:30,720 --> 00:38:38,720
Still more powerful than my phone, but not needing as large clusters of compute to do the work that it does.


260
00:38:38,920 --> 00:38:42,720
That's a really key part of the paper, in the abstract in the beginning.


261
00:38:42,720 --> 00:38:49,320
I think that ubiquitous hardware concept growing, and we know how Moore's law is out the window,


262
00:38:49,520 --> 00:38:57,320
and all these things that we've seen over time, that's going to showcase that we can do a lot more work in a decentralized way.


263
00:38:57,520 --> 00:39:04,920
That brings people who work in climate change, who work in places who never maybe would have the money or the sources


264
00:39:05,120 --> 00:39:09,720
to actually run certain models that they want to run on their data, they now can.


265
00:39:09,720 --> 00:39:12,520
I think that's where everything opens up.


266
00:39:12,720 --> 00:39:19,520
Can I just add one thing? We kind of missed the most obvious scalability thing attached to AI.


267
00:39:19,720 --> 00:39:25,720
AIs aren't going to have bank accounts, so how are they going to transact? Blockchain.


268
00:39:25,920 --> 00:39:30,520
Yeah, great point. Thank you. Thank you everyone.


269
00:39:30,720 --> 00:39:38,120
This is the end of our panel on AI data and compute, and also end of our track on decentralized computing AI.


270
00:39:38,120 --> 00:39:40,920
Thank you everyone for coming and listening.
