1
00:00:00,000 --> 00:00:10,040
So I'm going to be talking about the HTTP gateways, which is an aspect of IPFS that


2
00:00:10,040 --> 00:00:16,560
everyone has probably interacted with, right? You go to ipfs.io and we have a lot of traffic


3
00:00:16,560 --> 00:00:24,260
from web browsers, from current web2 interactions that aren't running a full IPFS client but


4
00:00:24,260 --> 00:00:29,980
need to interact with IPFS. So this is a bridge that currently is operated by a set of centralized


5
00:00:29,980 --> 00:00:37,080
operators and depending on which URL you go to, you'll get a specific operator. So one


6
00:00:37,080 --> 00:00:42,400
of the efforts that a relatively large group of people across a number of teams at PL has


7
00:00:42,400 --> 00:00:46,300
been working on over the last months has been to think about how do we decentralize and


8
00:00:46,300 --> 00:00:50,360
improve what that bridging looks like and how do we make the gateways better, what should


9
00:00:50,360 --> 00:00:56,100
the next iteration of these gateways look like? To that end, I'm going to start by giving


10
00:00:56,100 --> 00:01:00,460
you a little bit more view into what the gateways look like today so that we can see how do


11
00:01:00,460 --> 00:01:06,160
we do better. So when you go to the gateways, you're making requests largely of the form


12
00:01:06,160 --> 00:01:14,900
slash IPFS slash SID maybe slash path after that to get a file or a collection of files


13
00:01:14,900 --> 00:01:21,520
and those requests first go to a load balancer and then the load balancer sends it to an


14
00:01:21,520 --> 00:01:26,580
IPFS node that looks like the IPFS node that you might run on your own computer. It's tuned


15
00:01:26,580 --> 00:01:30,500
differently, it has different configuration, but it is an IPFS node that then goes and


16
00:01:30,500 --> 00:01:37,300
gets the content when it doesn't already have it from the broad public IPFS network. And


17
00:01:37,300 --> 00:01:43,100
so when we think about how are we going to do better here, what are our opportunities?


18
00:01:43,100 --> 00:01:47,220
Well one thing that is, well where are these? Can we make it faster? That's a natural thing


19
00:01:47,220 --> 00:01:54,260
that we might say is, you know, are there limitations in this current setup? And when


20
00:01:54,260 --> 00:02:00,180
we look at IPFS.io or dweb.link specifically, there are seven locations around the world


21
00:02:00,180 --> 00:02:05,300
where those are. So if I was to access IPFS.io today, I would be going up to the Amsterdam


22
00:02:05,300 --> 00:02:10,980
location. You can see that we're down in Brussels. And when we look at that topology of where


23
00:02:10,980 --> 00:02:17,580
things are, there actually is an interconnect and a data center about six blocks from us.


24
00:02:17,580 --> 00:02:22,180
The Belgian commercial data center is there. Our traffic goes through it on the way up


25
00:02:22,180 --> 00:02:28,980
to Amsterdam. And a lot of other CDN-like things that you have, like Cloudflare or Netflix,


26
00:02:28,980 --> 00:02:35,180
are terminating in Belgium and have lower latency than going to one of seven. We can


27
00:02:35,180 --> 00:02:39,860
think about sort of these classes of latency. And so we've got a couple options. We would


28
00:02:39,860 --> 00:02:47,860
need to either have a lot more points of presence to terminate these connections closer to users.


29
00:02:47,860 --> 00:02:51,700
And if we want to do that, that's going to cost money, right? Like there needs to be


30
00:02:51,700 --> 00:02:58,100
these servers in not just seven places, but in hundreds of places. If we're going to be


31
00:02:58,100 --> 00:03:05,140
able to get that final hop to users faster, we need to figure out how do we get this not


32
00:03:05,140 --> 00:03:10,740
just in these specific data centers. The good news is Protocol Labs already has


33
00:03:10,740 --> 00:03:18,540
one of these CDNs with lots of locations. Last fall, project Saturn launched. Saturn


34
00:03:18,540 --> 00:03:24,420
has almost 2,000 nodes currently. And you can see that it has nodes both in Belgium


35
00:03:24,420 --> 00:03:31,340
and in Amsterdam, or in the Netherlands, along with nodes scattered through many countries


36
00:03:31,340 --> 00:03:37,540
at this point. And so we've already incentivized people to be running servers in most metro


37
00:03:37,540 --> 00:03:44,020
areas. And so there's this question then of can we leverage that to get these final hops,


38
00:03:44,020 --> 00:03:51,260
to get IPFS data and perform this bridge faster for users and make a better experience.


39
00:03:51,260 --> 00:03:57,780
And so with that, project Rhea launched. It has three sort of concurrent goals overall.


40
00:03:57,780 --> 00:04:03,860
We want to be able to retrieve not just from IPFS, but also from Filecoin and have sort


41
00:04:03,860 --> 00:04:07,820
of a broader set of retrievals of content address data and think about how we iterate


42
00:04:07,820 --> 00:04:14,180
there. We want to validate Saturn as a CDN so that we're eating our own dog food, but


43
00:04:14,180 --> 00:04:21,460
also validating that this CDN and in practice a decentralized CDN is an effective way to


44
00:04:21,460 --> 00:04:27,740
serve traffic. And we want to reduce the centralization that has ended up occurring of most of the


45
00:04:27,740 --> 00:04:34,860
most traffic going through a PL run entity. That IPFS.io as a website that people have


46
00:04:34,860 --> 00:04:38,620
sort of gotten in their heads is like the place that you could type in and put as a


47
00:04:38,620 --> 00:04:44,660
URL. That means you're sort of going to some DNS that Protocol Labs runs. And for many


48
00:04:44,660 --> 00:04:50,180
of the requests, you're trusting Protocol Labs to validate the content that you're getting.


49
00:04:50,180 --> 00:04:56,940
And so from this, we've ended up I think with two outcomes that are worth socializing to


50
00:04:56,940 --> 00:05:03,140
the broad IPFS community. The first is we're going to have a series of iterations of shifting


51
00:05:03,140 --> 00:05:10,820
the trust model. Right now, when the bulk of requests that go over the HTTP gateway


52
00:05:10,820 --> 00:05:15,820
end up getting sort of a rendered file back, right? You want your image, you want your


53
00:05:15,820 --> 00:05:22,860
HTML page, and what you get back from that IPFS.io slash IPFS slash Baffy whatever is


54
00:05:22,860 --> 00:05:27,140
the final thing when you render it in your browser or follow the link today. And when


55
00:05:27,140 --> 00:05:33,020
that happens, that final rendering isn't your client revalidating that it's actually gotten


56
00:05:33,020 --> 00:05:38,500
the hash that it asked for. That validation is happening currently on the servers. And


57
00:05:38,500 --> 00:05:43,700
so we want to encourage more clients to get built and that the default should be that


58
00:05:43,700 --> 00:05:49,100
you get back the blocks that you ask for and then locally reconstruct and render and build


59
00:05:49,100 --> 00:05:53,260
the file. And there's a number of ways to do this. One is with service workers. Another


60
00:05:53,260 --> 00:05:57,900
is with sort of somewhat thicker clients or just JavaScript libraries depending on how


61
00:05:57,900 --> 00:06:02,620
you're integrating that. But if the client validates, there's suddenly a lot more flexibility


62
00:06:02,620 --> 00:06:10,340
and we can offer both better speed and cheaper access to content address data and more resilient.


63
00:06:10,340 --> 00:06:16,040
And so that's going to sort of be an increasing carrot in some sense is that if you're willing


64
00:06:16,040 --> 00:06:20,780
to expand your client experience so that you can do validation, you'll be able to get a


65
00:06:20,780 --> 00:06:26,380
much better experience. We're also building a lot of stuff to enable this. And so some


66
00:06:26,380 --> 00:06:33,300
that I want to call out that we'll be diving into over the coming days. One is Bifrost


67
00:06:33,300 --> 00:06:40,280
Gateway. So on the left diagram, you see the first sort of stage of how we're thinking


68
00:06:40,280 --> 00:06:45,940
about this change in RIA is that inbound requests from the load balancer are going to go not


69
00:06:45,940 --> 00:06:53,100
to a Kubo node, but to a new piece of software called Bifrost Gateway. And Bifrost Gateway


70
00:06:53,100 --> 00:06:59,440
is a refactoring of the gateway's code that can do the same parsing and interpretation


71
00:06:59,440 --> 00:07:07,220
of client requests. But instead of then being Kubo and taking that into a bit swap combination


72
00:07:07,220 --> 00:07:12,060
with a local block store, it's going to pretend that it has a remote block store and it's


73
00:07:12,060 --> 00:07:19,060
going to understand how to make a more semantically meaningful request for the data that it needs


74
00:07:19,060 --> 00:07:24,220
to serve those HTTP requests. So it's going to be able to say, in order to serve this


75
00:07:24,220 --> 00:07:30,300
render directory, what I actually need is the car file with this set of blocks. And


76
00:07:30,300 --> 00:07:36,100
so it's able to describe that in terms of a selector that describes the depth of how


77
00:07:36,100 --> 00:07:41,620
many blocks and in what shape it wants back to some remote backend. And so that's the


78
00:07:41,620 --> 00:07:46,100
request that then goes back to Saturn. This gateway is able to do the verification for


79
00:07:46,100 --> 00:07:50,780
clients that aren't verifying yet in the same way that the current gateway does. But it


80
00:07:50,780 --> 00:07:56,220
starts to define what we're going to need for the trustless HTTP spec for how a gateway


81
00:07:56,220 --> 00:08:02,580
that isn't trusted is able to serve content. The second big piece that we'll hear about


82
00:08:02,580 --> 00:08:07,420
later today in the data transfer track is a project called LASI, which then is just


83
00:08:07,420 --> 00:08:11,940
the fetching part. And so it gets these requests and is looking at how does it get it from


84
00:08:11,940 --> 00:08:19,740
the IPFS and from the Filecoin network. It uses IPNI as a primary way to find where content


85
00:08:19,740 --> 00:08:25,380
is around the content addressed universe, and then is able to retrieve, construct into


86
00:08:25,380 --> 00:08:31,980
cars and return the content that is being asked for. And so it's not saving stuff. It


87
00:08:31,980 --> 00:08:37,380
doesn't have a block store. It doesn't have the interpretation of Merkle DAGs in the full


88
00:08:37,380 --> 00:08:43,060
way. So in some ways it's a much lighter and it's able to be embedded as a library within


89
00:08:43,060 --> 00:08:50,220
other pieces of software. And so by putting that into Saturn, we're able to then take


90
00:08:50,220 --> 00:08:55,460
Saturn responses of cars, do the trust when we need to at the current gateways, but also


91
00:08:55,460 --> 00:09:00,140
start to encourage people to talk to Saturn directly because that's how they get the thousands


92
00:09:00,140 --> 00:09:04,220
of points of presence instead of just the single one. So that's RIA. You'll hear a lot


93
00:09:04,220 --> 00:09:07,580
about it over the next couple of days. There's a lot of people in this room who have been


94
00:09:07,580 --> 00:09:11,900
working on it in various ways over the last months. And I'm excited to talk more about


95
00:09:11,900 --> 00:09:36,900
it.
