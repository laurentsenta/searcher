1
00:00:00,000 --> 00:00:09,360
Well thanks y'all for coming out to do database stuff with us today. So I'm going to be talking


2
00:00:09,360 --> 00:00:14,360
about Fireproof, which this is the official launch. If there's an official launch of Fireproof


3
00:00:14,360 --> 00:00:19,720
it's right now and I can prove it because I have this super hot hot sauce that all the


4
00:00:19,720 --> 00:00:26,880
Europeans are going to start sweating when they eat. And yeah, I got into this maybe


5
00:00:26,880 --> 00:00:36,120
14, 15 years ago building peer-to-peer databases with Apache CouchDB and have been super excited


6
00:00:36,120 --> 00:00:41,920
about this space since then. And kind of sitting back and letting the infrastructure create


7
00:00:41,920 --> 00:00:48,760
new opportunities to build cool databases. There wasn't a way to build a database cooler


8
00:00:48,760 --> 00:00:56,740
than Apache CouchDB until now. And so with the advent of Prolly Trees, well we were able


9
00:00:56,740 --> 00:01:05,320
to do stuff on the immutable deterministic data backend that we weren't able to do before.


10
00:01:05,320 --> 00:01:12,720
So Fireproof takes advantage of Prolly Trees and it's been really enlightening to hear


11
00:01:12,720 --> 00:01:18,480
the other discussions today from folks who are deep in the database engineering side


12
00:01:18,480 --> 00:01:23,160
of it because I knew there was a lot that I was just touching the tip of the iceberg


13
00:01:23,160 --> 00:01:30,240
as far as my understanding of the data structures. So as far as the value that I'm trying to


14
00:01:30,240 --> 00:01:35,200
bring to the table in this community, it's how can we package this stuff to be so easy


15
00:01:35,200 --> 00:01:41,680
to use. There's a lot of power in Web 3 and almost all of it has gone toward like let's


16
00:01:41,680 --> 00:01:47,280
do something so totally super new that we never could have done before. And so that's


17
00:01:47,280 --> 00:01:53,880
great and my instinct is to take that capabilities and say let's take that thing that everybody


18
00:01:53,880 --> 00:01:58,520
wants to do and just make it so easy that they can't believe it got this easy. So that's


19
00:01:58,520 --> 00:02:03,480
what the goal with Fireproof is, is build a database that React developers can drop


20
00:02:03,480 --> 00:02:08,680
in the page and have interactive application features and then get all the data integrity


21
00:02:08,680 --> 00:02:15,720
and replication capabilities that you normally need a backend database to handle. So we talk


22
00:02:15,720 --> 00:02:21,640
about IPFS and peer-to-peer means the data can be anywhere. You can ask the network for


23
00:02:21,640 --> 00:02:29,000
the data. The database is the network. And if that's all true, then the place that you


24
00:02:29,000 --> 00:02:35,000
want your operations to start is as close to that user as possible. So in the browser


25
00:02:35,000 --> 00:02:45,680
or in the mobile device. So that's what Fireproof is about. And the big goal here being to make


26
00:02:45,680 --> 00:02:55,760
it just easier than whatever, what was the previous easiest database? Like whatever it


27
00:02:55,760 --> 00:03:04,800
was was like 10 times harder to use. So that's the idea here. And then there's a bunch of


28
00:03:04,800 --> 00:03:10,480
stuff that we get because of being part of this community and using these data structures


29
00:03:10,480 --> 00:03:22,360
that is like more serious than the previous easiest to use database. So with Merkle proofs,


30
00:03:22,360 --> 00:03:29,240
we can do things like data provenance for AI API calls. Anything that you see signed,


31
00:03:29,240 --> 00:03:34,720
you drop in there. You can sign Merkle proofs. You can put Merkle proofs on the blockchain.


32
00:03:34,720 --> 00:03:39,080
And those are all use cases we were talking about in the last couple hours. So I feel


33
00:03:39,080 --> 00:03:49,240
like that sitting in this space in the engineering tradeoffs by being on the browser but using


34
00:03:49,240 --> 00:03:56,520
Merkle proofs and using IPFS and Web3 storage for replication allows us to make a database


35
00:03:56,520 --> 00:04:03,800
that doesn't give away any seriousness by being so easy to use and so close to the end


36
00:04:03,800 --> 00:04:12,800
user. Which, you know, with my experience in Couch, you can do that, but we had to work


37
00:04:12,800 --> 00:04:18,040
a lot harder, I think, than Fireproof has to work in order to make the database useful


38
00:04:18,040 --> 00:04:25,320
for enterprise contexts. Yeah, I want to get into the actual how it works by diving through


39
00:04:25,320 --> 00:04:33,880
the mirror. And so we're going to talk about document lookup and the event feed and how


40
00:04:33,880 --> 00:04:38,720
the indexes are built. So that's all the application layer data management stuff. And then we'll


41
00:04:38,720 --> 00:04:43,400
talk about the infrastructure side of it. So how transactions work, how replication


42
00:04:43,400 --> 00:04:49,560
works, how encryption works, and how sync works. And about 50% of this stuff isn't written


43
00:04:49,560 --> 00:04:56,680
yet but, you know, the whole thing is discovered, not invented. So it's not like there's a whole


44
00:04:56,680 --> 00:05:03,840
bunch of other ways it could go. So there's two main data structures in Fireproof. We


45
00:05:03,840 --> 00:05:08,240
just heard all about Prolly trees. So that's what's going on over here. And we'll come


46
00:05:08,240 --> 00:05:16,680
back to that in a second. But we also use Merkle CRDT clock. And that is a lot like


47
00:05:16,680 --> 00:05:23,920
the data structures that we heard about from Aaron's talk at the beginning. So what the


48
00:05:23,920 --> 00:05:29,240
clock does, and thanks Alan Shaw, who's going to be giving a detailed talk on this clock


49
00:05:29,240 --> 00:05:37,840
library in this room in the afternoon, is it allows you to have update histories that


50
00:05:37,840 --> 00:05:45,640
diverge and merge, and then to ask questions of that update history in an efficient way.


51
00:05:45,640 --> 00:05:53,080
So the way that we use this is every new update is based on a previous update. So when you


52
00:05:53,080 --> 00:05:58,480
write into the clock, you know, your event is always appended to the clock's current.


53
00:05:58,480 --> 00:06:05,920
But when you come back to the clock, you can have it give you everything since, like, say


54
00:06:05,920 --> 00:06:13,280
this update. So if you were here, what would you need to sync to get everything you didn't


55
00:06:13,280 --> 00:06:19,000
already have before? Well, it would include this, you know, as well as everything since


56
00:06:19,000 --> 00:06:25,280
then. So being able to pull those nodes out of the data structure and play them forward


57
00:06:25,280 --> 00:06:30,880
is one of the things the clock offers. And we leverage that not just for, like, the update


58
00:06:30,880 --> 00:06:35,680
cycle, but also to power the indexes. Because when you go to query an index, like, you're


59
00:06:35,680 --> 00:06:40,000
not going to want to update the indexes on write, because as we saw, Prolly trees are


60
00:06:40,000 --> 00:06:44,000
going to be expensive to churn every single time there's a little change. So instead,


61
00:06:44,000 --> 00:06:47,840
we query the index on read, and that gives us natural batching, all the updates that


62
00:06:47,840 --> 00:06:51,840
happened since the last time, then get ingested at once into the index. But that means we


63
00:06:51,840 --> 00:06:57,360
need to go to the clock and say, what's that batch look like? And so that's what this is.


64
00:06:57,360 --> 00:07:02,120
We'll go, you know, from the last time the index was updated, and then pull in all the


65
00:07:02,120 --> 00:07:10,560
relevant events and play them into the index again before we query.


66
00:07:10,560 --> 00:07:21,360
So what is hung off of each of these nodes is a version of one of these. And this is


67
00:07:21,360 --> 00:07:26,600
the Prolly tree JavaScript implementation that Michael Rogers wrote that is part of


68
00:07:26,600 --> 00:07:37,480
the same spec as the Go one that we were just hearing about. So it's going to, I mean, I


69
00:07:37,480 --> 00:07:41,720
think it may be diverging from the spec a little bit, which means we're innovating,


70
00:07:41,720 --> 00:07:48,520
and now it's time to go push those changes back up. But because of the way these Prolly


71
00:07:48,520 --> 00:07:54,680
trees work, if I were to put a change in here, you know, it's only going to change these


72
00:07:54,680 --> 00:08:02,840
nodes. And so the operations are relatively lightweight, which means that although we


73
00:08:02,840 --> 00:08:08,800
have a version of the tree hanging off of each one of these events, the delta, the difference


74
00:08:08,800 --> 00:08:13,720
between what's going on in one of these events, is pretty small. So it's not like we're duplicating


75
00:08:13,720 --> 00:08:18,640
the tree over and over again. This essentially means that we couldn't have built this at


76
00:08:18,640 --> 00:08:22,000
all without Prolly trees. If you tried to build this without Prolly trees, you'd get


77
00:08:22,000 --> 00:08:27,160
huge write amplification with, you know, just a ton of disk work for every little thing


78
00:08:27,160 --> 00:08:32,800
you did. But instead, almost all the writes resolve to stuff that's already been written.


79
00:08:32,800 --> 00:08:40,520
And so it makes it efficient on the back end to work with these kind of things. So, yeah,


80
00:08:40,520 --> 00:08:54,240
that's how the storage works. And a little bit of how the event feed works. I'll talk


81
00:08:54,240 --> 00:09:00,920
about, you know, how the indexes are built. It's like an Apache CouchDB style index. So


82
00:09:00,920 --> 00:09:07,400
you write a JavaScript function and you say, I want to index, you know, anything that has


83
00:09:07,400 --> 00:09:15,200
a title, I want to index it by title, and now you can browse all the blog posts alphabetically.


84
00:09:15,200 --> 00:09:21,800
So the indexes are also built out of the same data structure, which was, is a choice that


85
00:09:21,800 --> 00:09:27,800
I didn't have to make, right? Like, the core data structure needs to be a Prolly tree,


86
00:09:27,800 --> 00:09:33,360
and it needs to be working with this clock. But the derived indexes, you could just say,


87
00:09:33,360 --> 00:09:38,440
hey, I'm going to keep those only in the browser and have them be raw indexed DB. You might


88
00:09:38,440 --> 00:09:46,000
get like a small performance gain by doing that, even. But the reason to have the indexes


89
00:09:46,000 --> 00:09:53,120
as well play in the Prolly tree content address game is that when you issue a query into the


90
00:09:53,120 --> 00:09:57,520
index, you know, it's going to hop down the nodes until it finds the range it's working


91
00:09:57,520 --> 00:10:03,160
on. And what I can do when I give you those results and what Fireproof does and where


92
00:10:03,160 --> 00:10:06,800
the name Proof comes from is it includes all those CIDs in the result set, and you can


93
00:10:06,800 --> 00:10:12,800
use that for all kinds of stuff. Maybe the most straightforward is just an accelerator.


94
00:10:12,800 --> 00:10:17,360
You know, you know, perhaps you have a big data set and a client wants to query it, so


95
00:10:17,360 --> 00:10:21,280
they send the query over, and oh, these are the blocks the client needs to have in order


96
00:10:21,280 --> 00:10:26,960
to traverse into the tree, even the big tree, and not miss anything. So then you can ship


97
00:10:26,960 --> 00:10:33,440
all those blocks over, you know, like a Graph Sync style operation, and the client is able


98
00:10:33,440 --> 00:10:40,400
to operate seamlessly on the big data set without having to replicate it in advance.


99
00:10:40,400 --> 00:10:47,960
That's most of what there is for the front end. There's also, I suppose, the same way


100
00:10:47,960 --> 00:10:52,200
that the indexes know to bring themselves up to date by consuming all the changes that


101
00:10:52,200 --> 00:10:56,880
are relevant since the last time they were updated. There's public APIs, so you can hang


102
00:10:56,880 --> 00:11:04,000
your own indexer off of that and hydrate a full-text index from your document set or


103
00:11:04,000 --> 00:11:08,040
machine learning model or anything that needs to play causally through the data and be able


104
00:11:08,040 --> 00:11:13,880
to pause and restart and pick up again, you know, or pick up new changes after they've


105
00:11:13,880 --> 00:11:23,180
been applied. So the same feed, which, you know, in that case allows you to incrementally


106
00:11:23,180 --> 00:11:30,960
index stuff and is kind of a database-y feature, is also a very, like, it's probably the priority


107
00:11:30,960 --> 00:11:38,360
zero most important feature, aside from being able to put and get data, for React developers,


108
00:11:38,360 --> 00:11:44,840
because once you have this time to refresh function, and here's what you need to refresh


109
00:11:44,840 --> 00:11:50,560
with, now you can provide React developers with that auto-refresh that repaints the page,


110
00:11:50,560 --> 00:11:54,760
so when you're collaborating with other users, if database changes come in, the page will


111
00:11:54,760 --> 00:12:00,960
repaint automatically, or if you don't want to have to thread all those data changes and


112
00:12:00,960 --> 00:12:07,680
know what to refresh as a developer when you make a change, you know, you can put listeners


113
00:12:07,680 --> 00:12:12,420
into your app listening for specific events in the database, make changes in the database,


114
00:12:12,420 --> 00:12:20,320
and have the database dispatch those events to your UI, which most React apps end up doing


115
00:12:20,320 --> 00:12:24,400
that anyway, but without the benefit of the database, right? They have to do all this


116
00:12:24,400 --> 00:12:29,800
data flow just to get the events to refresh in the UI, and if they were to just put that


117
00:12:29,800 --> 00:12:36,200
through the database on the way, then they get both jobs done at once. So it makes for


118
00:12:36,200 --> 00:12:47,520
a simpler, faster development. So when we get into the storage engine side of it, now,


119
00:12:47,520 --> 00:12:54,000
this is the part that is less visible to the functional requirements and more about what


120
00:12:54,000 --> 00:13:01,640
you can do with it. Everything that the database does writes to an in-memory block store, which


121
00:13:01,640 --> 00:13:12,160
is just a JavaScript map of blocks from CID to block, and that at the end of an operation,


122
00:13:12,160 --> 00:13:17,940
you're going to have updated a handful of blocks. So your per-operation block store


123
00:13:17,940 --> 00:13:24,520
might only have, like, six blocks in it that you just wrote. So what we'll do is we take


124
00:13:24,520 --> 00:13:34,040
those blocks, and we wrap them in a car file, and this gets us, like, entry into all the


125
00:13:34,040 --> 00:13:40,040
new transport stuff that's coming online with Web3 storage and Saturn, because car files


126
00:13:40,040 --> 00:13:47,440
are, they're just a bag of blocks. It's like a tar or a zip, but for IPFS blocks.


127
00:13:47,440 --> 00:13:53,800
And by putting those at our transaction boundary, we get all kinds of neat stuff, right? Like,


128
00:13:53,800 --> 00:13:58,840
it doesn't have to be at our transaction boundary, and there's other times we might repack it


129
00:13:58,840 --> 00:14:05,080
into other sizes, but if my car files are coming off this, you know, let's say you've


130
00:14:05,080 --> 00:14:10,040
got an active database, now you just have a queue of car files, one per transaction


131
00:14:10,040 --> 00:14:18,280
coming off of it, and I can, it's more I don't do this, but I could, and it works, upload


132
00:14:18,280 --> 00:14:22,280
those car files to Web3 storage as they happen. And there's certain use cases where you'd


133
00:14:22,280 --> 00:14:27,560
want your data to be in the clear, and for that to be your replication strategy. When


134
00:14:27,560 --> 00:14:33,080
you do it that way, you can just delete your local index DUB, and as long as you have the


135
00:14:33,080 --> 00:14:38,520
ID of the root of your PRAWLI tree, then the app works just fine, even if a little bit


136
00:14:38,520 --> 00:14:44,680
slow. So it is kind of nice to replicate in the clear like that, and we'll keep that


137
00:14:44,680 --> 00:14:50,760
an option for databases with encryption disabled, but encryption is enabled by default, and


138
00:14:50,760 --> 00:14:55,720
so that kind of replication doesn't work with encrypted data. You have to think about


139
00:14:55,720 --> 00:14:59,720
it a little bit differently. Luckily, when you replicate encrypted data, I think it's


140
00:14:59,720 --> 00:15:09,000
faster, because you're pulling down car files instead of blocks. So to talk a little bit


141
00:15:09,000 --> 00:15:15,800
about how it works in the encrypted case, what we do is between the clear blocks that


142
00:15:15,800 --> 00:15:22,680
were written to the in-memory store and the car file, we encrypt the blocks with a symmetric


143
00:15:22,680 --> 00:15:28,040
key, and so the symmetric key, it's not like we're doing some kind of fancy public key


144
00:15:28,040 --> 00:15:31,800
thing there. We're not doing key management. We're just making it so that the blocks that


145
00:15:31,800 --> 00:15:36,680
go in that car file and get sent to Web3 storage aren't readable by anybody who happens by,


146
00:15:36,680 --> 00:15:44,680
and so you have now, as part of the local data that you need, the head that it takes


147
00:15:44,680 --> 00:15:51,720
to decode the PRAWLI tree, not only do you need the CID of that root, but you also need


148
00:15:51,720 --> 00:15:57,240
the decryption key. So now you've got a tuple of kind of boot-up information that you need


149
00:15:57,240 --> 00:16:07,640
to read the tree. But it also means that what you're replicating up to the cloud is gonna


150
00:16:07,640 --> 00:16:17,640
be something you're fetching back by car instead of by block. So it's gonna give you, in general,


151
00:16:17,640 --> 00:16:23,640
more of like a GraphSync accelerated refetch. There's a lot of other things that we can


152
00:16:23,640 --> 00:16:31,080
do when we expand the car file scope beyond a single transaction. So one use case for


153
00:16:31,080 --> 00:16:39,080
these car files is like, let's say I have a user profile builder page that I give the


154
00:16:39,080 --> 00:16:44,520
user, and they're gonna choose the background color and which image they want and what their


155
00:16:44,520 --> 00:16:50,840
caption is and that kind of stuff. And so that would be an HTML file with a fireproof


156
00:16:50,840 --> 00:16:55,240
in it, and as they're working, saving that information to fireproof, and at the end,


157
00:16:55,240 --> 00:17:01,800
they'll have that cookie-sized thing that you need to rehydrate from. So they can save


158
00:17:01,800 --> 00:17:08,040
that to your existing application database, your MySQL or whatever, and then when you


159
00:17:08,040 --> 00:17:17,400
go to build your static site, including all those profile pages, you can say, hey, fireproof,


160
00:17:17,400 --> 00:17:25,040
give me the car file for that entire database. So maybe it's like 1,000 blocks. It's everything


161
00:17:25,040 --> 00:17:30,920
the user wanted on their profile all wrapped up into one content delivery network-friendly


162
00:17:30,920 --> 00:17:40,880
package. And so then you'd have your compact database in one file in your Gatsby static


163
00:17:40,880 --> 00:17:47,440
site build so that when the page fetch first happens, the users are interacting with a


164
00:17:47,440 --> 00:17:51,880
dataset that's been scoped for exactly them. If there have been changes that came, you


165
00:17:51,880 --> 00:17:57,200
know, if the user, the composer of the dataset has done work since the publish operation,


166
00:17:57,200 --> 00:18:02,960
you can fetch the diff and hydrate the new data right there in the browser.


167
00:18:02,960 --> 00:18:08,960
So that's a use case for packing the car files or packing more than one transaction in a


168
00:18:08,960 --> 00:18:15,080
car file is you can put the whole database together for an accelerated download. One


169
00:18:15,080 --> 00:18:20,880
of the other things, like maybe let's go back and if these encrypted blocks are the


170
00:18:20,880 --> 00:18:29,280
neon green, if we have the non-encrypted blocks, right, in a car file, there's other useful


171
00:18:29,280 --> 00:18:36,200
stuff we can do because it's on a transaction boundary. So one problem in this world of


172
00:18:36,200 --> 00:18:39,480
building collaborative apps, probably one of the harder problems is key management,


173
00:18:39,480 --> 00:18:45,160
especially for shared data. So you can do, there's lots of fancy solutions. I might even


174
00:18:45,160 --> 00:18:49,800
outline like one that's on the horizon for doing key management with shared data because


175
00:18:49,800 --> 00:18:57,400
I think it's neat. But what's neater is not having to do stuff. And so the way that the


176
00:18:57,400 --> 00:19:03,840
sync, I distinguish sync from replication in this context. Replication is taking your


177
00:19:03,840 --> 00:19:07,560
data from your application and putting it somewhere persistent like Web3 storage. And


178
00:19:07,560 --> 00:19:12,520
sync is working with collaborators in real time on your data. And so if I want to sync


179
00:19:12,520 --> 00:19:18,880
with some people on a dataset and fireproof, then what I would do is create a trusted channel


180
00:19:18,880 --> 00:19:24,280
like a WebRTC or something that we're all in and just bladder around these car files


181
00:19:24,280 --> 00:19:29,120
in the clear. There's no reason to encrypt it because we are already, you know, as end


182
00:19:29,120 --> 00:19:35,080
users encrypting our own data at rest copy of it. And we already trust the channel to


183
00:19:35,080 --> 00:19:38,760
be secure and who else is in the channel to belong in the channel. So we send them the


184
00:19:38,760 --> 00:19:43,640
data inside that channel and then they're able to encrypt it for their own persistence.


185
00:19:43,640 --> 00:19:54,160
So just one of the ways that the, you know, my hypothesis starting this project or rather


186
00:19:54,160 --> 00:19:57,800
joining Protocol Labs, you know, a year and a half ago because I wanted to do this project


187
00:19:57,800 --> 00:20:00,080
was I bet that when you get.


188
00:20:00,000 --> 00:20:09,560
ľI think it's database building time and so that's it turned out to be database building time.


189
00:20:09,560 --> 00:20:19,920
ľI think that's it for the how it works section. Oh yeah, okay, so question about the encryption key.


190
00:20:19,920 --> 00:20:33,920
ľI actually I did I said I did a teaser about maybe something a little bit more nuanced. So what I'm doing right now is the simplest thing that could possibly work,


191
00:20:33,920 --> 00:20:47,920
which is for the entire database. There's just a fixed AES symmetric key that's used to put that in to make the car files and then that means it's safe to ship the car files over the wire to somebody you don't trust.


192
00:20:47,920 --> 00:20:55,920
And that's essentially the only purpose of it and it punts on the key management. It means somebody's going to have to figure out a sane thing to do with these keys.


193
00:20:55,920 --> 00:21:05,920
But in the absence of figuring that out, if all you do is treat it like it's part of the rehydrate cookie, you're still better than if everything was in the clear.


194
00:21:05,920 --> 00:21:21,920
So that's the initial scope, but let's say I wanted to do instead of a live sync to live participants, if I wanted to do an encrypted exchange,


195
00:21:21,920 --> 00:21:35,920
then what we could do and this is with some of the new UCAN like attestation stuff that the team, Web3 storage team has been working on recently,


196
00:21:35,920 --> 00:21:49,920
is each one of these encrypted cars gets its own one time key and then in the UCAN that's associated with it, you'd say, you know, for each person in your group,


197
00:21:49,920 --> 00:21:58,920
you would encrypt the one time key with their public key. And so you're only shipping the diff once, but it's encrypted securely for the participants


198
00:21:58,920 --> 00:22:08,920
and you only use that one time key one time. So it's not, or rather I haven't analyzed it to see if it's already there,


199
00:22:08,920 --> 00:22:18,920
but it's shaping up to be like the perfect forward secrecy stuff that you get from open secure messaging and those types of protocols.


200
00:22:18,920 --> 00:22:30,920
So it would allow, I think, with a little bit of thought for us to build that level of security into the messaging system on top of it.


201
00:22:30,920 --> 00:22:40,920
So yeah, I'll hop back into this. I've got just a couple of example use cases. The big one I mentioned already was add data to any page.


202
00:22:40,920 --> 00:22:46,920
And so the rehydration cookie, as I called it, is just something that you can put in local storage.


203
00:22:46,920 --> 00:22:53,920
Fireproof does this for you by default. You don't have to worry about it, but that's all it is, is putting that somewhere so that when you rehydrate the page,


204
00:22:53,920 --> 00:23:06,920
it gets the right state. And so, you know, depending on your replication settings, then whether or not that Fireproof ever existed on that node or not,


205
00:23:06,920 --> 00:23:12,920
it'll be able to fetch down the data and interact with it. So use cases where putting data into any page is valuable,


206
00:23:12,920 --> 00:23:20,920
something like a Salesforce app or like any other enterprise app where you've built some line of business thing and it's going to be super expensive to change,


207
00:23:20,920 --> 00:23:30,920
but you just need to add a couple more fields or bring in another widget or do just like some lightweight stuff on top of an existing line of business app,


208
00:23:30,920 --> 00:23:37,920
there's just so much institutional inertia sometimes to making changes through the back end.


209
00:23:37,920 --> 00:23:48,920
But if you can make those changes all on the front end and have the data, you know, kind of guarantees and security and integrity that Fireproof offers,


210
00:23:48,920 --> 00:23:58,920
then why would you do it on the back end? So definitely a lot of opportunity in those kind of upgrading those line of business apps.


211
00:23:58,920 --> 00:24:01,920
Anytime you want to do, you know, structured local data on the browser.


212
00:24:01,920 --> 00:24:10,920
So A, B testing or bringing up features from behind feature flags for users makes a lot of sense also. Customizable widgets.


213
00:24:10,920 --> 00:24:18,920
And then, you know, kind of going one deeper, like you've got a whole enterprise line of business set of apps for your users.


214
00:24:18,920 --> 00:24:25,920
And now you want to add some kind of auditing layer where you just want to take a secondary copy of the interactions and put them somewhere else


215
00:24:25,920 --> 00:24:33,920
because the original apps are, you know, crufty and you don't even know what they're doing. So you want to do that and figure out what's going on with the original apps.


216
00:24:33,920 --> 00:24:42,920
But really any of that kind of injectable logic. And then outside of, you know, these use cases where it's allowing you to do something maybe you couldn't do before.


217
00:24:42,920 --> 00:24:54,920
And so you can upgrade apps you couldn't upgrade before. There's also just a ton of let's build something and use the right tool for the job that these kind of data structures offer.


218
00:24:54,920 --> 00:25:02,920
A huge one. I mean, we talked a lot about like side chain safety and how you can link to Merkle proofs from blockchains.


219
00:25:02,920 --> 00:25:08,920
But provenance tracking, which is, you know, popularized, I guess, by NFTs.


220
00:25:08,920 --> 00:25:16,920
But if you take those NFT data structures and you use them on the outputs from AI models and not just generated images with text and everything,


221
00:25:16,920 --> 00:25:25,920
then it allows you to get a layer of accountability for AI that's missing right now. And not just accountability, but acceleration.


222
00:25:25,920 --> 00:25:31,920
Because if you know that it's, you know, if you set your temperature to zero, so you have a deterministic result,


223
00:25:31,920 --> 00:25:40,920
then you can keep a rainbow table of this model, this prompt, this output, and then you don't have to re-prompt all the time.


224
00:25:40,920 --> 00:25:48,920
So AI models are just an expensive version of any kind of data science workload. But if you've ever worked with data scientists,


225
00:25:48,920 --> 00:25:54,920
you'll see that they often rerun the whole notebook, even though only some of the data has changed.


226
00:25:54,920 --> 00:26:00,920
And that's fine. You're not going to tell them not to do that, but it sure would be nice if that rerun was instant and free.


227
00:26:00,920 --> 00:26:08,920
So any kind of memoization makes a lot of sense, not just with Fireproof, but with any of these content address databases.


228
00:26:08,920 --> 00:26:19,920
So, yeah, the other really interesting one, when the network is the database, is edge creation of IoT data.


229
00:26:19,920 --> 00:26:29,920
So like one of the use cases I explained to my kids is, what about the, you know, the point of sale at all the Burgerville drive-thrus?


230
00:26:29,920 --> 00:26:37,920
Right. So they have all kinds of analytics that they're keeping about, like, which products are selling which time of day and whatnot.


231
00:26:37,920 --> 00:26:47,920
Why not put that all locally and then use the database as the network to push the predicates down to the edge devices


232
00:26:47,920 --> 00:26:54,920
and get the trends and stuff reported back? People are using Couchbase Mobile for that on wind turbines and stuff,


233
00:26:54,920 --> 00:26:58,920
and content address databases are a better fit.


234
00:26:58,920 --> 00:27:06,920
So the last one that's good for the people in this room, and something I want to talk about with the other database creators here,


235
00:27:06,920 --> 00:27:10,920
is how we can do some of the mix and match.


236
00:27:10,920 --> 00:27:17,920
So let's say you laid down a 32 gigabyte Filecoin slab, and in the cold data you'd already pre-indexed it,


237
00:27:17,920 --> 00:27:22,920
and some kind of Merkle tree, it doesn't have to be a Prolly tree, I don't know.


238
00:27:22,920 --> 00:27:31,920
Then put a Fireproof UI on top of that cold data, let people remix, reorder the playlist, whatever,


239
00:27:31,920 --> 00:27:40,920
pick their favorite media, tag things, and then save that back to warm storage that doesn't have to duplicate any of the stuff that was in cold storage.


240
00:27:40,920 --> 00:27:53,920
So there's a lot of ability to take cold data, archived data, and Filecoin, and work with it just like it's fast without having to do copies.


241
00:27:53,920 --> 00:28:03,920
And that's going to be the same for all of these databases, but I think Fireproof especially, because it runs locally in the browser.


242
00:28:03,920 --> 00:28:07,920
So, yeah, it's got all the attributes that we like.


243
00:28:07,920 --> 00:28:16,920
One of my favorites is fork by default, I think that's going to help end users understand some of the value of owning their own data.


244
00:28:16,920 --> 00:28:27,920
And the other fun part, so I started this project in like February, and it's basically been like 14 hour days hacking out,


245
00:28:27,920 --> 00:28:35,920
because I want to make something that could be the most commercial database in the IPFS community.


246
00:28:35,920 --> 00:28:49,920
So if y'all like these attributes, and you want to join, then what you get for joining is the database dashboard,


247
00:28:49,920 --> 00:28:52,920
which is open source, you could run your own, but it's all integrated.


248
00:28:52,920 --> 00:29:01,920
And then you get inside of kind of like the private forums and whatnot.


249
00:29:01,920 --> 00:29:06,920
So it's a dollar a month, and that pays for your first dollar of meter.


250
00:29:06,920 --> 00:29:12,920
And I made a European price just for this conference.


251
00:29:12,920 --> 00:29:19,920
But yeah, the fun part about that is that the dollar a month gets you membership into the forums where people are asking questions


252
00:29:19,920 --> 00:29:21,920
and doing open source contributions and whatnot.


253
00:29:21,920 --> 00:29:33,920
But it also, although I don't have any metered services activated yet, when the meter starts to run, you know, the first dollar is paid.


254
00:29:33,920 --> 00:29:40,920
So that allows you to experiment with apps, and probably most apps, like experimental size apps, are going to run for less than a dollar a month.


255
00:29:40,920 --> 00:29:44,920
So it gets you in the door and lets you have fun.


256
00:29:44,920 --> 00:29:51,920
So thanks a ton, super excited to have a bunch of Fireproof users.


257
00:29:51,920 --> 00:29:57,920
Very cool.


258
00:29:57,920 --> 00:30:06,920
The actual like payloads, the diffs that go into the Merkle clocks, would you say like you're kind of like a, you're making a CRDT structure


259
00:30:06,920 --> 00:30:13,920
that's basically like a CRDT of the Prolly tree, because the diffs of the Prolly tree are like the deltas that go into the Merkle clock?


260
00:30:13,920 --> 00:30:22,920
You could derive those diffs, all the data is there for it, but it's more like the other use case Aaron talked about where you just put a CID in,


261
00:30:22,920 --> 00:30:27,920
and you know, so the thing that's in the clock is just hanging the whole next Merkle tree off of there.


262
00:30:27,920 --> 00:30:31,920
So I don't do any diffing operations when I'm running.


263
00:30:31,920 --> 00:30:42,920
The closest thing to like a diff resolution is that when you come to do a read, any writes that haven't been processed yet that are in,


264
00:30:42,920 --> 00:30:48,920
what we're gonna do is, you know, those writes might be sitting in parallel next to each other I guess,


265
00:30:48,920 --> 00:30:52,920
and so merge all those together until you have one root, and then you read off of that,


266
00:30:52,920 --> 00:30:55,920
and that's when we would do the conflict detection and stuff.


267
00:30:55,920 --> 00:31:03,920
Awesome, and you said during the last talk, during one of your questions, that it's like JSON based,


268
00:31:03,920 --> 00:31:07,920
so when you make an update, when I do like you know, document.update or whatever,


269
00:31:07,920 --> 00:31:14,920
is it a patch similar to what Aaron was talking about, ceramic, is it some other structure that's being like encoded?


270
00:31:14,920 --> 00:31:20,920
This is a super good question, no, it's like, it's just a big old JSON stringifier, it doesn't care.


271
00:31:20,920 --> 00:31:21,920
For every update?


272
00:31:21,920 --> 00:31:22,920
What's that?


273
00:31:22,920 --> 00:31:23,920
For every update?


274
00:31:23,920 --> 00:31:30,920
Yeah, if you, I mean this is the same, in Apache CouchDB and Couchbase, I went through this and decided not to patch,


275
00:31:30,920 --> 00:31:34,920
like it's just, that's not where the problem lives.


276
00:31:34,920 --> 00:31:38,920
It might be that I could get a little bit more compact with some of that on the browser,


277
00:31:38,920 --> 00:31:43,920
so it might be worth doing optimizations, it also hasn't really been optimization time yet,


278
00:31:43,920 --> 00:31:51,920
but the thing that NoSQL in general optimizes for is making it so the thing that's at the end of the get or the put


279
00:31:51,920 --> 00:31:57,920
is already got all the data locality done, right, so if you start fracturing that, then you lose some of that benefit.


280
00:31:57,920 --> 00:31:58,920
All right, makes sense.


281
00:31:58,920 --> 00:32:05,920
Yeah, so what's in the database is each document lives underneath an ID,


282
00:32:05,920 --> 00:32:09,920
and so you look, they live in the poly tree under the ID,


283
00:32:09,920 --> 00:32:14,920
and then anything that's indexed lives in the index tree under its index key.


284
00:32:14,920 --> 00:32:23,920
Anytime you do a document update, so Apache CouchDB used multiversion concurrency control


285
00:32:23,920 --> 00:32:28,920
with like optimistic concurrency token that was required,


286
00:32:28,920 --> 00:32:31,920
and that's a really good way to just have all your users use Mongo instead,


287
00:32:31,920 --> 00:32:37,920
so what we did in Fireproof was make the concurrency control token optional,


288
00:32:37,920 --> 00:32:42,920
and that means that your update is gonna be uncontrolled,


289
00:32:42,920 --> 00:32:46,920
you're just gonna update the last write wins,


290
00:32:46,920 --> 00:32:51,920
but if you want to and you put that optional MVCC token into your update,


291
00:32:51,920 --> 00:32:55,920
then it'll error on you if somebody else has messed with the document since then.


292
00:32:55,920 --> 00:32:58,920
Yeah, is there a rap video to go with this launch?


293
00:32:58,920 --> 00:33:02,920
That's definitely apropos.


294
00:33:02,920 --> 00:33:04,920
Oh, how does it go?


295
00:33:04,920 --> 00:33:13,920
Bacon, lettuce, bacon, lettuce, avocado, and tomato,


296
00:33:13,920 --> 00:33:18,920
you don't need servers when you blotch your data with this JavaScript code in your browser,


297
00:33:18,920 --> 00:33:23,920
your users get rowdy, your app gets louder when you got data blat,


298
00:33:23,920 --> 00:33:26,920
when you want to do data blat, that was back from when I was calling it blat,


299
00:33:26,920 --> 00:33:29,920
I think it's better calling it Fireproof.


300
00:33:29,920 --> 00:33:36,920
That wasn't actually supposed to be a question, but I appreciate the answer.


301
00:33:36,920 --> 00:33:39,920
I use CouchDB.


302
00:33:39,920 --> 00:33:46,920
So, you know, we've talked a lot about replication and conflicts,


303
00:33:46,920 --> 00:33:49,920
and in this case calling it sync,


304
00:33:49,920 --> 00:33:54,920
but there's cases where you need to push that up into the application layer,


305
00:33:54,920 --> 00:33:59,920
and there's other cases where if you push it up into the application layer, it's unknowable.


306
00:33:59,920 --> 00:34:03,920
And so, you know, what do we do in those cases?


307
00:34:03,920 --> 00:34:09,920
We had an app on CouchDB that was for cable plant documentation,


308
00:34:09,920 --> 00:34:15,920
and if we had a conflict, the answer was you send a technician back out into the head end


309
00:34:15,920 --> 00:34:19,920
and see where that cable actually went.


310
00:34:19,920 --> 00:34:25,920
So I think comparing it with Git probably isn't a good idea in general,


311
00:34:25,920 --> 00:34:31,920
because normally we know what to do in Git or can figure it out eventually,


312
00:34:31,920 --> 00:34:35,920
but the application user may not.


313
00:34:35,920 --> 00:34:41,920
So are there strategies for avoiding that, you know, not having the lock, obviously,


314
00:34:41,920 --> 00:34:46,920
is one way of doing it, going to last write.


315
00:34:46,920 --> 00:34:50,920
Might have worked in CouchDB because they would have had documentation


316
00:34:50,920 --> 00:34:56,920
and eventually known that it was wrong, but how do we avoid getting there to begin with?


317
00:34:56,920 --> 00:34:57,920
That's a super good question.


318
00:34:57,920 --> 00:35:02,920
I mean, I think maybe what Aaron was saying in the first talk about having tunable, you know,


319
00:35:02,920 --> 00:35:05,920
choices for when you need that strong consistency.


320
00:35:05,920 --> 00:35:11,920
So in the real world use case you talked about, like imagining writing that app on Couch,


321
00:35:11,920 --> 00:35:17,920
I could see how everything's great and then wouldn't it be cool if then you had a barrier you could throw,


322
00:35:17,920 --> 00:35:22,920
right, before you rolled truck back away or something, you know?


323
00:35:22,920 --> 00:35:30,920
And so we don't have any notion of that in Fireproof, but aside from having that transaction boundary,


324
00:35:30,920 --> 00:35:34,920
which we could maybe throw additional guarantees on in the future.


325
00:35:34,920 --> 00:35:41,920
But I think maybe the more power is going to come out of tools for inspecting the diffs


326
00:35:41,920 --> 00:35:44,920
and understanding the context of conflicts.


327
00:35:44,920 --> 00:35:49,920
I mean, on the assumption that as the database vendor I can't make people not write conflicts,


328
00:35:49,920 --> 00:35:52,920
then the best thing I can do is make it easier for them when that happens.


329
00:35:52,920 --> 00:35:55,920
And so in Couch we would never throw any data away,


330
00:35:55,920 --> 00:35:59,920
but it wasn't like super well documented what to do in those contexts.


331
00:35:59,920 --> 00:36:09,920
So, you know, I don't know that there's anything inherently better or fixed about the current situation,


332
00:36:09,920 --> 00:36:13,920
but I do think because of the way that the trees interact and everything being immutable,


333
00:36:13,920 --> 00:36:19,920
you're way more likely to be able to kind of recreate that context,


334
00:36:19,920 --> 00:36:23,920
even if maybe you have to do the work to stand, you know.


335
00:36:23,920 --> 00:36:25,920
So Fireproof has a snapshot feature.


336
00:36:25,920 --> 00:36:28,920
You can give it a historical clock and it'll run from back then.


337
00:36:28,920 --> 00:36:31,920
So you could go ask, you know, when you notice the conflict,


338
00:36:31,920 --> 00:36:35,920
you could go query the whole database at a snapshot of when that conflict occurred


339
00:36:35,920 --> 00:36:38,920
and give you some opportunity to do some auto merging.


340
00:36:38,920 --> 00:36:42,920
For the transactions, are you using a car v1 or a v2?


341
00:36:42,920 --> 00:36:44,920
V2, I think.


342
00:36:44,920 --> 00:36:48,920
Do you use like the index stuff at all at the tail end?


343
00:36:48,920 --> 00:36:52,920
I don't need to yet, but the encryption will.


344
00:36:52,920 --> 00:36:58,920
So like the encrypted blocks do, I guess, but I have a clear index in my index DB


345
00:36:58,920 --> 00:37:04,920
of from basically from clear CID to encrypted car file.


346
00:37:04,920 --> 00:37:07,920
So you can use those car indexes to rebuild.


347
00:37:07,920 --> 00:37:08,920
You have to decrypt it first.


348
00:37:08,920 --> 00:37:12,920
Like basically you have an index that tells you what the encrypted blocks are


349
00:37:12,920 --> 00:37:16,920
and then you have to use that to rehydrate the CID set of the decrypted blocks.


350
00:37:16,920 --> 00:37:22,920
So the blocks and the, like looking at the perspective from the Merkle clock,


351
00:37:22,920 --> 00:37:27,920
would you say like the head is like the encrypted block or is the head the,


352
00:37:27,920 --> 00:37:30,920
like the CID of the plain text block?


353
00:37:30,920 --> 00:37:33,920
It's strictly the plain text stuff.


354
00:37:33,920 --> 00:37:38,920
So like everything that is semantic about the database lives above that line, right,


355
00:37:38,920 --> 00:37:41,920
and the encryption is just part of the storage engine.


356
00:37:41,920 --> 00:37:45,920
So, you know, that's why it makes sense to exchange the clear blocks


357
00:37:45,920 --> 00:37:47,920
when you're in a trusted work group.


358
00:37:47,920 --> 00:37:52,920
So then like if you receive an encrypted block, it's not until after you decrypt it


359
00:37:52,920 --> 00:37:55,920
and process it do you know if it's actually like a valid block in this context?


360
00:37:55,920 --> 00:37:56,920
Yeah, totally.


361
00:37:56,920 --> 00:37:58,920
Yeah, the encryption is opaque from the outside.


362
00:37:58,920 --> 00:37:59,920
Okay.


363
00:37:59,920 --> 00:38:03,920
Which means you need to keep a secondary manifest of the car files you need and all that.


364
00:38:03,920 --> 00:38:04,920
All right.


365
00:38:04,920 --> 00:38:08,920
If there's no more questions, we should all start calling ourselves Cloudless.


366
00:38:08,920 --> 00:38:09,920
I've been market testing it.


367
00:38:09,920 --> 00:38:11,920
It works.


368
00:38:11,920 --> 00:38:17,920
Thank you.
