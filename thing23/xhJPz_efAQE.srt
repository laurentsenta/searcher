1
00:00:00,000 --> 00:00:07,000
Hi, I'm Lajdo, I'm part of IPFS stewards group at Protocol Labs. This talk will be different,


2
00:00:10,720 --> 00:00:17,720
we've been talking about Project RIA and big gateways, I'll be talking about taking tools


3
00:00:18,680 --> 00:00:24,480
and specs and libraries we've built, continue building for the Project RIA and using that


4
00:00:24,480 --> 00:00:31,480
for self-hosting. So I started this talk with one slide and I was like thinking, could I


5
00:00:31,640 --> 00:00:38,640
just stop here and then talk for 30 minutes and then I realized there's no point in showing


6
00:00:40,200 --> 00:00:47,200
people, I mean if you don't have time, that's the gist of the talk, but then you don't understand


7
00:00:47,720 --> 00:00:53,200
how important self-hosting is without some historical context. So majority of this talk


8
00:00:53,200 --> 00:00:59,480
will be talking about what gateways are, what types of gateway we have, what types of gateway


9
00:00:59,480 --> 00:01:05,040
functionalities exist on IPFS-IO and dweb.link gateways, those are public gateways provided


10
00:01:05,040 --> 00:01:10,780
by Protocol Labs, and then what's the future of that specific infrastructure, what type


11
00:01:10,780 --> 00:01:16,960
of services we are building to enable that, and then we'll zoom in to the Bifrost gateway,


12
00:01:16,960 --> 00:01:23,320
which is the first element of those services, and at the end, with that knowledge, we'll


13
00:01:23,320 --> 00:01:30,320
quickly go over how to do self-hosting and there will be a fake demo which will work.


14
00:01:33,480 --> 00:01:40,480
What IPFS is, that's a question we don't want to answer and the lowest common denominator


15
00:01:40,480 --> 00:01:46,920
that we all agreed on was that, oh yeah, we work with CIDs and we use them for operations


16
00:01:46,920 --> 00:01:53,400
like retrieval in the case of gateways, and to say this is like IPFS implementation, you


17
00:01:53,400 --> 00:02:00,400
need to actually verify the data you are making operations with is matching the CID. So what


18
00:02:00,400 --> 00:02:07,400
gateway is, it's IPFS implementation itself, it works with CIDs, it verifies them and then


19
00:02:13,040 --> 00:02:20,040
returns some sort of a representation to the client, and the client itself could be still


20
00:02:20,040 --> 00:02:27,040
doing IPFS if it's using trustless gateways and verifying hashes, or it does just regular


21
00:02:27,040 --> 00:02:34,040
HTTP, it no longer does IPFS, it's working with data that was loaded through IPFS, but


22
00:02:34,400 --> 00:02:41,080
then it delegates trust to the gateway. And just maybe like other framing is that the


23
00:02:41,080 --> 00:02:47,080
difference between trustless and trusted responses. If I want to load a cat picture from some


24
00:02:47,080 --> 00:02:54,080
sub-directory under some CID route, if I'm using trusted responses in the browser, I


25
00:02:54,080 --> 00:03:01,080
want the thing to render, I'm delegating trust to the gateway, I ask gateway for that path


26
00:03:01,200 --> 00:03:06,760
and I get the bytes, they most likely are the cat, but the gateway could lie and return


27
00:03:06,760 --> 00:03:13,080
something else. The trustless response is that I get all the blocks for entire path,


28
00:03:13,080 --> 00:03:18,160
not only for all the blocks, entire DAG for the cat, but I also get blocks for the parent


29
00:03:18,160 --> 00:03:23,040
directory and the top level CID, and we need all the blocks because I don't know what the


30
00:03:23,040 --> 00:03:29,200
CID of the directory or a cat, I need to learn that from the parent and I only have the top


31
00:03:29,200 --> 00:03:36,200
level parent CID. So with that in mind, like the trusted responses are extremely important


32
00:03:37,760 --> 00:03:44,760
because that's how the most prolific runtime on the planet works in web browsers. And there's


33
00:03:44,760 --> 00:03:51,760
nothing wrong with using trusted responses as long as you trust the gateway. And like


34
00:03:53,840 --> 00:03:58,000
depends on the person, I personally only trust the gateway that I run on my own machine on


35
00:03:58,000 --> 00:04:05,000
the local host. And you can see this is a Wikipedia mirror loaded from my local machine.


36
00:04:06,920 --> 00:04:12,680
And it's renders in the browser, it's like the verification happened still on my machine,


37
00:04:12,680 --> 00:04:18,120
but it's returned the serialized data to the browser and browser rendered that. And that's


38
00:04:18,120 --> 00:04:25,120
how Brave works with a bit better UX. And now, a year after the last IPFS thing, we


39
00:04:27,440 --> 00:04:34,440
no longer, we not only have just the serialized files, we also can request the serialized


40
00:04:34,440 --> 00:04:43,440
arbitrary IPD data model objects as like JSON or like CBOR. You can also like fetch arbitrary


41
00:04:43,640 --> 00:04:50,640
directory trees of UnixFS as a tar stream that was recently, that recently shipped.


42
00:04:53,800 --> 00:04:58,200
And what changed since last IPFS thing when I talked about gateways is the trustless verifiable


43
00:04:58,200 --> 00:05:04,480
responses are now possible end to end. We not only have them, we had them for blocks and


44
00:05:04,480 --> 00:05:11,480
cars, but now since Kubo 19, it's possible also to request a sign IPNS record from the


45
00:05:13,400 --> 00:05:18,440
gateway and verify it on the client. And that gives us, if you are using IPNS records with


46
00:05:18,440 --> 00:05:23,400
cryptographic identifiers, you get end to end verifiability across both immutable and


47
00:05:23,400 --> 00:05:30,000
immutable namespace as long as you don't use DNS link. Okay, so with that in mind, how


48
00:05:30,000 --> 00:05:37,000
gateways on IPFS.io and dweb.link work today. It's Kubo based. So this is a very simplified


49
00:05:37,000 --> 00:05:44,000
way of looking at the gateway at IPFS.io. Both IPFS.io and dweb.link are hosted on the


50
00:05:55,360 --> 00:05:59,920
same infrastructure. The difference is the host header. So you can see at the very bottom


51
00:05:59,920 --> 00:06:05,440
of the left side, the subdomain on dweb.link, the only difference is that the request have


52
00:06:05,440 --> 00:06:12,120
CID in the host header. The gateways are very powerful and they support multiple requests


53
00:06:12,120 --> 00:06:18,560
and response types. Some are trusted, some are trustless, some are more or less expensive.


54
00:06:18,560 --> 00:06:25,320
But at the end of the day, the request goes to our load balancers and DLS termination


55
00:06:25,320 --> 00:06:32,320
and there's a get request for a specific content path. And all that goes to the Kubo, Kubo's


56
00:06:32,320 --> 00:06:38,280
multiple Kubo instances and then Kubo either has the data in the local block store or it


57
00:06:38,280 --> 00:06:45,280
will reach out to the IPFS swarm to retrieve that data. And that's a lot of work and it's


58
00:06:48,080 --> 00:06:55,080
also a lot of challenges when it comes to scaling, adjusting costs. By the fact that


59
00:06:55,080 --> 00:07:02,080
both HTTP, purely HTTP interface is on the left side, we attempted to use HTTP scaling


60
00:07:05,400 --> 00:07:09,880
tools and best practices for scaling Kubo. At the same time, on the right side, we have


61
00:07:09,880 --> 00:07:16,440
peer-to-peer protocols which are not HTTP, mainly BitSwap. Majority of IPFS peers use


62
00:07:16,440 --> 00:07:25,640
BitSwap and that poses challenges around scaling and trading costs. So often you have provider,


63
00:07:25,640 --> 00:07:31,360
you are able to like deploy and optimize costs around HTTP as long as you don't use other


64
00:07:31,360 --> 00:07:38,360
protocols. Here we are joined at the hip and we may have a good opportunity to reduce costs


65
00:07:39,640 --> 00:07:44,440
around HTTP, but at the same time, we don't really control how much cost will be eaten


66
00:07:44,440 --> 00:07:51,440
by BitSwap. Often those two services could be potentially scaled to be like less expensive


67
00:07:52,080 --> 00:07:59,080
even if we had like two services instead of one. So that's the direction we are moving


68
00:08:01,320 --> 00:08:08,320
into towards compassible services. So Project RIA is an example of that and we are using


69
00:08:08,320 --> 00:08:14,320
that as an opportunity to split Kubo into those separate services. So you can see in


70
00:08:14,320 --> 00:08:19,920
the place where we had Kubo and the swarm, now we have Bifrost Gateway, which the rest


71
00:08:19,920 --> 00:08:26,920
of the stock will be about, and then the Bifrost Gateway on both ends speaks HTTP. So that's


72
00:08:27,120 --> 00:08:32,600
a very important takeaway from this talk is that the only thing you need to worry when


73
00:08:32,600 --> 00:08:39,600
it comes to Bifrost Gateway is how to manage HTTP service, how to scale it. And there's


74
00:08:39,600 --> 00:08:44,560
like a lot of people with expertise around that, a lot of services, very cost effective


75
00:08:44,560 --> 00:08:51,560
services for optimizing caching on both ends. You can put a load balancer, HTTP cache solutions


76
00:08:53,680 --> 00:09:00,680
on both ends because you may be optimizing for content path caching or you may also be


77
00:09:00,680 --> 00:09:07,680
caching or you may also want to leverage frequency of request for popular blocks. And then the


78
00:09:10,760 --> 00:09:17,760
blocks are fetched from some trustless gateway, in this case it's Saturn, and then content


79
00:09:19,360 --> 00:09:25,440
routing responsibilities, the data transfer are on the right side. And like the Project


80
00:09:25,440 --> 00:09:30,760
CREA is still a work in progress, and in general the idea is that the data will slowly trickle


81
00:09:30,760 --> 00:09:37,760
down towards the Bifrost Gateway. And we are still trying to understand what's happening


82
00:09:37,760 --> 00:09:44,760
there, but in general we are trying to avoid cycles like that. But maybe just to zoom in


83
00:09:47,360 --> 00:09:52,560
into the right side of the thing, you can see at the top there's a Kubo which did all


84
00:09:52,560 --> 00:09:59,560
the things, and on the bottom we got Bifrost Gateway which takes over some responsibilities


85
00:10:01,360 --> 00:10:08,080
while other responsibilities are moved down to the right. So for example, the request


86
00:10:08,080 --> 00:10:14,760
for the HTTP content path is still handled by the service. It's actually using the same


87
00:10:14,760 --> 00:10:22,760
code as Kubo because we extracted the gateway code into box.so.slash gateway library. So


88
00:10:25,560 --> 00:10:30,400
the risk of divergence is very low because we have multiple compliance test suite, we


89
00:10:30,400 --> 00:10:35,720
are growing that. There's a talk later today I believe, or later, yeah, by Piotr about


90
00:10:35,720 --> 00:10:42,720
gateway compliance test suite. Both Kubo and Bifrost Gateway run the same tests. And the


91
00:10:42,720 --> 00:10:48,160
DAD and DAG verification and the serial, optional deserialization also happens on Bifrost Gateway.


92
00:10:48,160 --> 00:10:53,040
But then the data transfer responsibilities, this like the biggest, the fattest arrow,


93
00:10:53,040 --> 00:10:57,840
now is moved to the trustless gateway, which in Project Rhea is Saturn CDN, but as you


94
00:10:57,840 --> 00:11:04,840
will see later in this talk, it could be something else. And content routing is pushed even further


95
00:11:04,840 --> 00:11:11,840
beyond even that trustless gateway, because you may have like a multiple points of, you


96
00:11:15,600 --> 00:11:20,640
may want to scale all those boxes here are just like a single box, but in reality, those


97
00:11:20,640 --> 00:11:27,640
are like multiple machines and they may require different strategies for scaling. So running


98
00:11:27,640 --> 00:11:34,640
the same, like running content routing for the same, the most popular CID over and over


99
00:11:34,680 --> 00:11:41,200
again, traversing DHT, in the case when you effectively have a single gateway product


100
00:11:41,200 --> 00:11:46,760
like IPFSIO, it's not very cost effective. And by introducing a caching layer in form


101
00:11:46,760 --> 00:11:53,640
of IPNI in front of DHT and sharing that across all the trustless gateway instances, we do


102
00:11:53,640 --> 00:12:00,640
that resolution only once and then subsequent requests don't need to wait for DHT results.


103
00:12:02,720 --> 00:12:09,720
So we have additional, we have essentially like layers of proxy in front of the thing


104
00:12:09,840 --> 00:12:15,240
that we had in the past. The trustless gateway takes care of retrieval and provides caching


105
00:12:15,240 --> 00:12:21,640
for that. And IPNI, I would argue, provides the similar type of caching, but for content


106
00:12:21,640 --> 00:12:28,640
routing results. And the difference here is also now we can use the best block DAG transfer


107
00:12:30,760 --> 00:12:36,120
protocol for retrieving data from peers. Those could be Kubernetes nodes, but it could be


108
00:12:36,120 --> 00:12:41,360
also like elastic IPFS. It could be something else, could be IRO with the new novel transfer


109
00:12:41,360 --> 00:12:48,360
protocol. The backend, which is no longer part of Bypass Gateway can take care of retrieval.


110
00:12:48,360 --> 00:12:55,360
And finally, going back to the Bypass Gateway itself. So we are like zooming into the very


111
00:12:55,720 --> 00:13:02,720
first block on this infrastructure. The Bypass Gateway as a project has, we made certain


112
00:13:07,880 --> 00:13:14,880
design decisions and identified responsibilities, which responsibilities should be local and


113
00:13:14,880 --> 00:13:21,880
which ones should be delegated. So namely the main design decision was to not have peer-to-peer


114
00:13:23,800 --> 00:13:28,760
stack, which may be controversial, but at the same time, you can still have that peer-to-peer


115
00:13:28,760 --> 00:13:35,760
stack. You just run it as a separate service, which you can scale independently. So what's


116
00:13:35,760 --> 00:13:42,760
local? Essentially, HTTP daemon provides HTTP services. It's responsible for verifying that


117
00:13:46,360 --> 00:13:53,000
blocks the data that it processes matches CID. And effectively, the only API that we


118
00:13:53,000 --> 00:14:00,000
are committed to supporting in this project are the official HTTP gateway specs. And we


119
00:14:00,000 --> 00:14:07,000
use that using the reference implementation in Boxer Gateway library. And to do that,


120
00:14:08,200 --> 00:14:15,200
we also support mutable namespaces for DNS link and IPNS. For that, we reuse names from


121
00:14:15,200 --> 00:14:21,480
Boxer as well. And we have a minimal set of IPLD for the most popular types of data in


122
00:14:21,480 --> 00:14:28,480
the wild. And probably we'll evolve that approach, but for now, that's UnixFS. Some web things


123
00:14:28,480 --> 00:14:35,480
which I'll describe later, and some Duxy support. And then content routing, we have IPIP and


124
00:14:37,200 --> 00:14:42,200
there are other IPIPs to delegate even more content routing needs. It's delegated to the


125
00:14:42,200 --> 00:14:49,200
routing V1 API, which is provided by IPNI. And the data transfer storage, that's provided


126
00:14:49,760 --> 00:14:56,760
by some trustless gateway spec. In project, we have that starter. So I tried to draw the


127
00:14:56,760 --> 00:15:03,760
boxes that are inside of Bifrost Gateway. And then I realized probably in a month or


128
00:15:05,840 --> 00:15:11,240
two, those boxes will be different. We'll probably rename them and the stock will be


129
00:15:11,240 --> 00:15:18,240
useless. So maybe I created a responsibility map, which is not the architectural design.


130
00:15:18,240 --> 00:15:25,240
It's more about things that Bifrost Gateway has to do, to do its job. So you can see the


131
00:15:28,000 --> 00:15:33,320
biggest one, probably the most complexity is around resolving content paths. Because


132
00:15:33,320 --> 00:15:39,280
we have a request for a specific file on some path and we need to know how to traverse that.


133
00:15:39,280 --> 00:15:46,280
If it's mutable, we need resolving through names. If it's DNS link, maybe it's DNS. If


134
00:15:46,280 --> 00:15:53,280
it's, is it a single block or is it a directory? Going through XFS or maybe going through IPLD


135
00:15:54,240 --> 00:16:00,760
data model. That, you would think that's the end, but no. When it comes to the gateway,


136
00:16:00,760 --> 00:16:07,000
we also have things around web. And those things are, if I open a directory, it's not


137
00:16:07,000 --> 00:16:11,240
a file, the gateway needs to generate, if it's a request comes from a browser, it needs


138
00:16:11,240 --> 00:16:17,000
to generate HTML directory listing. If there is a special CID or maybe DNS link name in


139
00:16:17,000 --> 00:16:21,560
the host header, I also need to normalize that and update the content path. If I request


140
00:16:21,560 --> 00:16:26,800
a directory and there's index HTML, I return index HTML because that's how most of websites


141
00:16:26,800 --> 00:16:33,360
work. If there's HTTP range request, I don't want to fetch entire Wikipedia archive, which


142
00:16:33,360 --> 00:16:39,240
is 80 gigabytes. I want a specific sliver of that. And finally, like how do I handle


143
00:16:39,240 --> 00:16:44,360
things like redirects? So all those things are provided by Boxo library, but I mentioned


144
00:16:44,360 --> 00:16:51,360
it here because a majority of the work happened here. It lives in a Boxo gateway, but it was


145
00:16:51,800 --> 00:16:58,800
part of this project to extract it and make it generic enough. And then this purple thing


146
00:16:58,800 --> 00:17:04,080
on the right bottom, it's effectively the interface that the Boxo gateway requires a


147
00:17:04,080 --> 00:17:09,240
Bifrost gateway to implement. And we have two implementations. The default is the block backend,


148
00:17:09,240 --> 00:17:16,240
which will fetch things block by block. But we are working on graph backend, which will


149
00:17:16,320 --> 00:17:23,320
reduce the number of round trips. The greenish branches are how we handle storage. There's


150
00:17:26,200 --> 00:17:32,200
no physical storage in Bifrost gateway. We have in-memory ephemeral cache. And then we


151
00:17:32,200 --> 00:17:38,040
have exchange backend, which is responsible for fetching blocks or bugs of blocks, either


152
00:17:38,040 --> 00:17:44,760
using a single HTTP request for specific single gateway. Or in case of Saturn, we have Caboose,


153
00:17:44,760 --> 00:17:51,440
which I believe the talk is right after mine. So you will hear more about that. So Caboose


154
00:17:51,440 --> 00:17:58,440
is what we use for graph backend experimentation in Project Saturn. But for now, if you run


155
00:17:58,440 --> 00:18:05,440
the default settings, that will be block backend with plain HTTP fetch. And finally, a lot


156
00:18:08,080 --> 00:18:15,080
of work went into adding instrumentation around metrics and telemetry. And I have slides about


157
00:18:15,080 --> 00:18:22,080
that later. So maybe like going from this, if you want to have what's the most important


158
00:18:23,760 --> 00:18:28,400
part of this? I think the most important part is that the Bifrost gateway, if you want to


159
00:18:28,400 --> 00:18:35,400
like summarize it in one sentence, it's like a verifier service of an untrusted remote


160
00:18:35,400 --> 00:18:42,400
block store. Everything else is like implementation detail. You may not want to use the serialized


161
00:18:43,280 --> 00:18:48,240
responses and you may want to request block by block, or maybe you want to request a car


162
00:18:48,240 --> 00:18:54,400
stream and you only have a block store which supports block by block, and you don't want


163
00:18:54,400 --> 00:18:59,880
to pay the verification cost on the client. The Bifrost gateway will act as a verifier


164
00:18:59,880 --> 00:19:06,880
for you. And finally, that's where we get to the self-hosting. The self-hosting today,


165
00:19:11,520 --> 00:19:18,520
I think this is the first slide, right? We provide the Docker images for Intel architecture


166
00:19:18,520 --> 00:19:25,160
and ARM, so you can run it on the Raspberry Pi. There are tags for releases, and there


167
00:19:25,160 --> 00:19:29,920
are also like developer builds for every stable commit in the main branch. And I suggest using


168
00:19:29,920 --> 00:19:36,920
the main latest for now, because it's a very early preview of this project. How the project


169
00:19:38,440 --> 00:19:45,440
looks like? You run Bifrost gateway and you configure it by passing environment variables.


170
00:19:45,440 --> 00:19:52,440
The only one that you actually need to pass is the URL of the block store. In this case,


171
00:19:54,440 --> 00:20:01,440
I'm using something untrusted. Bifrost gateway will start, and it will expose the block store


172
00:20:01,000 --> 00:20:18,000
etrusted endpoint, which will return you a verified responses. All the response types that currently are supported on IPSIO, those response types are supported here as well. And you've got some test links that you can click to verify it works as expected.


173
00:20:18,000 --> 00:20:26,040
Yeah, so there's a demo which I totally pre-recorded using Screenshot Tool.


174
00:20:26,040 --> 00:20:37,200
So I run the latest Bifrost gateway build against IPFS.io because IPFS.io supports trustless


175
00:20:37,200 --> 00:20:38,200
responses.


176
00:20:38,200 --> 00:20:44,440
And I click the very first smoke test link for a single JPEG.


177
00:20:44,440 --> 00:20:50,240
And that's a good test because it's a small file, so it's a single block.


178
00:20:50,240 --> 00:20:51,240
There are no paths.


179
00:20:51,240 --> 00:20:53,040
So I effectively should...


180
00:20:53,040 --> 00:20:55,600
It should translate to a request for a single block.


181
00:20:55,600 --> 00:20:59,120
And that's what you see on the last line.


182
00:20:59,120 --> 00:21:03,800
The request to the local gateway got translated to a row block request to the gateway, remote


183
00:21:03,800 --> 00:21:06,040
gateway.


184
00:21:06,040 --> 00:21:07,320
The hash was verified.


185
00:21:07,320 --> 00:21:14,080
And then the response, the serverless response was returned and loaded in my browser.


186
00:21:14,080 --> 00:21:19,080
Yeah, so it's local verifier of remote untrusted gateway.


187
00:21:19,080 --> 00:21:25,160
And it goes beyond immutable identifiers because one of the...


188
00:21:25,160 --> 00:21:29,980
The second example is a Wikipedia mirror, which is a very good test case.


189
00:21:29,980 --> 00:21:33,800
It has both DNS link, immutable pointer.


190
00:21:33,800 --> 00:21:38,160
It also has a very huge directory, which has 20 million of files.


191
00:21:38,160 --> 00:21:40,480
And there's an index HTML inside.


192
00:21:40,480 --> 00:21:47,620
So if you did not implement things correctly, it will not load immediately, right?


193
00:21:47,620 --> 00:21:49,660
So it takes care of resolution.


194
00:21:49,660 --> 00:21:51,060
It takes care of verification.


195
00:21:51,060 --> 00:21:52,840
And then it takes care of deserialization.


196
00:21:52,840 --> 00:21:58,760
And at the end, you end up with a gateway on your local host machine.


197
00:21:58,760 --> 00:22:06,720
And finally, for people who want to start thinking about running this, it's very, very


198
00:22:06,720 --> 00:22:07,720
easy.


199
00:22:07,720 --> 00:22:14,200
For self-hosting, you have a very small set of things that you want to adjust.


200
00:22:14,200 --> 00:22:19,120
Aside from the endpoint URL, you may want to adjust the size of the in-memory block


201
00:22:19,120 --> 00:22:20,520
cache.


202
00:22:20,520 --> 00:22:22,480
You may experiment with the graph backend.


203
00:22:22,480 --> 00:22:28,080
But right now, it's not supported on the public gateways yet.


204
00:22:28,080 --> 00:22:30,800
And we are still writing the specification for it.


205
00:22:30,800 --> 00:22:38,120
So I think self-hosting and sections related to testing, tracing, and self-diagnostics


206
00:22:38,120 --> 00:22:40,320
are the most important ones.


207
00:22:40,320 --> 00:22:47,960
If you are interested in Saturn CDN, and there will be a talk about Caboose after this one,


208
00:22:47,960 --> 00:22:50,440
there's built-in support for Caboose.


209
00:22:50,440 --> 00:22:57,000
And you can opt in by using Saturn orchestrator URL instead of proxy.


210
00:22:57,000 --> 00:23:02,280
And that will switch the backend into the Caboose.


211
00:23:02,280 --> 00:23:03,760
So how to think about self-hosting?


212
00:23:03,760 --> 00:23:09,600
Well, one way of thinking this is if you already run Kubo in your gateway infrastructure, you


213
00:23:09,600 --> 00:23:15,400
probably want to split Kubo and keep Kubo.


214
00:23:15,400 --> 00:23:18,920
You essentially add the Bifrost gateway in front of Kubo.


215
00:23:18,920 --> 00:23:22,800
Kubo no longer provides the gateway functionality.


216
00:23:22,800 --> 00:23:27,840
It provides a trustless block store.


217
00:23:27,840 --> 00:23:30,800
And you scale Bifrost gateway in front of Kubo.


218
00:23:30,800 --> 00:23:34,120
You can introduce caching layer in between them.


219
00:23:34,120 --> 00:23:40,120
And I think it's a nice property being able to slowly migrate your infrastructure from


220
00:23:40,120 --> 00:23:45,200
this old Kubo-only setup into the new one.


221
00:23:45,200 --> 00:23:49,640
Maybe you introduce Bifrost gateway as the first service, and then you can benchmark


222
00:23:49,640 --> 00:23:52,280
it against other things.


223
00:23:52,280 --> 00:24:00,320
You could introduce some LASI nodes, and then you compare, do A-B testing against Kubo versus


224
00:24:00,320 --> 00:24:03,720
LASI without risking your traffic.


225
00:24:03,720 --> 00:24:05,560
You could do that with mirror traffic.


226
00:24:05,560 --> 00:24:10,040
That's what we are doing internally for the project RIA.


227
00:24:10,040 --> 00:24:19,000
And finally, when you do that, you will appreciate how much work went into IPFS Box or SDK.


228
00:24:19,000 --> 00:24:27,760
The Box or gateway library has not only pretty good logging on different levels.


229
00:24:27,760 --> 00:24:34,240
If you enable debug, as I did in my screen-shotted demo, you will see what happens for every


230
00:24:34,240 --> 00:24:39,760
requested path, how it was translated to what request to the backend.


231
00:24:39,760 --> 00:24:44,320
But then if you work at the scale, that's not enough.


232
00:24:44,320 --> 00:24:47,200
And you may look into tracing.


233
00:24:47,200 --> 00:24:55,240
So when you start Bifrost gateway, you get a Prometheus metrics exposed, and you can


234
00:24:55,240 --> 00:24:56,920
build visualization.


235
00:24:56,920 --> 00:25:02,160
This is an example of Grafana board built and showing performance of some internals


236
00:25:02,160 --> 00:25:04,160
from Box or gateway library.


237
00:25:04,160 --> 00:25:11,800
So actually, the same metrics in Kubo and in Bifrost gateway enable you to use the same


238
00:25:11,800 --> 00:25:17,720
boards and monitoring tools to compare A-B performance in between them.


239
00:25:17,720 --> 00:25:22,480
But then metrics are not always showing the full picture.


240
00:25:22,480 --> 00:25:27,840
Sometimes you see the problem, and then you want to zoom in into problems around a specific


241
00:25:27,840 --> 00:25:28,840
request.


242
00:25:28,840 --> 00:25:35,280
And that's where tracing, and specifically trace context support comes handy.


243
00:25:35,280 --> 00:25:42,140
So in the main branch of the project, we already support, I believe, trace parent header.


244
00:25:42,140 --> 00:25:48,760
If you pass the trace ID with the request, we will enable tracing of that request, and


245
00:25:48,760 --> 00:25:55,660
we also forward that header to all the backend services that are used.


246
00:25:55,660 --> 00:26:02,760
So if we had to resolve IPNs record, your indexer will get information about that.


247
00:26:02,760 --> 00:26:09,160
If you did not have that block cached and you had to fetch it from the remote block


248
00:26:09,160 --> 00:26:16,760
store, be that Kubo, Lassie, Saturn, that HTTP endpoint will get a request with this


249
00:26:16,760 --> 00:26:21,080
identifier, and that lets you trace the request across services.


250
00:26:21,080 --> 00:26:28,680
And that has to start on the incoming place where you essentially enable tracing for that


251
00:26:28,680 --> 00:26:29,800
request.


252
00:26:29,800 --> 00:26:35,840
And then there are multiple exporters supported by Box or Gateway Library.


253
00:26:35,840 --> 00:26:42,920
If you go to Box repository, there's a docs tracing document with details.


254
00:26:42,920 --> 00:26:47,920
Probably the most useful one is Jaeger UI, which you can run locally.


255
00:26:47,920 --> 00:26:58,120
And then in the top right, you can see I pasted the trace ID from here, and then I see how


256
00:26:58,120 --> 00:27:01,720
long each stage of processing the request took.


257
00:27:01,720 --> 00:27:07,120
The one with red thing is when I made the request to remote system.


258
00:27:07,120 --> 00:27:12,640
When I was making the screenshot, it did not give me visibility into that, but with this


259
00:27:12,640 --> 00:27:18,940
tooling, you will be able to trace it across systems and see how long local versus delegated


260
00:27:18,940 --> 00:27:21,740
tasks took.


261
00:27:21,740 --> 00:27:31,440
So I'll be wrapping up because I feel I'm running out of time, but finally, the self-hosting


262
00:27:31,440 --> 00:27:37,160
aspect I wanted to convey with this talk is not about migrating your existing infrastructure.


263
00:27:37,160 --> 00:27:43,000
It's more about leveraging the same tools for personal use.


264
00:27:43,000 --> 00:27:50,040
And I would argue that the Bifrost Gateway could act as a user agent, not replacing Kubo,


265
00:27:50,040 --> 00:27:53,600
but filling the niche where Kubo could not be run.


266
00:27:53,600 --> 00:27:59,120
Namely, there's a huge amount of users that were complaining that they would love to use


267
00:27:59,120 --> 00:28:03,240
IPFS, but they are not able to use it on laptop.


268
00:28:03,240 --> 00:28:07,680
They are not able to use it on mobile or IoT because running peer-to-peer all the time


269
00:28:07,680 --> 00:28:11,560
is too expensive, and there was no linear alternative.


270
00:28:11,560 --> 00:28:16,480
And at the same time, they really want to have the integrity guarantee that come with


271
00:28:16,480 --> 00:28:18,240
content addressing.


272
00:28:18,240 --> 00:28:22,280
Then there are aspects of privacy and resiliency.


273
00:28:22,280 --> 00:28:28,440
When it's only HTTP, you can leverage all the tooling that provide more privacy and


274
00:28:28,440 --> 00:28:30,240
resiliency around that.


275
00:28:30,240 --> 00:28:31,760
You could be using...


276
00:28:31,760 --> 00:28:39,080
There could be remote trustless gateway, something that just exposes a single block request for


277
00:28:39,080 --> 00:28:43,760
a specific CID, nothing more, and you could be using that over Tor.


278
00:28:43,760 --> 00:28:51,880
Sure, someone can shut down DNS in your country, someone can tweak with BGP rules, but in many


279
00:28:51,880 --> 00:28:56,600
cases you will still be able to browse resources that are on IPFS.


280
00:28:56,600 --> 00:29:00,280
Sure, they will be slower, but it will work.


281
00:29:00,280 --> 00:29:08,680
And finally, in cases where, like education or libraries, there's a bunch of light terminals


282
00:29:08,680 --> 00:29:15,200
which were not able to run full-fledged node, and now in those cases you could have one


283
00:29:15,200 --> 00:29:19,320
of the oldest Raspberry Pis, not the current ones which are pretty beefy, but the very


284
00:29:19,320 --> 00:29:25,560
old ones, you could still be running something lean like Bifrost gateway and have verification


285
00:29:25,560 --> 00:29:30,960
on the same machine where the user agency should happen at the edge.


286
00:29:30,960 --> 00:29:37,280
And have only reduced cost for your classroom or library by having a single, maybe single


287
00:29:37,280 --> 00:29:40,840
cache for entire organization.


288
00:29:40,840 --> 00:29:45,720
Finally, some caveats, works in progress and future plans.


289
00:29:45,720 --> 00:29:46,720
It's an early preview.


290
00:29:46,720 --> 00:29:48,560
We are still working on performance.


291
00:29:48,560 --> 00:29:50,960
There are still many hard-coded configuration options.


292
00:29:50,960 --> 00:29:54,560
If there's something you would like to tweak, let us know, fill an issue.


293
00:29:54,560 --> 00:29:56,080
It's block by block by default.


294
00:29:56,080 --> 00:29:57,880
We have a small cache in memory.


295
00:29:57,880 --> 00:30:02,960
You can increase it, but we keep it default small so people can run it on low-powered


296
00:30:02,960 --> 00:30:03,960
hardware.


297
00:30:03,960 --> 00:30:08,880
We are working on experimental car-based graph backend, and that's it.


298
00:30:08,880 --> 00:30:09,920
That's all I will say.


299
00:30:09,920 --> 00:30:11,440
It will save round trips.


300
00:30:11,440 --> 00:30:16,800
You will no longer, you will fetch multiple blocks at the same time, but there will be


301
00:30:16,800 --> 00:30:23,960
an IPIP in IPFS specs repo about that after we are happy with the results.


302
00:30:23,960 --> 00:30:30,280
Currently, if you want to use a pool of gateways, you need to use caboose, but we hope after


303
00:30:30,280 --> 00:30:38,160
project 3 we will be able to uplift a bunch of generic code to the by-force gateway itself,


304
00:30:38,160 --> 00:30:43,240
so you will be able to pass a pool and not have to introduce load balancer right after


305
00:30:43,240 --> 00:30:45,680
it to route to multiple gateways.


306
00:30:45,680 --> 00:30:51,280
Currently, IPNS support, being able to verify signature of IPNS on the by-force gateway


307
00:30:51,280 --> 00:30:53,840
itself requires kubar-pc.


308
00:30:53,840 --> 00:30:59,280
I mention it here just so people are not surprised, but there's IPIP to add IPNS resolution support


309
00:30:59,280 --> 00:31:04,920
to routing v1 which will be provided by IPNI shortly.


310
00:31:04,920 --> 00:31:09,720
And finally, many people who want to run something like public gateway or maybe gateway only


311
00:31:09,720 --> 00:31:17,000
for their own use want to have, like having the ability to accept or deny some specific


312
00:31:17,000 --> 00:31:24,960
CADs or path, it's a very popular feature, and there is IPIP that we want to support


313
00:31:24,960 --> 00:31:33,440
in both kubo, probably it will be supported in the box or library level.


314
00:31:33,440 --> 00:31:34,440
That's it.


315
00:31:34,440 --> 00:31:43,760
I don't know how I look like with time, but if there are any questions, I'm happy to answer.


316
00:31:43,760 --> 00:31:50,160
Awesome.


317
00:31:50,160 --> 00:31:52,640
This is looking really great.


318
00:31:52,640 --> 00:31:58,680
Who do you see, you gave a couple examples, often when I hear the phrase self-hosting,


319
00:31:58,680 --> 00:32:01,600
I also think of like one-click deployment.


320
00:32:01,600 --> 00:32:06,400
I tried to get it running on my phone just now and deploy it and it failed.


321
00:32:06,400 --> 00:32:08,960
I will file an issue.


322
00:32:08,960 --> 00:32:14,160
But do you have any thoughts on what hosts you might want to target to have people easily


323
00:32:14,160 --> 00:32:16,720
run this in the cloud?


324
00:32:16,720 --> 00:32:17,960
It's a very good question.


325
00:32:17,960 --> 00:32:19,960
I think like it's very early.


326
00:32:19,960 --> 00:32:27,080
So our current focus is to give people, to not spend too much time on reaching a lot


327
00:32:27,080 --> 00:32:36,480
of users, but get the biggest bang for our buck from the way we are providing by first


328
00:32:36,480 --> 00:32:40,360
gateway to users, and that would be Docker.


329
00:32:40,360 --> 00:32:47,560
I think at the end of the day, a bunch of cloud services, we will see something similar.


330
00:32:47,560 --> 00:32:53,560
In the past, we had Azure one-click deployments for Kubo clusters, for IPFS clusters.


331
00:32:53,560 --> 00:32:58,880
I think there's also like IPFS Kubernetes, which was built with Kubo nodes and cluster.


332
00:32:58,880 --> 00:33:05,560
I think we will see by first gateway trickling down to those places and people using it as


333
00:33:05,560 --> 00:33:07,640
a component.


334
00:33:07,640 --> 00:33:12,320
But I think like the focus of this project will be to provide a single binary with a


335
00:33:12,320 --> 00:33:17,920
very strict set of responsibilities, and then if we are very strict about what this project


336
00:33:17,920 --> 00:33:24,640
does and have HTTP on both ends, that makes it very easy to integrate with existing infrastructures.


337
00:33:24,640 --> 00:33:25,640
Awesome.


338
00:33:25,640 --> 00:33:42,640
Thank you.
