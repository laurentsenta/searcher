1
00:00:00,000 --> 00:00:09,920
Hi everyone, I'm Franz, I work with the small workaround collective in Freiburg, Germany


2
00:00:09,920 --> 00:00:14,840
called Arso Collective and today I'll talk about a project we've been working on in the


3
00:00:14,840 --> 00:00:19,320
past month called RepGo.


4
00:00:19,320 --> 00:00:27,120
The project emerged from some quite long going discussion between different community media


5
00:00:27,120 --> 00:00:28,720
outlets throughout Europe.


6
00:00:28,720 --> 00:00:33,440
So what is community media?


7
00:00:33,440 --> 00:00:41,320
When we talk about community media we mostly mean small media outlets that are not commercial


8
00:00:41,320 --> 00:00:48,360
media outlets and that are not public broadcasters, state affiliated media.


9
00:00:48,360 --> 00:00:57,400
Community radio stations, independent journalistic publishers, DIY video sites, archives of social


10
00:00:57,400 --> 00:01:02,360
movements, all these smallish outlets that you find throughout the internet where people


11
00:01:02,360 --> 00:01:12,560
publish media, create media, often rely on volunteers or on little funding, very DIY


12
00:01:12,560 --> 00:01:23,120
focused often, grassroots based and democratically organized usually.


13
00:01:23,120 --> 00:01:30,160
This kind of publishing has some particular challenges, especially in the technical part


14
00:01:30,160 --> 00:01:31,160
of it.


15
00:01:31,160 --> 00:01:37,320
They often have little resources, so no big funds to invest into big solutions of their


16
00:01:37,320 --> 00:01:44,120
own and also they often, out of different reasons, don't want to rely too much on big


17
00:01:44,120 --> 00:01:56,200
tech companies like YouTube and the other big media platforms because they often want


18
00:01:56,200 --> 00:02:02,200
to keep their independence and not rely too much on external infrastructure out of experience


19
00:02:02,200 --> 00:02:05,400
that were made.


20
00:02:05,400 --> 00:02:14,960
So this project in concrete started about a year ago at a conference in Austria where


21
00:02:14,960 --> 00:02:17,600
a few outlets came together.


22
00:02:17,600 --> 00:02:24,520
The cultural broadcasting archive in Austria is the biggest of them and also Freie Radios


23
00:02:24,520 --> 00:02:30,760
Net in Germany, which are both platforms that emerge out of the community radio movements


24
00:02:30,760 --> 00:02:35,200
and that collect media from different outlets.


25
00:02:35,200 --> 00:02:41,280
And these platforms and a few others like Media CCC from the Chaos Computer Club, some


26
00:02:41,280 --> 00:02:47,120
people were there and also some people from Spain from a radio platform, they had this


27
00:02:47,120 --> 00:02:54,160
long-standing desire to get out of their bubbles and not have these separate platforms because


28
00:02:54,160 --> 00:02:58,960
that makes it really hard to compete in a way with big tech platforms because that's


29
00:02:58,960 --> 00:03:05,720
what you do in the end if you want to reach people and if you want to build up audiences.


30
00:03:05,720 --> 00:03:11,240
All these platforms are based on quite old tech stacks usually, some use WordPress, some


31
00:03:11,240 --> 00:03:14,360
use custom, very old PHP code.


32
00:03:14,360 --> 00:03:21,800
There's some more modern tools like some podcasting tools, one is called CastoPod that use ActivityPub


33
00:03:21,800 --> 00:03:25,680
for a federation, also Peertube.


34
00:03:25,680 --> 00:03:32,960
But so far there is no tool that allows them to collaborate on a shared repository of their


35
00:03:32,960 --> 00:03:36,520
media and to transfer audiences.


36
00:03:36,520 --> 00:03:42,640
One of the features that was imagined in the beginning is to have on the existing platforms


37
00:03:42,640 --> 00:03:46,760
embed widgets in the sidebars of something where you'd reference new content from the


38
00:03:46,760 --> 00:03:52,960
network and you'd have similar content to something that is displayed there to be able


39
00:03:52,960 --> 00:04:03,960
to exchange audiences and to engage in better ways with people across these existing platforms.


40
00:04:03,960 --> 00:04:09,360
On the technical level we have very different tools that are used, we have different standards,


41
00:04:09,360 --> 00:04:14,440
like the most common likely is RSS that is used to syndicate content.


42
00:04:14,440 --> 00:04:22,120
RSS works quite well and for a very long time but RSS also is a very limited specification


43
00:04:22,120 --> 00:04:26,680
and most of these platforms when they use RSS they add custom things on top and there


44
00:04:26,680 --> 00:04:28,400
is not many standards.


45
00:04:28,400 --> 00:04:33,360
With ActivityPub actually it's kind of similar even if it's a much more modern project that


46
00:04:33,360 --> 00:04:39,840
mostly when you talk about ActivityPub people mean Mastodon, which is the biggest project,


47
00:04:39,840 --> 00:04:45,840
but there are others like Peertube or CastoPod that have custom extensions to the ActivityPub


48
00:04:45,840 --> 00:04:52,560
protocol to add more media style metadata and so on.


49
00:04:52,560 --> 00:05:00,040
So out of all this the Repco project was created with the idea to aggregate content from these


50
00:05:00,040 --> 00:05:07,640
different outlets to replicate repositories between nodes, which also serves as a way


51
00:05:07,640 --> 00:05:09,560
to back up things.


52
00:05:09,560 --> 00:05:17,400
The challenge for these smallish outlets often is that once funding runs out or once people


53
00:05:17,400 --> 00:05:21,960
start to do other things we have these repositories of content in the internet like these old


54
00:05:21,960 --> 00:05:29,560
Web 2.0 or Web 1.0 sites, but they are not forever usually.


55
00:05:29,560 --> 00:05:33,640
They drop out of the internet after 5 years or 10 years and then things actually get lost


56
00:05:33,640 --> 00:05:36,280
this way.


57
00:05:36,280 --> 00:05:41,880
We want to aggregate the content, we want to preserve the content and to replicate it


58
00:05:41,880 --> 00:05:43,880
between these nodes.


59
00:05:43,880 --> 00:05:49,480
And then on top we can create shared indexes to offer better ways to browse content through


60
00:05:49,480 --> 00:05:58,120
different platforms and a future thing also is to connect services that add more metadata


61
00:05:58,120 --> 00:06:05,560
or value to these platforms like automatic transcription of audio and video or translation.


62
00:06:05,560 --> 00:06:13,080
The results of this, the initial plan should be embedded back into these platforms because


63
00:06:13,080 --> 00:06:19,040
they all didn't really, out of many reasons of course, want to put all their weight into


64
00:06:19,040 --> 00:06:25,240
some new tool that would solve the problems they have but instead expand the existing


65
00:06:25,240 --> 00:06:27,320
offers part by part.


66
00:06:27,320 --> 00:06:36,960
So basically we in a way want to bridge Web 1.0 or 2.0 apps into a new format that allows


67
00:06:36,960 --> 00:06:39,960
to seamlessly exchange the data.


68
00:06:39,960 --> 00:06:47,480
And for this we, after some evaluation, decided to build on tools that are a big topic in


69
00:06:47,480 --> 00:06:50,160
this forum here.


70
00:06:50,160 --> 00:06:56,320
We decided if we do this we want the data that we ingest once it's in the new system


71
00:06:56,320 --> 00:07:03,560
to be authenticated, to be self-verifying and to be able to replicate and exchange it


72
00:07:03,560 --> 00:07:07,400
in a trustless way.


73
00:07:07,400 --> 00:07:15,160
So the basic workflow that we came up with is we want to ingest the existing content,


74
00:07:15,160 --> 00:07:19,680
store this in the original format first of all, assign it content IDs everywhere and


75
00:07:19,680 --> 00:07:28,600
then map it onto a shared data format to then be able to query it in a common or in the


76
00:07:28,600 --> 00:07:33,600
same way no matter if the data comes from RSS or comes from ActivityPub or whatever


77
00:07:33,600 --> 00:07:38,480
other data sources likely will be developed in the future.


78
00:07:38,480 --> 00:07:43,440
One problem that we, or one challenge that we thought about quite a lot early on is how


79
00:07:43,440 --> 00:07:46,240
to deal with IDs.


80
00:07:46,240 --> 00:07:54,160
Content IDs are nice but they are immutable and they only can refer to a single revision.


81
00:07:54,160 --> 00:07:59,240
We needed of course also mutable identifiers because things change and these existing platforms


82
00:07:59,240 --> 00:08:02,160
might have updates.


83
00:08:02,160 --> 00:08:07,480
What we came up with, which I think is quite common and works out quite well, is not to


84
00:08:07,480 --> 00:08:13,040
have a new single ID for each entry but to have entity URIs and there can be multiple


85
00:08:13,040 --> 00:08:18,160
of them because sometimes the content that we ingest already came through a couple of


86
00:08:18,160 --> 00:08:24,320
different services on the way and then there might be multiple identifiers.


87
00:08:24,320 --> 00:08:29,520
We basically store all of them and whenever we import something new and when we find an


88
00:08:29,520 --> 00:08:37,040
existing match we assign the same internal ID to this content.


89
00:08:37,040 --> 00:08:38,120
How does the system work?


90
00:08:38,120 --> 00:08:41,080
I'll just go through a couple of examples.


91
00:08:41,080 --> 00:08:46,280
We have the existing data sources which might be RSS, ActivityPub, also a WordPress API


92
00:08:46,280 --> 00:08:50,440
is something we built an adapter for because quite a few of these platforms use WordPress


93
00:08:50,440 --> 00:08:53,120
and WordPress offers a REST API.


94
00:08:53,120 --> 00:08:58,920
RDF also, we don't have something for that yet but this is on the roadmap, especially


95
00:08:58,920 --> 00:09:04,960
once you move more towards public broadcasters and bigger media outlets there are some established


96
00:09:04,960 --> 00:09:08,560
metadata standards in RDF.


97
00:09:08,560 --> 00:09:14,600
First we fetch the content and then basically like a caching layer we store the content


98
00:09:14,600 --> 00:09:16,720
exactly as retrieved.


99
00:09:16,720 --> 00:09:28,160
This allows us in the next step to map this original content onto a shared data format.


100
00:09:28,160 --> 00:09:33,840
This actually was one of the longer running things in this process to arrive at this shared


101
00:09:33,840 --> 00:09:34,840
data format.


102
00:09:34,840 --> 00:09:40,160
This certainly will develop further because we store the original content we can iterate


103
00:09:40,160 --> 00:09:45,080
on that because we can run a remapping step that would just go through all the stored


104
00:09:45,080 --> 00:09:52,120
source records and recreate the mapped records on top.


105
00:09:52,120 --> 00:09:57,240
This already proved quite valuable to be able to iterate without contacting the original


106
00:09:57,240 --> 00:10:00,000
APIs again and again which can be quite slow.


107
00:10:00,000 --> 00:10:03,520
Some of them have rate limits and so on.


108
00:10:03,520 --> 00:10:09,120
Now we have the set of source records, we have the set of mapped records that have the


109
00:10:09,120 --> 00:10:15,600
custom types here, we have content items and media assets.


110
00:10:15,600 --> 00:10:23,440
And then we do something that is I think becoming something of a common pattern in many of these


111
00:10:23,440 --> 00:10:28,340
projects building on IPLD and IPFS tech these days.


112
00:10:28,340 --> 00:10:38,360
We store these records and we call it commits where we sign the list of content IDs included


113
00:10:38,360 --> 00:10:47,120
in a commit and add a UConn proof to validate the capability.


114
00:10:47,120 --> 00:10:56,560
This basically makes the system multi-writer because each repository, the basic organization


115
00:10:56,560 --> 00:11:03,720
of the content is repositories and repositories are identified by a key pair basically.


116
00:11:03,720 --> 00:11:10,440
We use decentralized identifiers but only support the key pair method.


117
00:11:10,440 --> 00:11:15,160
Maybe we'll also, we're not sure about if we actually need the DID abstraction because


118
00:11:15,160 --> 00:11:17,360
we only ever use key pairs so far.


119
00:11:17,360 --> 00:11:23,920
But anyway, we have a key pair that identifies the repository and then through UConns we


120
00:11:23,920 --> 00:11:30,560
can create additional capabilities that allow other key pairs to publish into that repository.


121
00:11:30,560 --> 00:11:39,880
And this basically allows to have different writers, so one key pair per device that is


122
00:11:39,880 --> 00:11:43,720
allowed to write into the repository.


123
00:11:43,720 --> 00:11:52,080
And yeah, so the commits are just a list of content IDs plus a signature plus the UConn


124
00:11:52,080 --> 00:11:53,520
proof.


125
00:11:53,520 --> 00:12:03,040
And in the end, this repository then is basically an append-only log of commits.


126
00:12:03,040 --> 00:12:10,720
And after each commit, the repository has a new head which is the content ID of the


127
00:12:10,720 --> 00:12:12,920
latest commit.


128
00:12:12,920 --> 00:12:20,040
And this is a very simple structure that makes it very straightforward to replicate this


129
00:12:20,040 --> 00:12:25,240
content from one node to the other, which is what we do.


130
00:12:25,240 --> 00:12:28,040
Oh, there's one slide in between.


131
00:12:28,040 --> 00:12:32,080
So replication comes in a second.


132
00:12:32,080 --> 00:12:37,480
What then happens at each node is that we index all this content, no matter from what


133
00:12:37,480 --> 00:12:39,120
repository.


134
00:12:39,120 --> 00:12:41,960
And this is basically a two-step process.


135
00:12:41,960 --> 00:12:47,320
The first step is the lower data model as we call it, where we have the entities, the


136
00:12:47,320 --> 00:12:50,760
extensions, commits, repos, the UConn proofs.


137
00:12:50,760 --> 00:12:55,040
This is basically independent of the data that is actually in there.


138
00:12:55,040 --> 00:13:02,280
And then we have the domain data model part, which is the shared data model that was or


139
00:13:02,280 --> 00:13:04,520
is being developed in this project.


140
00:13:04,520 --> 00:13:05,920
This is in a way specific.


141
00:13:05,920 --> 00:13:09,680
We're thinking about adding like extensions points to this.


142
00:13:09,680 --> 00:13:15,480
This would basically be the step to make this whole project be usable for other things as


143
00:13:15,480 --> 00:13:16,480
well.


144
00:13:16,480 --> 00:13:22,400
We did not do that initially to keep the abstraction low and to first get our product ready, but


145
00:13:22,400 --> 00:13:26,840
this would be quite straightforward likely to add.


146
00:13:26,840 --> 00:13:32,360
And this is currently then indexed in a SQL database, which through some tooling gives


147
00:13:32,360 --> 00:13:35,000
us automated GraphQL queries.


148
00:13:35,000 --> 00:13:41,400
This is all public read-only data, which makes it quite simple to query it from frontends


149
00:13:41,400 --> 00:13:42,480
then.


150
00:13:42,480 --> 00:13:49,880
So basically on each node, there's two APIs to talk with.


151
00:13:49,880 --> 00:14:01,360
The first is the replication API, which is a very simple HTTP API where the repo is treated


152
00:14:01,360 --> 00:14:08,160
as an append-only log and you can just say, give me all new content since the last commit


153
00:14:08,160 --> 00:14:14,120
that I know of, and then the node replies with the cost stream of all the commits, the


154
00:14:14,120 --> 00:14:16,280
proofs, and the content within.


155
00:14:16,280 --> 00:14:24,320
And on the receiving end, everything is validated, the signatures, the proofs, and so on.


156
00:14:24,320 --> 00:14:29,120
So there is no trust involved in the transport.


157
00:14:29,120 --> 00:14:36,760
And the other API that we have is a GraphQL API that translates to SQL queries that is


158
00:14:36,760 --> 00:14:43,600
used in frontends and the embed widgets that I talked about that allow to, on an existing


159
00:14:43,600 --> 00:14:51,840
platform, have this sidebar widget or so on where you display new content from the system.


160
00:14:51,840 --> 00:14:53,400
All this works very well.


161
00:14:53,400 --> 00:14:58,280
However, and this is like the crossover to the data transfer part that we've been thinking


162
00:14:58,280 --> 00:15:03,120
about a lot recently, is we have this split in these APIs.


163
00:15:03,120 --> 00:15:09,280
We have the replication API that is very simple but is verified and authenticated end-to-end.


164
00:15:09,280 --> 00:15:18,280
And then we have the querying API, which is very expressive and quite simple to use.


165
00:15:18,280 --> 00:15:23,240
You can have this nested GraphQL queries with filters and selectors and so on, but this


166
00:15:23,240 --> 00:15:24,760
is all unverified.


167
00:15:24,760 --> 00:15:30,880
You post a GraphQL query, you get back JSON, and that's it.


168
00:15:30,880 --> 00:15:36,560
And this is a gap that doesn't matter too much for some use cases.


169
00:15:36,560 --> 00:15:43,920
However, once we get to bigger repos and also once we want to move more logic to the edge


170
00:15:43,920 --> 00:15:49,160
or to the client, so currently the nodes are more imagined as like classic server software


171
00:15:49,160 --> 00:15:54,880
that runs somewhere, ingests content, talks with other bigger nodes, and then clients


172
00:15:54,880 --> 00:15:58,560
and frontends use the query API.


173
00:15:58,560 --> 00:16:04,320
We'd like to bridge this gap more, to be able to do better caching and to push validation


174
00:16:04,320 --> 00:16:09,880
and authentication to the edge or to the client.


175
00:16:09,880 --> 00:16:14,360
So what we're thinking at the moment and what I think is interesting also in all this data


176
00:16:14,360 --> 00:16:25,860
transfer talk and discussion is we're thinking about having the query API only ever return


177
00:16:25,860 --> 00:16:31,840
content IDs and then have the data transfer protocol resolve these content IDs.


178
00:16:31,840 --> 00:16:37,120
I think this is something that will also come up in other contexts, because for example


179
00:16:37,120 --> 00:16:43,960
in IRO there was a lot of talk, we want to only ever just exchange lists of blocks and


180
00:16:43,960 --> 00:16:48,200
hashes and nothing else.


181
00:16:48,200 --> 00:16:53,200
I think it's a quite interesting point to think of the query layer as something separate


182
00:16:53,200 --> 00:16:57,760
from the data transfer layer and have the query layer be app-specific.


183
00:16:57,760 --> 00:17:06,240
So there are also, even with dark queries, there is some expressiveness to it, but also


184
00:17:06,240 --> 00:17:12,240
often you will deviate from the strict selector model that you can do with IPLD selectors.


185
00:17:12,240 --> 00:17:17,820
You want app-specific queries, or like full-text search for example, that doesn't align to


186
00:17:17,820 --> 00:17:21,020
graph queries at all usually.


187
00:17:21,020 --> 00:17:25,840
So what we're thinking is having a query layer that is separate from the data transfer layer,


188
00:17:25,840 --> 00:17:33,080
but that returns only the content IDs of the content within, plus potentially some metadata,


189
00:17:33,080 --> 00:17:42,760
like snippets for full-text search or something like that, and then use a data transfer layer


190
00:17:42,760 --> 00:17:46,080
to exchange the actual content.


191
00:17:46,080 --> 00:17:52,080
And then the query layer will likely be quite app-specific and very different between different


192
00:17:52,080 --> 00:17:56,600
apps, however that data transfer layer could then also be something where different apps


193
00:17:56,600 --> 00:18:00,880
can collaborate on the most efficient strategies.


194
00:18:00,880 --> 00:18:07,000
And once you take out the query part a bit more, it becomes a much simpler layer.


195
00:18:07,000 --> 00:18:12,520
And this was one of the things we're thinking about, not yet working on, but thinking about


196
00:18:12,520 --> 00:18:16,440
how to move this further in the future.


197
00:18:16,440 --> 00:18:21,880
Because then if you get the results of the query, you can also decide locally, or look


198
00:18:21,880 --> 00:18:27,480
up locally, what part of the results do I already have cached or stored locally, and


199
00:18:27,480 --> 00:18:29,440
then only fetch the rest.


200
00:18:29,440 --> 00:18:37,420
And then also this opens the door to fetch from multiple parties the results.


201
00:18:37,420 --> 00:18:42,960
So this is the data transfer things we're thinking about in this project.


202
00:18:42,960 --> 00:18:44,600
We have also some other future ideas.


203
00:18:44,600 --> 00:18:50,680
We want to have persistent subscriptions on the repository so that via also a simple HTTP


204
00:18:50,680 --> 00:18:55,840
API you can subscribe to a repository and get notified of changes, which makes it very


205
00:18:55,840 --> 00:19:01,440
easy to integrate other services on top, like transcription or translation services that


206
00:19:01,440 --> 00:19:07,880
would listen on incoming things, create the jobs, and push back the results into the repo.


207
00:19:07,880 --> 00:19:13,200
And for these jobs, we're also quite actively following what is being discussed in the UCAN


208
00:19:13,200 --> 00:19:15,840
invocation drafts.


209
00:19:15,840 --> 00:19:20,800
I think there is a talk in the next days about IPVM.


210
00:19:20,800 --> 00:19:26,780
This is something we're very curious about.


211
00:19:26,780 --> 00:19:27,780
This is it.


212
00:19:27,780 --> 00:19:33,500
I want to show you a very, not a demo, but just a quick impression of how things look


213
00:19:33,500 --> 00:19:34,940
like at the moment.


214
00:19:34,940 --> 00:19:37,380
This is the demo instance that's running at the moment.


215
00:19:37,380 --> 00:19:40,040
We have different nodes running.


216
00:19:40,040 --> 00:19:46,920
This one indexes content from this cultural broadcasting archive in Austria.


217
00:19:46,920 --> 00:19:53,660
This is another one that indexes content from the Freie Radios net, the German radio network,


218
00:19:53,660 --> 00:19:55,060
community radio network.


219
00:19:55,060 --> 00:20:00,100
And then we have a shared instance that replicates the different nodes together.


220
00:20:00,000 --> 00:20:10,000
The front end is still a work in progress. But basically you have this browsing interface where all the content floats in.


221
00:20:10,000 --> 00:20:16,000
You can listen to audio and have some filters. This is all still a work in progress.


222
00:20:16,000 --> 00:20:24,000
But the basic system works and once things flow together from the different nodes, it's all authenticated and validated.


223
00:20:24,000 --> 00:20:33,000
Of course, because most of the content comes from external services, there is an initial point of trust basically,


224
00:20:33,000 --> 00:20:37,000
where you have to trust the node that does the ingest part.


225
00:20:37,000 --> 00:20:46,000
And the idea there is to make this software very simple to run so that it can run socially close to the source.


226
00:20:46,000 --> 00:20:56,000
So not necessarily technically, but there are social trust relations towards the ingest nodes from the original creators.


227
00:20:56,000 --> 00:21:03,000
And then from there on it can all flow in the authenticated and validated, verified ways.


228
00:21:03,000 --> 00:21:08,000
Yeah, that's it. Thank you. If you have any questions.


229
00:21:08,000 --> 00:21:16,000
So on the query with proofs thing, I'm just trying to imagine how that works in my head. Are you able to use a generic full-text index


230
00:21:16,000 --> 00:21:22,000
and then find the relevant CIDs and send those back? Or do you need to have an IPLD-aware index?


231
00:21:22,000 --> 00:21:29,000
Yeah, this is basically the idea. We'd have a regular, any full-text search engine.


232
00:21:29,000 --> 00:21:39,000
There's some quite interesting Rust projects in that regard also, if you don't want to use the big and resource-intensive Elasticsearch,


233
00:21:39,000 --> 00:21:46,000
which is kind of the standard in many places. And then those would index the content.


234
00:21:46,000 --> 00:21:51,000
You'd likely have to do some schema mapping, again, to do that in a nice way.


235
00:21:51,000 --> 00:21:57,000
And then for each document, not necessarily the CID, but likely the mutable ID.


236
00:21:57,000 --> 00:22:00,000
And then through the existing database, map that to the CIDs.


237
00:22:00,000 --> 00:22:09,000
And then return the list of CIDs, plus likely some metadata structure that would include something like snippets or scores.


238
00:22:09,000 --> 00:22:20,000
And then you could locally just fetch all the SIDs and then store them through the data or replication API.


239
00:22:20,000 --> 00:22:30,000
Just two things. One is this project is so awesome. Before I was doing this stuff, I had about a 10-year career in the nonprofit sector


240
00:22:30,000 --> 00:22:38,000
and intimately familiar with the disappearance of data in small organizations with low tech competencies.


241
00:22:38,000 --> 00:22:45,000
So it's pretty cool. And then the other thing is just the pattern you talked about with the query protocol.


242
00:22:45,000 --> 00:22:55,000
It's interesting. Were you at IPFS thing a year ago? We had a name that was proposed, the manifest pattern,


243
00:22:55,000 --> 00:23:03,000
which is like rather than do a data transfer up front, do a query to get a list of CIDs and then go from there.


244
00:23:03,000 --> 00:23:09,000
There's some challenges around verifiability, but it's definitely a good approach.


245
00:23:09,000 --> 00:23:14,000
And it also makes multi-party data transfer pretty accessible.


246
00:23:14,000 --> 00:23:21,000
I think it also would align quite nicely to something like persistent queries, like a publish-subscribe system,


247
00:23:21,000 --> 00:23:30,000
where you subscribe to some topics or a subset of the data and then get notified of new CIDs and then request them through the usual means.


248
00:23:30,000 --> 00:23:41,000
Thanks. I might have missed this, but when you said the ingest, do you crawl these RSS feeds from people who have agreed to get it,


249
00:23:41,000 --> 00:23:45,000
or do you just crawl what you think is valuable?


250
00:23:45,000 --> 00:23:54,000
Currently, these public demo instances, the data that is crawled there was part of the project.


251
00:23:54,000 --> 00:24:03,000
The whole thing started with some funding from the European Cultural Foundation that wanted to connect different media outlets.


252
00:24:03,000 --> 00:24:09,000
So these are all part of the project. How this would evolve is, I think, an open question.


253
00:24:09,000 --> 00:24:18,000
You could treat it as a general RSS feed reader thing, where you just put in any feeds you wanted to.


254
00:24:18,000 --> 00:24:26,000
The idea so far is more that it would be, as I said, socially close to the publishers.


255
00:24:26,000 --> 00:24:30,000
So how exactly that would turn out is not defined yet.


256
00:24:30,000 --> 00:24:37,000
Thank you very much. This is a young project. It's still quite prototype-y and alpha.


257
00:24:37,000 --> 00:24:44,000
If you know people or are yourself interested in advancing this further, please contact us.


258
00:24:44,000 --> 00:25:09,000
Thanks.
