1
00:00:00,000 --> 00:00:11,000
Thank you everybody. Thank you Robin and Dietrich and everybody for inviting me to this event.


2
00:00:11,000 --> 00:00:16,800
My name is Cade. I run a small research organization called the New Design Congress and today I


3
00:00:16,800 --> 00:00:21,440
wanted to share a research project that we did with an IPFS aligned organization with


4
00:00:21,440 --> 00:00:29,640
the support of the Filecoin Foundation. It's going to be a bit deep. Let me get my stopwatch


5
00:00:29,640 --> 00:00:38,140
going. Great. It's going to be pretty rich in content. I've erred on the side of caution


6
00:00:38,140 --> 00:00:42,700
by putting a little bit too much information into the slides. We're not going to cover


7
00:00:42,700 --> 00:00:47,320
all of it today, but what I will say is that at the end of this presentation is a link


8
00:00:47,320 --> 00:00:52,720
to the research and to these slides. So please just let it all wash over you and then anything


9
00:00:52,720 --> 00:00:56,480
that you don't get, either email me or come and check out the material because it's all


10
00:00:56,480 --> 00:01:00,960
going to be there for you. But before we get started, I'd like to set the scene with a


11
00:01:00,960 --> 00:01:05,160
brief video that will summarize a little bit about where I come from.


12
00:01:05,160 --> 00:01:11,160
Are you playing Roblox? Yeah. Explain to me what you're doing though. You're playing Roblox.


13
00:01:11,160 --> 00:01:15,840
Yeah, I'm playing Roblox. I'm taking money from people.


14
00:01:15,840 --> 00:01:18,560
You're getting on the phone and then you're stealing money from people?


15
00:01:18,560 --> 00:01:19,560
Yeah.


16
00:01:19,560 --> 00:01:23,480
And you have this in your basement of your Roblox house.


17
00:01:23,480 --> 00:01:33,960
Don't do that. My intern Kay actually is helping.


18
00:01:33,960 --> 00:01:34,960
She's your intern?


19
00:01:34,960 --> 00:01:35,960
Yeah.


20
00:01:35,960 --> 00:01:42,160
You have a whole office in your basement where you're scamming people?


21
00:01:42,160 --> 00:01:49,360
So I love this video. Essentially, the reason why it's really funny is this is the perfect


22
00:01:49,360 --> 00:01:54,040
example of what New Design Congress is all about. Essentially, while we build these rich


23
00:01:54,040 --> 00:02:01,280
technical systems, be it IPFS or Roblox, the ways in which they're used are often far beyond


24
00:02:01,280 --> 00:02:07,920
the intent that we have for them. I love the idea that there's these tiny children basically


25
00:02:07,920 --> 00:02:14,200
becoming the next chapos, basically scamming the hell out of people through these iPads.


26
00:02:14,200 --> 00:02:19,400
It sort of makes me wonder after I saw that clip and did some diving into the kind of


27
00:02:19,400 --> 00:02:26,280
economics and labor abuse of children, like getting other children to build stuff for


28
00:02:26,280 --> 00:02:31,760
them in Roblox and get paid nothing for it. It made me really think about that child that


29
00:02:31,760 --> 00:02:36,720
you see sitting across from you at another table in a restaurant that's on their iPad.


30
00:02:36,720 --> 00:02:40,280
We used to think they're just watching Frozen over and over, but maybe in fact they're actually


31
00:02:40,280 --> 00:02:47,280
scamming everybody. This is one of the stickers for New Design Congress, ethical design, no


32
00:02:47,280 --> 00:02:51,280
thanks. Part of what this work is, part of the world that I come from and the foundation


33
00:02:51,280 --> 00:02:57,720
of New Design Congress is looking at the kinds of ways in which we see the world and then


34
00:02:57,720 --> 00:03:01,760
how that's out of alignment as system designers with the way the world actually is.


35
00:03:01,760 --> 00:03:06,600
So we're an independent socio-technical research organization, which means that we confront


36
00:03:06,600 --> 00:03:11,400
the gap between what appears to be happening and what is actually happening in digitized


37
00:03:11,400 --> 00:03:18,160
societies. Now, the point here is to say that it's not about being some kind of elitist,


38
00:03:18,160 --> 00:03:22,520
sort of everything that people are doing in systems design is wrong, but rather that the


39
00:03:22,520 --> 00:03:27,520
complexity combined with the shortcomings of tools and practices we have today often


40
00:03:27,520 --> 00:03:31,400
lead to bad outcomes. And the kind of response that we have, which are things like ethical


41
00:03:31,400 --> 00:03:35,760
design, ethical AI, things like that, are not enough to overcome some of the problems


42
00:03:35,760 --> 00:03:38,440
that we have. So the way to think of New Design Congress


43
00:03:38,440 --> 00:03:44,080
itself in this room would be to think of a digital security firm that comes in and consults


44
00:03:44,080 --> 00:03:49,600
or does research and finds vulnerabilities as red teamers, but we don't do it on a technical


45
00:03:49,600 --> 00:03:53,720
level, we do it on a social or an ethical level. So we're like an ethical red teaming


46
00:03:53,720 --> 00:03:58,880
organization. And what we do is we follow that same process. So we do deep research,


47
00:03:58,880 --> 00:04:03,960
such as what I'm going to present today. We then practice responsible disclosure, like


48
00:04:03,960 --> 00:04:09,440
any other security organization. And then we publish everything in open access after


49
00:04:09,440 --> 00:04:17,160
we complete that disclosure process. And so as part of that process, we engaged


50
00:04:17,160 --> 00:04:22,760
with Filecoin and a Filecoin aligned project, funded project called WebRecorder, between


51
00:04:22,760 --> 00:04:29,840
November 2021 and May 2022, which led to the project or the publication Memory and Uncertainty,


52
00:04:29,840 --> 00:04:35,800
or as was talked about today, Are We All Fucked? The answer is kind of. But it didn't start


53
00:04:35,800 --> 00:04:41,360
out that way. So the team, I want to throw this slide up


54
00:04:41,360 --> 00:04:48,240
because these are the people who can be credited on this. There was the really tight collaboration


55
00:04:48,240 --> 00:04:55,680
between WebRecorder and us at New Design Congress, myself and my co-conspirator, my co-defendant,


56
00:04:55,680 --> 00:04:59,880
Benjamin Royer, who can't unfortunately be here today. But then of course, the other


57
00:04:59,880 --> 00:05:05,080
people from WebRecorder, Ilya, Lorena, Ed Summers, basically helping to fuel that work


58
00:05:05,080 --> 00:05:09,880
and frame it for us. And then of course, Dietrich, who's here today, kind of acting as like the


59
00:05:09,880 --> 00:05:14,520
champion on the Filecoin slash Protocol Lab side. All of it came together to produce,


60
00:05:14,520 --> 00:05:19,640
I think, a piece of research that's really great. And this is where the slides... What's


61
00:05:19,640 --> 00:05:25,520
up? Down a little bit. Okay, great. This is where the slides begin to get a little bit


62
00:05:25,520 --> 00:05:30,640
dense. So I'm just going to riff a little bit on what we started to explore. And you


63
00:05:30,640 --> 00:05:34,200
can either listen to me or look at the slides, or both if you have like a brain that can


64
00:05:34,200 --> 00:05:39,880
do that. But as I said at the beginning, all of this is available online, so you can come


65
00:05:39,880 --> 00:05:43,120
back through it and read it. Essentially, what we did was we looked at


66
00:05:43,120 --> 00:05:49,840
the decentralization component of an emerging tool, in this case, the WebRecorder platform,


67
00:05:49,840 --> 00:05:55,560
which is a... Unlike the Internet Archive, it's kind of the inverse. Rather than being


68
00:05:55,560 --> 00:06:01,000
tied to an institution, WebRecorder allows you to take very high fidelity captures of


69
00:06:01,000 --> 00:06:06,040
the web and faithfully reproduce them in a local context. It's a very impressive piece


70
00:06:06,040 --> 00:06:10,160
of technology, and it's in its early stages. Of course, it's also gaining a lot of attention


71
00:06:10,160 --> 00:06:15,200
as it's very useful for researchers, everybody from cultural researchers all the way through


72
00:06:15,200 --> 00:06:21,320
to journalists working in disinformation to produce very high quality records of things


73
00:06:21,320 --> 00:06:25,400
on the Internet. But we were really looking broadly, when we


74
00:06:25,400 --> 00:06:30,800
were commissioned by Ilia and the team at WebRecorder, at the wider range of issues


75
00:06:30,800 --> 00:06:35,920
that could then inform both the specification of their WAXI format, that's the Web Archive


76
00:06:35,920 --> 00:06:41,520
Compressed Zip File archival format, and the interfaces of the tools that are then built


77
00:06:41,520 --> 00:06:47,080
with the WAXI specification baked into it. So this covers everything from identity and


78
00:06:47,080 --> 00:06:51,640
structures, what are the use cases, socio-technical security, what are the dangers and threats


79
00:06:51,640 --> 00:06:58,040
from not a device perspective, from a person-to-person perspective, integrity, how can archives be


80
00:06:58,040 --> 00:07:03,440
manipulated, can people make changes to archives. This is a big thing, I think, in IPFS land


81
00:07:03,440 --> 00:07:07,960
as well, the immutability and the kind of proof of record or the source of truth of


82
00:07:07,960 --> 00:07:12,120
different kinds of material that's stored in IPFS. Of course, permissions and connectivity,


83
00:07:12,120 --> 00:07:18,880
a lot of challenges with archives around how expensive they are to maintain indefinitely,


84
00:07:18,880 --> 00:07:23,640
they're kind of a complete opposite of a capitalist service because archives don't make money


85
00:07:23,640 --> 00:07:29,320
usually, they're kind of just giant black holes for financial input, things like that.


86
00:07:29,320 --> 00:07:34,000
But even going deeper into looking at how navigation affects the ways in which we relate


87
00:07:34,000 --> 00:07:40,880
to large collections of material and the agency questions, so what do the people whose material


88
00:07:40,880 --> 00:07:45,920
is stored in a digital archive, how are those people, do they have a say, how does that


89
00:07:45,920 --> 00:07:49,520
affect them, do they even know about it, what's the difference between someone who knows that


90
00:07:49,520 --> 00:07:53,120
they've been archived and somebody who doesn't, these sorts of things. And the point here


91
00:07:53,120 --> 00:07:59,040
is really to kind of look at this from a digital archiving perspective and understand that


92
00:07:59,040 --> 00:08:06,440
by doing so, it acts as a very good starting point for the broader understanding of the


93
00:08:06,440 --> 00:08:13,880
politics and the social conditions of data storage on a massive societal level. Your


94
00:08:13,880 --> 00:08:19,680
iCloud account or your Google Gmail account, both of these are actually archives. Everything


95
00:08:19,680 --> 00:08:25,800
that we have online, to quote Yokui, the Hong Kong philosopher, everything is an archive.


96
00:08:25,800 --> 00:08:30,840
So as part of this project, we did a landscaping study and we also did a round of consultation


97
00:08:30,840 --> 00:08:35,920
with WebRecorder, and then through WebRecorder's connections, we were very privileged enough


98
00:08:35,920 --> 00:08:42,800
to get a series of interviews with a wide number of very, very established people from


99
00:08:42,800 --> 00:08:48,320
various disciplines within the archival space, as a sort of large-scale qualitative first


100
00:08:48,320 --> 00:08:55,000
round of interviews that lasted for 90 minutes or longer and culminated in a broad snapshot


101
00:08:55,000 --> 00:08:59,840
of the conditions of archiving and how they relate to digitized structures, both from


102
00:08:59,840 --> 00:09:04,840
the emergence of open-source tooling such as WebRecorder, but also in the storage and


103
00:09:04,840 --> 00:09:12,520
maintenance of these things, such as, in this case, IPFS. And so what we found is in this


104
00:09:12,520 --> 00:09:18,120
research, and I think there aren't actually too many examples of a project that has this


105
00:09:18,120 --> 00:09:23,480
kind of depth and breadth in terms of how it approaches these sorts of things, we found


106
00:09:23,480 --> 00:09:28,040
like 10 key findings here that I'd like to share with you today. We're not going to spend


107
00:09:28,040 --> 00:09:33,440
equal amounts of time on all of them because, you know, time is a harsh, oppressive creature


108
00:09:33,440 --> 00:09:39,440
and I only have a small amount of it. So instead, I'm going to really riff on a couple of these


109
00:09:39,440 --> 00:09:43,560
that I think are extremely important for the people in this room today, and then again


110
00:09:43,560 --> 00:09:47,000
refer you to the slide deck and everything like that if you'd like to know more about


111
00:09:47,000 --> 00:09:51,520
the broader work. So number one, the definition of archiving


112
00:09:51,520 --> 00:09:55,920
is broad but flattened by digital tools. So what this meant is that in the process of


113
00:09:55,920 --> 00:10:00,480
archiving, whether people were from indigenous backgrounds as archivists or whether they


114
00:10:00,480 --> 00:10:06,760
were journalists or people working in institutions, like very big institutions, what we found


115
00:10:06,760 --> 00:10:12,280
is that, broadly speaking, the tools that are used to both create archives and also


116
00:10:12,280 --> 00:10:18,200
store them, including protocols, all of them had clashes with how different people in these


117
00:10:18,200 --> 00:10:23,440
spaces understood what archiving meant and what was possible as a result of that. The


118
00:10:23,440 --> 00:10:29,680
ways in which they could express their self-determination of their practice of collecting, categorizing,


119
00:10:29,680 --> 00:10:35,720
and saving material for recall later was all deeply influenced by digital tools and there


120
00:10:35,720 --> 00:10:42,120
were no two people that had the same kind of relationship or definition of archiving


121
00:10:42,120 --> 00:10:48,760
despite it being a fairly well-known practice. And so we had people who saw these from a


122
00:10:48,760 --> 00:10:54,200
perspective of political action, we saw things from people who came at it from a purely historical


123
00:10:54,200 --> 00:10:58,680
context, there were people who, again, like investigative journalism, which isn't necessarily


124
00:10:58,680 --> 00:11:05,360
tied to political activism, things like that. But broadly speaking, even things like the


125
00:11:05,360 --> 00:11:10,840
inability to conceptualize the kind of size of an archive, especially like a decentralized


126
00:11:10,840 --> 00:11:17,360
archive, when compared to physical archives, had drastic effects on how people then maintained


127
00:11:17,360 --> 00:11:21,520
or cultivated and curated those spaces.


128
00:11:21,520 --> 00:11:28,960
Two, web archives and, broadly speaking, decentralized storage is complex and overwhelming to the


129
00:11:28,960 --> 00:11:32,880
people that make them. So I'm almost certain that everybody in this room kind of knows


130
00:11:32,880 --> 00:11:39,800
this already, but what the research kind of found for us was just how severe and independent


131
00:11:39,800 --> 00:11:48,040
and individualized that was. So what we saw was the classic McLuhan, like Marshall McLuhan


132
00:11:48,040 --> 00:11:52,360
kind of thing, we make, we build our tools, we shape our tools and thereafter our tools


133
00:11:52,360 --> 00:11:58,280
shape us. The differences between the tools and their outputs makes it difficult to provide


134
00:11:58,280 --> 00:12:04,360
access that's universal. And so what archivists tend to do is like a grassroots kind of, in


135
00:12:04,360 --> 00:12:09,160
this current moment where the tools have these shortcomings, basically either rely on multiple


136
00:12:09,160 --> 00:12:14,080
tooling, so getting like sort of different categorizations and very few standardized


137
00:12:14,080 --> 00:12:21,160
sort of collections of material into an archived container, or then create hacks and workarounds


138
00:12:21,160 --> 00:12:27,080
to get into that. And what we also found is, you know, similar to things like content addressing


139
00:12:27,080 --> 00:12:32,720
and other forms of like linkage between things, people tended to just either use timestamps


140
00:12:32,720 --> 00:12:38,000
or the link itself as like the primary means for navigating large datasets. Like, and I


141
00:12:38,000 --> 00:12:41,960
say, when I say large datasets, I mean web archiving, which is like, you know, three


142
00:12:41,960 --> 00:12:47,280
dimensions of complexity. One is like the generation of material on a platform that


143
00:12:47,280 --> 00:12:52,640
can be, that's like infinite potentially just based entirely, the only limitation here is


144
00:12:52,640 --> 00:12:56,280
storage. The second dimension is like the breadth of websites, so the availability of


145
00:12:56,280 --> 00:13:01,040
different websites rather than like, you know, just focusing on content creation on one particular


146
00:13:01,040 --> 00:13:06,280
platform. And then of course time and the idea of material changing over time and needing


147
00:13:06,280 --> 00:13:10,240
to save all of that adds that third dimension. And so when you're looking at archives that


148
00:13:10,240 --> 00:13:16,120
have that level of complexity over time, then the ways in which people relate to those and


149
00:13:16,120 --> 00:13:22,760
how it can then overwhelm them is extremely, it's very easy to do that.


150
00:13:22,760 --> 00:13:26,200
So the third one, and this is one I want to touch on for a little bit longer, it's part


151
00:13:26,200 --> 00:13:29,640
of the broader work that we've done for the past few years at New Design Congress, is


152
00:13:29,640 --> 00:13:34,960
that we found that decentralized solutions such as IPFS are plagued with danger and much


153
00:13:34,960 --> 00:13:40,920
of it is unknown or unaddressed. Now what this means is that in 2020 I wrote an essay


154
00:13:40,920 --> 00:13:45,400
for New Design Congress called Optimism and Emergency in the Peer-to-Peer Network, which


155
00:13:45,400 --> 00:13:52,760
describes some of the historical context of how the utopian or the kind of, the optimism


156
00:13:52,760 --> 00:13:59,320
that we share around the kind of goals of IPFS to become a planetary-wide protocol,


157
00:13:59,320 --> 00:14:04,940
decentralization protocol, carries with it severe existential risk that to this day remains


158
00:14:04,940 --> 00:14:10,840
essentially unaccounted for. And this is true not just for IPFS, I'm not singling that out,


159
00:14:10,840 --> 00:14:16,200
it's relevant for this research because it's relationship to the work with WebRecorder


160
00:14:16,200 --> 00:14:21,200
and the idea that WebRecorder would like to put all of their archives on IPFS. But it's


161
00:14:21,200 --> 00:14:25,280
also true of Secure Scuttlebutt, the DAT Foundation. In the piece itself I went into


162
00:14:25,280 --> 00:14:29,920
detail about the historical context, which was how BitTorrent was weaponized against


163
00:14:29,920 --> 00:14:37,840
essentially anybody who was involved in like the distribution of copyright infringing material


164
00:14:37,840 --> 00:14:42,280
and how that then essentially kneecapped the entire sort of free information movement in


165
00:14:42,280 --> 00:14:47,240
the early 21st century, which then gave birth to services like Spotify and the media conglomerate


166
00:14:47,240 --> 00:14:51,680
that we have today, which was piggybacked off the technological achievements of BitTorrent


167
00:14:51,680 --> 00:14:56,800
but sacrificed almost everybody involved in that as a process. So that sort of historical


168
00:14:56,800 --> 00:15:02,400
context remains sort of forgotten and in the context of archival, whether or not it was


169
00:15:02,400 --> 00:15:09,440
a sort of an indigenous archival effort or a more institutional one, what we found is


170
00:15:09,440 --> 00:15:13,520
that there were differing levels of awareness of collective institutional power and how


171
00:15:13,520 --> 00:15:22,960
it's held and how the consolidation of custodianship in a decentralized system creates power imbalances


172
00:15:22,960 --> 00:15:27,800
that kind of rival the things that we see today in centralized systems. So what we have


173
00:15:27,800 --> 00:15:32,520
is like this, and I was talking a bit with Robin about this last night, if we think about


174
00:15:32,520 --> 00:15:38,920
things like Web3 and how Web3 itself has sort of is yet to reach its potential, at the same


175
00:15:38,920 --> 00:15:44,600
time I can't think of an industry that suffered more attacks, not just from hacking, but also


176
00:15:44,600 --> 00:15:49,680
from like attempts at capital to consolidate that, like you know just almost bottomless


177
00:15:49,680 --> 00:15:54,720
amounts of money being injected into the system with the goal of essentially maintaining control.


178
00:15:54,720 --> 00:16:00,640
You know the 51% argument, right, just as a really simple example of that, the amount


179
00:16:00,640 --> 00:16:08,380
of resources that go into attacking these systems is extremely high and so through this


180
00:16:08,380 --> 00:16:15,640
work and through other kind of sort of related research, we found that like the broader defenses


181
00:16:15,640 --> 00:16:20,800
that need to be there are just simply not. And so when we spoke to archivists, these


182
00:16:20,800 --> 00:16:25,440
were things like really simple things, such as an archival team in an institution has


183
00:16:25,440 --> 00:16:30,560
a high degree of consolidated institutional power, they have legal protections, they might


184
00:16:30,560 --> 00:16:36,800
have a full legal team that might help them to maintain their, they might live in a jurisdiction


185
00:16:36,800 --> 00:16:42,520
that grants them immunity from holding certain types of material, but then because they work


186
00:16:42,520 --> 00:16:48,120
alongside people who are freelancers and things like that, when those people then take a mirror


187
00:16:48,120 --> 00:16:52,840
of their IPFS collection and start engaging with it, those people don't have the same


188
00:16:52,840 --> 00:16:56,360
legal resources because they're not a part of those institutions, they don't have, they


189
00:16:56,360 --> 00:17:00,880
might live in different jurisdictions that see certain types of information as not having


190
00:17:00,880 --> 00:17:04,680
the same immunity that the people in sort of institutions might have. And so you have


191
00:17:04,680 --> 00:17:10,080
immediately just through the interactions between different people, the injection of


192
00:17:10,080 --> 00:17:16,280
IPFS or a similar decentralized system creates an entire sort of Merkle tree, if you like,


193
00:17:16,280 --> 00:17:22,360
of potential legal fragility, just in that act of like collaborating between people in


194
00:17:22,360 --> 00:17:30,600
this sort of decentralization first strategy. The fourth finding is that the shortcomings


195
00:17:30,600 --> 00:17:35,120
of today's tools affect the quality of archives. So this is essentially that like we have a


196
00:17:35,120 --> 00:17:43,520
series of tooling that has, it excels in some ways and doesn't excel in other ways. Immutability


197
00:17:43,520 --> 00:17:48,040
is part of this. It's also how the tools work and how quickly things can be retrieved, all


198
00:17:48,040 --> 00:17:53,280
these sorts of things. Over time, as people use them, they then form opinions around what's


199
00:17:53,280 --> 00:17:57,520
possible and what's not possible, which then becomes assumptions on the user side, which


200
00:17:57,520 --> 00:18:02,600
then become culturally entrenched, which then leads to the narrowing of possibilities once


201
00:18:02,600 --> 00:18:08,400
technology or interfaces, new systems have been developed and then people become familiar


202
00:18:08,400 --> 00:18:16,320
with them. So this is important to the context of IPFS because it means that the kind of


203
00:18:16,320 --> 00:18:21,280
historical custodianship that we might have today of like the internet archive being able


204
00:18:21,280 --> 00:18:26,560
to sort of shape or similar institutions being able to shape how these systems work. Once


205
00:18:26,560 --> 00:18:31,480
that moves into a more decentralized space, there's an opportunity here to reset the expectation


206
00:18:31,480 --> 00:18:37,200
of what's possible with large scale storage and large scale archiving, but that is dependent


207
00:18:37,200 --> 00:18:42,520
on the priorities and like the flexibility of everything from the protocol to the interfaces


208
00:18:42,520 --> 00:18:49,800
themselves. The fifth finding is that we need new interfaces for navigating complex archives.


209
00:18:49,800 --> 00:18:53,920
And I think this is related to a couple others, but deserves a special mention to itself.


210
00:18:53,920 --> 00:18:58,840
It also leads into a finding that's coming up in a second as well, which is that as we


211
00:18:58,840 --> 00:19:04,760
have, basically people were overwhelmed. We heard from large numbers of people who are


212
00:19:04,760 --> 00:19:09,160
in some cases very highly technical people who understand how to set up crawlers and


213
00:19:09,160 --> 00:19:14,040
use Docker containers either locally or like spin up a digital ocean droplets, things like


214
00:19:14,040 --> 00:19:19,240
this, and just basically go at it and start like automatic crawling services. And when


215
00:19:19,240 --> 00:19:24,400
people started to use those systems, they would then pull back from that when they started


216
00:19:24,400 --> 00:19:29,960
to then either curate or otherwise navigate or manage their collections. And in many cases,


217
00:19:29,960 --> 00:19:35,200
we saw working like practice of archivists, people who had been in these industries for


218
00:19:35,200 --> 00:19:42,080
like 20 or 30 years, even longer using literally Excel spreadsheets to basically navigate massive


219
00:19:42,080 --> 00:19:46,240
data sets that some, again, some of which were, you know, still using things like IPFS.


220
00:19:46,240 --> 00:19:51,520
So even with this idea of having like a navigation structure within the protocol or, you know,


221
00:19:51,520 --> 00:19:56,920
a navigation structure or even searchable structures inside of tools like web recorder,


222
00:19:56,920 --> 00:20:23,920
what we actually found on the ground is that all of those systems were not actually navigable.


223
00:20:00,000 --> 00:20:29,240
Ã¾Euexactly meeting the needs of the complexities of how people were struggling to use these systems. And so, yeah, the most astonishing thing was finding people who had massive, you know, terabytes upon terabytes of archive material who use markups within the archive so they modify and add annotations inside the archives for themselves and then keep track of things through these signposts and a giant Excel spreadsheet that allows them to main, that basically acts as like a giant menu system.


224
00:20:30,240 --> 00:20:37,080
And that, I think, remains one of the hardest and most interesting problems from a pure user side of dealing with this material.


225
00:20:37,080 --> 00:21:06,120
Alongside that is another thing which I think is relevant to IPFS, which is that we found that tool design influences the effects of traumatic content. And what that meant is that in the discussion that we spoke, when we spoke to people, and this is covered in the report as well, because of the time frame of when we started doing the work, which was November 2021, the war in Ukraine basically started, you know, a few months into the work that we were doing.


226
00:21:07,800 --> 00:21:18,120
And we actually lost a number of people who we wanted to interview because their commitments to, they're basically their entire priorities shifted in the first half of 2022.


227
00:21:18,120 --> 00:21:37,120
At the same time, as everybody knows, we were coming out of the kind of deep lockdowns of COVID. And so we spoke to a number of people who had been doing work in some cases for decades and had been following material organizations, archiving, you know, groups or like movements and things.


228
00:21:37,120 --> 00:22:07,080
And were then beginning to see, you know, the traumatic events of like large scale death emerging through the stories of the archives that they were basically curating. And what we found there is that as we reflected on that with the participants, there were participants who were very clearly able to show examples of how the decisions that had been made by a system design team and user experience research team to prioritize or create flows in which they could review content.


229
00:22:07,800 --> 00:22:33,200
Or display large amounts of content, whether it was in grid, whether it was algorithmically surfaced for them, whether it was a tagging system, whatever it was, different ways in which interfaces worked to then surface kind of content, different types of content had the potential to amplify or deaden the potential traumatic effects of violent or traumatic, other forms of traumatic content.


230
00:22:33,200 --> 00:22:53,200
And so in our landscape study, we kind of looked at this and we started to look into the idea of like, is there a lot of material? How much research is there into the idea of the psychological effects of traumatic content and the way that interfaces and systems can amplify that? And you'd be surprised, or maybe not, to learn that there's actually not that much out there about that.


231
00:22:53,200 --> 00:23:05,200
And so this is another area that I think that once you then deploy something at scale, at a protocol level, that there's an opportunity here to explore that and see how the protocol can mitigate some of that.


232
00:23:05,200 --> 00:23:27,200
Number seven, the colonial methodology and language has narrowed the potential for web preservation. This is a, I think, this was touched on a little bit earlier in some of the talks today. The idea here that like, differing perspectives are necessary in order to understand how computing can be, how we can transcend some of the limitations of computing today.


233
00:23:27,200 --> 00:23:45,200
The ways in which we find that immediately is in the sort of seeding of power for indigenous or other non-Western perspectives. This is true for privacy. It's true for decentralization. And it's true for even things like what is a permanent record? What is the source of truth?


234
00:23:45,200 --> 00:24:07,200
All of these, I think, can be challenged in a very healthy way by creating and amplifying those spaces and kind of de-centering the Western context from these conversations and also de-centering the kind of influence of the Western sort of explosion of computing and its computing culture that's sort of happened from the 1970s through to today.


235
00:24:07,200 --> 00:24:33,200
And some of that even can be seen in language, right? So things like the term capture in archiving has direct connotations to the kind of, you know, the extremely racist kind of late 1800s, 1900s and beyond act of, you know, literally taking stuff out of indigenous societies and hauling it back to Western countries, things like that.


236
00:24:33,200 --> 00:24:47,200
But that sort of like, that kind of language, which, you know, is deeply discussed as part of critique of archival in an academic sense and archival politics in an academic sense, doesn't translate into our sort of space.


237
00:24:47,200 --> 00:25:12,200
And just to add an extra provocation to that, one thing that came up in the discussions with participants was how archive as a term has kind of come to mean half delete in like things like emails applications or in your chat apps where you have the ability to delete something, but you also have the ability to archive something, which is kind of sort of a delete.


238
00:25:12,200 --> 00:25:32,200
And so even in these contexts of like the narrowing of the language of archival services as it's being archival practice as it's being used in other contexts has like potential dramatic effects in very subtle ways around how users and designers and systems people understand like what these actions actually mean, essentially.


239
00:25:32,200 --> 00:25:56,200
How am I good for time? Okay, a couple minutes. All right. So, eight, digital archiving is vulnerable to political and ecological threats. We've been doing a lot of research. It was backed up in this particular work around things like the decoupling of the sort of global supply chain and the global connectivity networks and how the kind of volatility around that threatens the kind of maintenance and storage of archives.


240
00:25:56,200 --> 00:26:09,200
We've looked at some of the ways in which things like the merging of the fusing of Filecoin alongside IPFS has like tremendous benefits, but also has tremendous drawbacks.


241
00:26:09,200 --> 00:26:31,200
For example, it makes it more difficult to use in places like China because of the connotations of blockchains that are not controlled by the state, for example. And that is a discussion that's like the New Design Congress isn't interested in so much as how that can change rapidly for people on the ground based on the sort of mood of the day.


242
00:26:31,200 --> 00:26:46,200
And that's sort of where we got into with the participants. This one here is a big, like if you're interested in this, then like there's a lot of work on New Design Congress that like delves into this.


243
00:26:46,200 --> 00:27:03,200
But essentially, the broader kind of as the kind of present begins to sort of destabilize, the ways in which that can have an adverse effect is just not really understood very well. And no more is that really visible than in the context of like archive integrity.


244
00:27:03,200 --> 00:27:18,200
So I touched on this at the beginning. Archive integrity is an interesting context because while we believe collectively that like there should be sources of truth and immutability, things like this, one of the challenges that come with that is that they introduce these new kinds of threats that aren't really dealt with.


245
00:27:18,200 --> 00:27:39,200
The before like when we started this work back in 2021, Twitter had not yet been bought by Elon Musk. The just the sheer fact that something can change so rapidly in the custodianship of an archive, in this case, the entire historical archive of every tweet ever made can be basically put into private hands.


246
00:27:39,200 --> 00:27:58,200
This is like a threat that we collectively haven't dealt with. And I talked about this a little bit when I talked about like the attacks of capital on Web3. This is an example of like something like what happened with Twitter could absolutely happen with a Web3 IPFS-backed service as well. Absolutely.


247
00:27:58,200 --> 00:28:18,200
But the other thing too is that when people talked about in as archivists, they talked about archival systems and integrity. There's this huge issue of people collecting information such as on Facebook or Instagram or other services that require a login, which then leaks personal data into those archives, which then needs to be redacted.


248
00:28:18,200 --> 00:28:37,200
And so what we have are these sort of this kind of this tension point between the desire for to create authenticity where we can say cryptographically that something hasn't been tampered with and the need, the real world today need of people to be able to redact any kind of personal information that leaks into an archive.


249
00:28:37,200 --> 00:28:50,200
And those two needs, those two goals are fundamentally diametrically opposed to each other. And there is not that much discussion into how to actually begin to start to properly solve that.


250
00:28:50,200 --> 00:29:08,200
Finally, the emerging tools such as WebRecorder, OpenArchive, these other tools that exist that are kind of the next generation, if you like, of open source tooling that would rely as the first adopters for services and protocols like IPFS are entering a field that desperately needs them but struggles with their shortcomings.


251
00:29:08,200 --> 00:29:20,200
And so as part of the research, because it was done in conjunction with WebRecorder, was to look at how WebRecorder and its ecosystem is being received by its user base. And the struggle there is real.


252
00:29:20,200 --> 00:29:33,200
The service is widely known to be quite good, but the ways in which the shortcomings affect people create a tension point there that goes from the interface level, which they're trying to solve now, all the way down to the protocol and deeper systems design.


253
00:29:33,200 --> 00:29:51,200
And it's that stack that I think IPFS can go towards solving more broadly for its user base. And I mean, there's a lot of work going into this, but I think the depth of how important this is today is potentially not as well understood or not as surface as it could be.


254
00:29:51,200 --> 00:30:03,200
Because essentially, overall, after 20 plus years of relative stability, the threats to large-scale data archive projects are now very, very suddenly here.


255
00:30:03,200 --> 00:30:23,200
From the deteriorating political situation in the United States to the deployment of surveillance infrastructure disguised as pandemic response to the unjustified war in Ukraine, the political assumptions of the way in which we collect and store and retrieve large data sets as archives are being tested in real time.


256
00:30:23,200 --> 00:30:33,200
And what concerns us as part of this research, as the sort of broader finding that comes out of this, is that there is no structure in place to kind of keep track of how that's happening.


257
00:30:33,200 --> 00:30:44,200
And what needs to happen, I think, is like a coalition or some other set of entities that are keeping track of that collectively in an open-source way.


258
00:30:44,200 --> 00:30:51,200
So, a couple minutes left. What can we do today? Because, you know, not everything is totally screwed.


259
00:30:51,200 --> 00:31:00,200
The first one is the anti-user story, which is essentially if the user story, if we've got an x-y axis, x-axis is ease of use and y is user happiness.


260
00:31:00,200 --> 00:31:06,200
A user story, which is a flawed way of doing design, but, you know, let's roll with it for anyway.


261
00:31:06,200 --> 00:31:12,200
User story, you want in the top right there. Ease of use, you want it super easy to use and you want your users to be really happy.


262
00:31:12,200 --> 00:31:23,200
An anti-user story is where you do the same sort of design system and design thinking, but where you want your user to be super sad and find the system really awful to use.


263
00:31:23,200 --> 00:31:31,200
Which is essentially the same as saying that digital security and user experience design are actually the same thing, but with like diametrically opposed goals.


264
00:31:31,200 --> 00:31:46,200
Whereas a user experience research team is working to make a system accessible, easy to use, frictionless. A digital security team is working to make that user, which is the attacker, as miserable as possible in the system.


265
00:31:46,200 --> 00:31:53,200
And so the idea is to corrupt your designers. Use design to make horrible UX for your attackers.


266
00:31:53,200 --> 00:32:01,200
And be aware of this concept of weaponized design, which is when a system harms users while behaving precisely as it's designed to.


267
00:32:01,200 --> 00:32:10,200
There's a little bit more on this term. It's a term that we use a lot at New Design Congress. There's an essay called Unweaponized Design on the site. I don't have time to jump into it today.


268
00:32:10,200 --> 00:32:17,200
The second is practicing alternative forking. So in the same way as we fork code, we need to fork concepts.


269
00:32:17,200 --> 00:32:32,200
So on the right here, you see Do Not Fold, Mutilate, or Spindle a Cultural History of the Punch Card, which is an amazing 1994 paper that talks about the civil rights era critique of digital identity as we know it today.


270
00:32:32,200 --> 00:32:38,200
And how close it got to basically dismantling what we understand of digital identity in this moment.


271
00:32:38,200 --> 00:32:51,200
It started in UC Berkeley as people's academic records were serialized into punch cards, and the Do Not Fold, Mutilate, or Spindle was like the kind of political rallying cry against the punch card, which said, I am not a punch card.


272
00:32:51,200 --> 00:33:05,200
I cannot be folded, mutilated, or spindled either. And so the idea here is to disconnect implementation from risk, looking to history and non-Western critical perspective as part of that not speculative but rather alternative forking of history.


273
00:33:05,200 --> 00:33:15,200
So you take an idea that you have today, and you look into history, and you say, okay, so what critiques were there that weren't tried, and how do they apply to us today?


274
00:33:15,200 --> 00:33:18,200
And then you fork that, and you go conceptually down that road.


275
00:33:18,200 --> 00:33:27,200
So a really good example that Robin and I talked about last night or shouted at each other at dinner last night was what if SMTP was invented after Web 3?


276
00:33:27,200 --> 00:33:56,200
So the idea is, aside from the fact that this is a preposterous sort of thing because email is the absolute basic component of the Internet, if you put all of that limitation or implementation aside and think purely conceptually, if SMTP was invented after Web 3, everything we know of what happened with SMTP, how it was abused for spam and then consolidated into Google, Microsoft, and Apple,


277
00:33:56,200 --> 00:34:11,200
as like the primary gatekeepers of the entire protocol, what would have happened if the Web 3 struggles of today, especially around fraud and attack from similar or higher levels of capital is what came after SMTP back in the day,


278
00:34:11,200 --> 00:34:18,200
what would that look like if a similar protocol had that emerged in a landscape like this today?


279
00:34:18,200 --> 00:34:30,200
And then it's that sort of thing and that kind of almost near-future science fiction thinking, but not quite, because you're firmly rooted in the things that exist today and then looking backwards into history to find the criticisms that are there.


280
00:34:30,200 --> 00:34:42,200
These are the sorts of ways, I think, in which we can begin to kind of break down some of the biases that we have that are inherent in how we produce systems simply through the nature of the practices and tools that we use to design systems.


281
00:34:42,200 --> 00:34:55,200
And the third thing would be to mitigate in documentation. We have excellent documentation for open source projects. We have no good documentation, I can't think of any single one, that openly embraces the ways in which tools can be abused.


282
00:34:55,200 --> 00:35:12,200
So in this context setting for the work that we did with IPFS and WebRecorder, we looked at real examples of how these tools had been used in abusive circumstances. So we looked at things like harassment campaigns.


283
00:35:12,200 --> 00:35:28,200
We looked at a series of different scenarios that have happened within the last five years that squarely centered the Internet Archive, IPFS, and WebRecorder as the tools that were flashpoints for these sorts of conflicts in the world.


284
00:35:28,200 --> 00:35:48,200
And when people sort of talk about the role or responsibility of an open source project in being abused by bad actors, it's a very uncomfortable conversation. A lot of people kind of shift in their seats and are like, well, we don't condone it, but the nature of free software is anybody can use it, or we have a license that says you can't do this or that.


285
00:35:48,200 --> 00:36:12,200
But what isn't there is the deep research into how to mitigate your own tools' bad effects, and then the documentation of that so that when someone is the target of a harassment campaign through a decentralized service or something similar, they can come to the documentation or reach out to an organization like Citizen Lab, Amnesty International, Torpopy, these kinds of crisis response for digital harassment or other sort of digital security risks,


286
00:36:12,200 --> 00:36:36,200
and actually begin to, rather than making those people have to spend time researching and sort of assessing the tool set from the outside, if the documentation is there and it's like alongside how to spin up a Docker container for your new service, that I think goes a long way to proactively allowing people to effortlessly, and organizations to effortlessly produce mitigation strategies in the context that they are working in,


287
00:36:36,200 --> 00:36:50,200
whether it's targeted journalists, whether it's employee, in the case of Torpopy, it's employees being harassed in sort of more civil context rather than governments going after journalists and things like that.


288
00:36:50,200 --> 00:37:00,200
Finally, this is very deep. Thank you for staying with me. It's covered a lot just now. The future. Here's the report, Memory and Uncertainty, Web Preservation in the Poly Crisis.


289
00:37:00,200 --> 00:37:14,200
It's a 94-page report. So if you've got a plane ride back to the United States, you should be able to read some of it. If you're bored and you've run out of movies to watch, the report is there, and it's pretty in-depth.


290
00:37:14,200 --> 00:37:32,200
It's available at members.newdesigncongress.org. It's free. We are going to present this work at two academic conferences this year in greater detail. That's the IIPC and RESOAR, both of which are archival-based academic conferences.


291
00:37:32,200 --> 00:37:44,200
RESOAR is actually peer-reviewed, and that got peer-reviewed, and it was very favorably peer-reviewed. I won a little gold star for that one. That was excellent.


292
00:37:44,200 --> 00:37:56,200
Finally, working with IPFS-interested projects. Right now we're working with Open Archive. We worked with Web Recorder. We did anti-user story work with Web Recorder. You can go to the Waxy Specs GitHub issues and find how we did that with them.


293
00:37:56,200 --> 00:38:03,200
Finally, and this is my call to action today, is that we need to broaden this research. Oh my god, right now is the time to be doing this work.


294
00:38:03,200 --> 00:38:18,200
As I said, we're really interested in this idea of some sort of informal or formal coalition of tracking and maintaining a repository of information that gets to the depths of the intricacies of how these systems are going to be used in the future,


295
00:38:18,200 --> 00:38:26,200
especially decentralization, especially within the goals that we have of a complete paradigm shift in terms of computing.


296
00:38:26,200 --> 00:38:36,200
If you're an individual person and you find this work compelling or anything we do, you can become a New Design Congress member at members.newdesigncongress.org. It helps us stay independent.


297
00:38:36,200 --> 00:38:49,200
We do have a fiscal sponsor, which is Super Bloom Design, which is a US Fiber 1C3 organization. It also does UX work, and if you need UX work for your project, they are excellent.


298
00:38:49,200 --> 00:38:53,200
I'm not just saying that because they're my fiscal sponsor, but also because they're amazing people.


299
00:38:53,200 --> 00:39:07,200
More broadly, this is the work that we're interested in and the trajectory that we're working on, and I invite you to spend some time looking through the New Design Congress stuff, and I hope that you found this crash course in a very deep research project informative.


300
00:39:07,200 --> 00:39:08,200
Thank you so much.


301
00:39:08,200 --> 00:39:26,200
Thank you very much.


302
00:39:26,200 --> 00:39:42,200
I was reading your report earlier this week, and I think it was point three you mentioned up here was one of the ones that I struggled with a little bit. You talk about being aware of the risks of archiving either incriminating material or material that's linked to harassment campaigns and so on.


303
00:39:42,200 --> 00:40:00,200
One of the things I had a hard time reconciling was the way that same material often ends up being useful as part of holding the state accountable for state violence or Kiwi Farms accountable for the harassment that they're subjecting people to and for your work.


304
00:40:00,080 --> 00:40:02,400
purelySze in particular I'm reminded specifically of a report that came out


305
00:40:02,400 --> 00:40:06,300
this week out of a forensic architect I believe that used footage from the


306
00:40:06,300 --> 00:40:12,960
Portland protests to reconstruct the use of tear gas in those protests to try to


307
00:40:12,960 --> 00:40:16,380
classify that Portland Police Department is having committed a war crime.


308
00:40:16,380 --> 00:40:19,760
So I was just wondering if you had any comment on kind of that dichotomy and


309
00:40:19,760 --> 00:40:21,240
how to navigate that?


310
00:40:24,040 --> 00:40:26,040
Sorry that's a dead question.


311
00:40:26,040 --> 00:40:39,280
Do I have a comment on that? There's like 20 things in my head. It comes down to


312
00:40:39,280 --> 00:40:45,360
there's a mistake I think more broadly that we think that the ways in which we


313
00:40:45,360 --> 00:40:50,360
have to respond is through some sort of, that's not something that we navigate


314
00:40:50,360 --> 00:41:00,720
through technology, but at the same time like, again there's like


315
00:41:00,720 --> 00:41:03,760
literally five things I want to say all at once and they're just coming out of


316
00:41:03,760 --> 00:41:06,920
my head at the same time. I guess the thing is that like one of the problems


317
00:41:06,920 --> 00:41:11,440
that we have is, and I think this is where New Design Congress kind of, I hope


318
00:41:11,440 --> 00:41:19,200
that this is like how people see us, is that on one side we have policy


319
00:41:19,200 --> 00:41:24,400
or tech criticism that's actually become very publicly visible and very


320
00:41:24,400 --> 00:41:32,280
accessible, but also tends to revel in the sort of the kind of conflict itself.


321
00:41:32,280 --> 00:41:36,480
And on the other side we have sort of people who are building technology who


322
00:41:36,480 --> 00:41:40,440
either engage in the concepts of things like design ethics, which essentially


323
00:41:40,440 --> 00:41:44,160
changes nothing because it's the systems themselves that we've developed and the


324
00:41:44,160 --> 00:41:47,200
context in which we're building them that creates bad outcomes, not whether


325
00:41:47,200 --> 00:41:51,560
or not you are, you know, thinking of design justice as you're building


326
00:41:51,560 --> 00:41:55,360
something. It can help a little bit and it can make the spaces, like the


327
00:41:55,360 --> 00:41:59,520
conditions, the material conditions of people building stuff more equitable and


328
00:41:59,520 --> 00:42:03,160
better, but it doesn't necessarily lead to good outcomes. And so I think there's


329
00:42:03,160 --> 00:42:11,480
a big gap between like how tech in the 2020s, the 2020ns, how tech in the


330
00:42:11,480 --> 00:42:17,400
2010s handled the kind of growing issues with, like that you've sort of talked


331
00:42:17,400 --> 00:42:20,680
about here, the weaponization of the design of technology, but at the same


332
00:42:20,680 --> 00:42:25,680
time we've reached a point with like policy and criticism where


333
00:42:25,680 --> 00:42:30,040
it's very reactive, it deals with things after they've arrived. I mean a really


334
00:42:30,040 --> 00:42:33,160
great example of this is like TikTok, right? I gave a talk last month in


335
00:42:33,160 --> 00:42:36,960
Amsterdam at the Institute of Network Cultures where I said the first half of


336
00:42:36,960 --> 00:42:40,440
it was literally like a bunch of people showing off their academic research into


337
00:42:40,440 --> 00:42:44,360
the TikTok algorithm, which is great and useful, but it's also not progressive


338
00:42:44,360 --> 00:42:47,920
because in order for it to be progressive it has to come before you


339
00:42:47,920 --> 00:42:51,920
can like see it on TikTok, because as everybody here knows, the version of


340
00:42:51,920 --> 00:42:57,120
TikTok that's in the public is at least six months old, maybe older,


341
00:42:57,120 --> 00:43:01,080
right? It's been tested internally for however long and what you're seeing


342
00:43:01,080 --> 00:43:05,280
here is essentially like the platform equivalent of the past. And so while we


343
00:43:05,280 --> 00:43:10,240
have that disconnect, we're never going to fully grapple with, we're always


344
00:43:10,240 --> 00:43:13,240
going to be sort of blindsided in this kind of way, right? Where we're never


345
00:43:13,240 --> 00:43:17,760
really going to be able to properly grapple with the potential for


346
00:43:17,760 --> 00:43:22,840
harm or the potential for even just like what like just destabilization in


347
00:43:22,840 --> 00:43:26,640
individual contexts, or then I think the other danger with that too is that


348
00:43:26,640 --> 00:43:32,920
there's a rising sense of, that starting with cybersecurity, a rising sense that


349
00:43:32,920 --> 00:43:37,440
protocol designers must be responsible for the integrity of those systems. I


350
00:43:37,440 --> 00:43:41,280
mean there's an AI paper, like the draft legislation has come out from the


351
00:43:41,280 --> 00:43:44,600
Chinese government about artificial intelligence and the guidance around how


352
00:43:44,600 --> 00:43:48,160
Chinese companies can use consumer-facing artificial intelligence. One of the


353
00:43:48,160 --> 00:43:51,880
things they have there is if your AI lies to your user, you're responsible for


354
00:43:51,880 --> 00:43:56,600
that, right? And so like the kind of tide is sort of shifting towards this idea


355
00:43:56,600 --> 00:44:00,720
that like platforms are culpable, which is very different from like section 23


356
00:44:00,720 --> 00:44:04,360
or these sorts of things, like the idea that like whatever happens on the


357
00:44:04,360 --> 00:44:08,080
platform is free speech in the United States, like that sort of thing might


358
00:44:08,080 --> 00:44:11,520
stay, but what will emerge in its place are things like you're responsible for


359
00:44:11,520 --> 00:44:16,680
the editorialization of your algorithm, or you're responsible for, you know, you


360
00:44:16,680 --> 00:44:20,840
can be sued alongside someone who gets burned by a decentralized archive, these


361
00:44:20,840 --> 00:44:24,700
sorts of things. And so I don't know if that answers your question, I think the


362
00:44:24,700 --> 00:44:28,300
point I'm trying to make is that it's like super complicated. Yeah, I think it


363
00:44:28,300 --> 00:44:33,360
does, and just to summarize to make sure I'm understanding you, is kind of the


364
00:44:33,360 --> 00:44:36,720
gist of what you're saying, that it's not necessarily about archiving less, but


365
00:44:36,720 --> 00:44:39,960
about being more proactive about understanding how what you're archiving


366
00:44:39,960 --> 00:44:44,080
can be weaponized, and trying to contextualize that information in ways


367
00:44:44,080 --> 00:44:47,760
that defends against that. And it comes down to, it comes really down to this


368
00:44:47,760 --> 00:44:52,040
idea of like the provocation of adding documentation. If you've got


369
00:44:52,040 --> 00:44:55,400
documentation for all of the ways in which you can troubleshoot an open


370
00:44:55,400 --> 00:44:59,680
source application, like why don't we have that but for like the kind of


371
00:44:59,680 --> 00:45:04,080
socio-technical issues that you might encounter, right? So like I can learn


372
00:45:04,080 --> 00:45:08,840
about how to fix my terrible Nextcloud server, but I can't think, I don't have


373
00:45:08,840 --> 00:45:13,640
like the equivalent of like how to deal with like the issues that


374
00:45:13,640 --> 00:45:16,440
might arise just from a social perspective or a political perspective.


375
00:45:16,440 --> 00:45:20,800
And again, this is like in lockstep with open sources inability to properly


376
00:45:20,800 --> 00:45:23,440
grapple with like moderation tooling and all sorts of things like that.


377
00:45:23,440 --> 00:45:31,680
Okay, awesome. Thank you. Good question. Thank you. Very, very hard question.


378
00:45:31,680 --> 00:45:35,680
Any others? There's a lot there.


379
00:45:35,680 --> 00:45:37,680
We'll leave it there for lunch. Thank you very much.


380
00:45:37,680 --> 00:45:55,680
Thank you so much.
