1
00:00:00,000 --> 00:00:10,760
The goal of Ceramic is we are trying to build a database where you can get nice, fast, web-to-like


2
00:00:10,760 --> 00:00:16,660
read performance, but still have shared composable data. So lots of people can contribute data


3
00:00:16,660 --> 00:00:21,920
to the datasets, lots of people can consume the data from the datasets, but your app doesn't


4
00:00:21,920 --> 00:00:26,560
get screwed because somebody else's app did something funny with the way their data is


5
00:00:26,560 --> 00:00:30,840
working. You can actually pull the datasets and compose them in infrastructure that you


6
00:00:30,840 --> 00:00:36,960
control so that you can have that reliability. Whether that counts as web 3 or not, we can


7
00:00:36,960 --> 00:00:43,600
decide. So let's talk about immutable data. To me, immutable data looks like this. You


8
00:00:43,600 --> 00:00:51,000
have some value. This value could be small, the number 42, this value could be large,


9
00:00:51,000 --> 00:00:58,160
a cache of the current state of Wikipedia. But the value just is the value. All 42s are


10
00:00:58,160 --> 00:01:04,080
equal to 42s somewhere else. I never mutate my 42 and break your application which was


11
00:01:04,080 --> 00:01:10,120
relying on the fact that your 42 continued to work. But it's not always convenient to


12
00:01:10,120 --> 00:01:15,360
work directly with immutable data because sometimes we want to have things that can


13
00:01:15,360 --> 00:01:21,080
actually be communicated or retrieved. So that brings us to what I call semi-immutable


14
00:01:21,080 --> 00:01:27,440
data. If I give you a CID, a lot of people will say, ah, you're retrieving a CID, this


15
00:01:27,440 --> 00:01:32,880
is immutable data, there's only one value that that CID could be. This is, of course,


16
00:01:32,880 --> 00:01:37,800
a lie. There's two values when you try to retrieve a CID. If I go to your system and


17
00:01:37,800 --> 00:01:44,640
say, give me this CID, your response can be, here is the value that has that hash, or I


18
00:01:44,640 --> 00:01:51,480
don't know. And a server can go from I don't know to knowing the value for that hash, and


19
00:01:51,480 --> 00:01:56,200
you can delete it and go from knowing the value of that hash back to I don't know. So


20
00:01:56,200 --> 00:02:01,400
I call this a semi-immutable state. There's exactly two states for that hash. Either I


21
00:02:01,400 --> 00:02:06,000
can give you the data or I don't know. So now we've got improvements. I can give you


22
00:02:06,000 --> 00:02:11,840
the CID of Wikipedia rather than all of Wikipedia, and now I can transfer it in a text message


23
00:02:11,840 --> 00:02:16,960
and you can go look that up offline somewhere else without me having to send all of that


24
00:02:16,960 --> 00:02:23,200
data. But a lot of our applications want even more flexibility than this. You know, a lot


25
00:02:23,200 --> 00:02:26,760
of artists, they make a song and they want to share it on their website, but they don't


26
00:02:26,760 --> 00:02:34,280
actually want to name their song by the SHA-256 of the MP3 data of their song. So we end up


27
00:02:34,280 --> 00:02:41,820
wanting sequential data. So I have some particular name that I've bound to the data, and at


28
00:02:41,820 --> 00:02:48,800
some point in the future, I say, hey, I want my tag that used to mean red to now mean yellow.


29
00:02:48,800 --> 00:02:54,380
So I can announce that to the world and people who are following my tag, it goes from red


30
00:02:54,380 --> 00:02:58,960
to yellow. And then at some point later, I want to update that thing again and it goes


31
00:02:58,960 --> 00:03:03,720
to green. And we've all experienced this. When I tell someone that I have installed


32
00:03:03,720 --> 00:03:10,760
Firefox on my computer, I don't mean a particular moment in time. It is my expectation that


33
00:03:10,760 --> 00:03:16,200
that Firefox is going to be continuously updating and continuously moving to new versions of


34
00:03:16,200 --> 00:03:21,520
that data. This is what we expect from our data. I don't want a snapshot of Wikipedia


35
00:03:21,520 --> 00:03:27,180
forever. I want something where as people make edits to Wikipedia, I can continue getting


36
00:03:27,180 --> 00:03:32,520
that new data, but still have authenticated ways to know this is really Wikipedia and


37
00:03:32,520 --> 00:03:40,120
this is the new version of that. And that works fine as long as there's only one updater.


38
00:03:40,120 --> 00:03:46,500
We don't live in a world with only one mutator. We live in a world where data can diverge.


39
00:03:46,500 --> 00:03:52,200
So let's say I have some social media account, you know, I'm doing a blue sky like thing,


40
00:03:52,200 --> 00:03:57,080
and I've got a phone and a tablet and a laptop, and each of those are capable of tweeting


41
00:03:57,080 --> 00:04:05,240
into my decentralized Twitter. What happens if both my laptop and my phone go offline,


42
00:04:05,240 --> 00:04:09,300
make a change to the profile and then come back online? Eventually, I'm going to have


43
00:04:09,300 --> 00:04:14,400
to merge that data back together. And if there's a merge conflict, we're going to have to come


44
00:04:14,400 --> 00:04:20,920
up with some way to make that consistent. And in databases, there is a whole set of


45
00:04:20,920 --> 00:04:28,200
things that you can do in CRDTs in this fork and merge model. And there's a whole bunch


46
00:04:28,200 --> 00:04:34,800
of things that you can only get if you have a strict sequential model. So databases have


47
00:04:34,800 --> 00:04:40,860
talked about consistency for a long time. If you look up strict linearizable consistency,


48
00:04:40,860 --> 00:04:45,920
you can see hundreds of different definitions, hundreds of different consistency models.


49
00:04:45,920 --> 00:04:52,000
But basically, they come down to if you want some kind of mutual exclusion on a resource,


50
00:04:52,000 --> 00:04:57,480
you're going to need to have this sequential behavior. And if you don't need that, you


51
00:04:57,480 --> 00:05:03,080
can do things with this lattice-like behavior. So how do we build systems where some of the


52
00:05:03,080 --> 00:05:07,840
edits we want to do, in fact, most of the edits that we want to do, may not need this


53
00:05:07,840 --> 00:05:16,640
kind of strict serializability, but some edits do? So the second thing we come into is we


54
00:05:16,640 --> 00:05:24,200
have to have this concept of time. If I want to do a happens before and say, all right,


55
00:05:24,200 --> 00:05:30,560
if my laptop revokes the keys of my phone, and my phone revokes the keys of my laptop,


56
00:05:30,560 --> 00:05:34,760
whichever one of those events happened first, you took away the permission from the other


57
00:05:34,760 --> 00:05:38,960
device. And whichever one of those events happened second is irrelevant because you


58
00:05:38,960 --> 00:05:43,160
already took away the permission of that device. The phone that's already been revoked can't


59
00:05:43,160 --> 00:05:47,800
revoke the laptop. So we need to put everything in order. And this brings us to this weird


60
00:05:47,800 --> 00:05:55,280
question of, like, well, what does order mean? And what does simultaneous mean? So if I have


61
00:05:55,280 --> 00:06:01,400
these events where event two has the hash of event one and event three has the hash


62
00:06:01,400 --> 00:06:09,400
of the event two, I can know that three happens after two happens after one. And in this diagram,


63
00:06:09,400 --> 00:06:14,800
I don't have a happens before relationship between, say, A and two. So it's fair for


64
00:06:14,800 --> 00:06:21,800
me to say A and two are simultaneous. They both happened before one. They both happened


65
00:06:21,800 --> 00:06:27,240
after one. They both happened before three. And that's when you start realizing that


66
00:06:27,240 --> 00:06:34,720
distributed systems are really not intuitive. Because event A is simultaneous with event


67
00:06:34,720 --> 00:06:42,400
two. And event B is simultaneous with event two. But event A and event B are not simultaneous


68
00:06:42,400 --> 00:06:53,440
with each other. B is after A. So our simultaneous concept isn't transitive. Which is weird to


69
00:06:53,440 --> 00:06:59,440
me. Like, I learned these laws of logic. I was like, law of identity, these things are


70
00:06:59,440 --> 00:07:05,440
simultaneous, you'd think they would be. So how do we solve these problems of turning


71
00:07:05,440 --> 00:07:12,440
a lattice into sequential data so that I can actually get this straightened out? So we


72
00:07:12,440 --> 00:07:19,760
go to recent distributed systems creator Aristotle, who taught us that time is the measurable


73
00:07:19,760 --> 00:07:26,200
unit of movement concerning before and after. It turns out that if you look at the fundamental


74
00:07:26,200 --> 00:07:33,280
nature, particularly in a distributed system, causality is a very real thing. If I have


75
00:07:33,280 --> 00:07:40,160
one file, and it contains the CID of another file, I know that the file that contains the


76
00:07:40,160 --> 00:07:47,880
CID is younger, and the file that's identified by the CID is older. If I am caused by something,


77
00:07:47,880 --> 00:07:58,000
I must be after it. Cogno ergo sum. So I can get time by building this graph, and building


78
00:07:58,000 --> 00:08:07,040
this graph using hashes. So now I can have this guarantee where I say I want to recover


79
00:08:07,040 --> 00:08:14,240
my sequential behavior. So I have event one, and then I have event two, and I can accept


80
00:08:14,240 --> 00:08:20,240
event two, and now when the blue event shows up and says, hey, I would like to turn from


81
00:08:20,240 --> 00:08:29,640
red to blue, that is going to fail, because someone has already turned from red to yellow.


82
00:08:29,640 --> 00:08:34,240
And when I show up with the green event, it can do the merge and say, hey, I've realized


83
00:08:34,240 --> 00:08:41,800
that turning red to blue never actually happened, so I'm going to rebase that, and instead of


84
00:08:41,800 --> 00:08:46,560
adding blueness to my red, I'm going to add blueness to my yellow, and I'm going to get


85
00:08:46,560 --> 00:08:51,760
a green. So I can sort of rebase that thing in, because I've detected, oh, there was this


86
00:08:51,760 --> 00:08:56,740
event that got thrown out, because it failed that compare and swap. If I was doing this


87
00:08:56,740 --> 00:09:04,280
in memory on one node, I can simply use the CAS instruction that exists inside of a CPU


88
00:09:04,280 --> 00:09:09,760
and say, hey, read this memory address. If the memory address hasn't changed, update


89
00:09:09,760 --> 00:09:14,520
it. If the memory address has changed, it fails, and now I have to do something else


90
00:09:14,520 --> 00:09:19,400
to try and create a new event on top of what would have happened. So we have this long


91
00:09:19,400 --> 00:09:26,480
history of using CAS in the local system to enable multiple threads to do consistent updating


92
00:09:26,480 --> 00:09:31,640
of value. And in our distributed systems, we have the


93
00:09:31,640 --> 00:09:37,400
cap theorem. The cap theorem is usually presented as under partition, you're going to have to


94
00:09:37,400 --> 00:09:45,120
choose between consistency and availability. If I split-brain my system, and I allow each


95
00:09:45,120 --> 00:09:50,800
side to continue making progress independently, that's very available, because the split-brain


96
00:09:50,800 --> 00:09:55,240
didn't stop me from making progress, but I become inconsistent because I've gone in two


97
00:09:55,240 --> 00:10:00,920
different directions. If I say, no, when I go split-brain, stop the world, we're not


98
00:10:00,920 --> 00:10:06,800
changing anything until we get back to one brain, now I can stay consistent, I didn't


99
00:10:06,800 --> 00:10:12,640
go into that inconsistent state, but at the same time, I have become unavailable because


100
00:10:12,640 --> 00:10:16,520
I can't make changes, I stopped the world until I came back together.


101
00:10:16,520 --> 00:10:22,000
I argue that this is not actually the right way to think about the cap theorem. It's better


102
00:10:22,000 --> 00:10:28,120
to think about it as availability comes before consistency. The way that I build a consistent


103
00:10:28,120 --> 00:10:33,640
database is to say, I'm going to hold all of the rights until I know that I have reached


104
00:10:33,640 --> 00:10:39,160
a consistent state. I get availability by saying, you can go ahead and read stuff that


105
00:10:39,160 --> 00:10:43,600
hasn't become consistent yet, but that can be a choice that I leave to the user. If I


106
00:10:43,600 --> 00:10:49,440
have two APIs, one that says, read the consistent state, and one that says, read uncommitted,


107
00:10:49,440 --> 00:10:54,020
there's nothing that stops me from letting people continue to make changes and continue


108
00:10:54,020 --> 00:11:01,400
to do things in that CRDT-like merge and join world, and then at some point later, we say,


109
00:11:01,400 --> 00:11:07,520
okay, now we have reached consistency. Everything up until this point in time is final, and


110
00:11:07,520 --> 00:11:11,520
we have this common prefix that the whole world agrees on, and everything after this


111
00:11:11,520 --> 00:11:18,240
point in time is in fuzzy eventual consistency CRDT land, and eventually that stuff will


112
00:11:18,240 --> 00:11:23,920
be merged, and we'll say, now I can append to the consistent tail. And that will enable


113
00:11:23,920 --> 00:11:28,880
us to build systems that are available because we can build things now, and if you need to


114
00:11:28,880 --> 00:11:32,800
do consistent things, you just have to wait a little bit longer for consistency and really


115
00:11:32,800 --> 00:11:38,080
make this a choice for the application developer rather than a choice for the database. If


116
00:11:38,080 --> 00:11:42,800
I want to switch back and forth between a consistent model and an available model, maybe


117
00:11:42,800 --> 00:11:49,200
the right way to do that isn't rewrite your entire application for a consistent database


118
00:11:49,200 --> 00:11:54,800
or rewrite your entire application for a eventual consistency database. You know, if my answer


119
00:11:54,800 --> 00:12:00,480
is switch from Postgres to Cassandra or switch from Cassandra to Postgres, that's pretty


120
00:12:00,480 --> 00:12:05,920
heavy weight. Maybe it should just be a query type. And we can implement this by just saying,


121
00:12:05,920 --> 00:12:09,520
look, we're holding things for consistency and we're making things available, and the


122
00:12:09,520 --> 00:12:15,280
application has to make an intelligent decision about that. And this gives us the ability


123
00:12:15,280 --> 00:12:20,400
to operate on time. If you have a globally distributed system, I can make a consistent


124
00:12:20,400 --> 00:12:27,840
write on memory on a single node using the CAS instruction in nanoseconds. I can get


125
00:12:27,840 --> 00:12:32,640
durability where I say I know that I have a copy of this data, and if my node fails,


126
00:12:32,640 --> 00:12:36,360
some other node has a copy of the data. I can get that fast. I just have to get it to


127
00:12:36,360 --> 00:12:43,120
any other computer. If I want majority, now I start needing something like 51%. This is


128
00:12:43,120 --> 00:12:48,560
where you get your Paxos and your Raft. These are much slower than getting durability because


129
00:12:48,560 --> 00:12:52,680
I don't have to give it to any arbitrary computer who's going to remember my event. I have to


130
00:12:52,680 --> 00:12:58,200
get it to the specific computers that are the consensus group for my thing. And if I


131
00:12:58,200 --> 00:13:04,120
want finality, now I need two thirds plus one and probably a three phase commit. I have


132
00:13:04,120 --> 00:13:08,680
to do a Byzantine fault tolerant consensus algorithm, and those are really slow. It's


133
00:13:08,680 --> 00:13:14,600
going to take you something like three round trips to the consensus group to get that.


134
00:13:14,600 --> 00:13:20,120
These can take a really long time, and you're just not ever going to get really fast transactions


135
00:13:20,120 --> 00:13:27,120
out of that. I think seven seconds is about as fast as we're going to get for things like


136
00:13:27,120 --> 00:13:33,160
global scale blockchains, dropping blocks. So the thing that I'm introducing here is


137
00:13:33,160 --> 00:13:39,520
to talk about the ceramic anchor service. This is our CAS for distributed systems. How


138
00:13:39,520 --> 00:13:43,900
do we make it so that we know we can do a compare and swap and turn those lattice-like


139
00:13:43,900 --> 00:13:50,380
structures into this is an exact set of events that happened in an exact total ordering,


140
00:13:50,380 --> 00:13:56,560
and we can all agree on this is the common prefix of our system. And it's a relatively


141
00:13:56,560 --> 00:14:03,160
straightforward idea. I need something that is my change over time, as Aristotle said


142
00:14:03,160 --> 00:14:10,120
it. If I have a server that is publishing a new timestamp event every few seconds, and


143
00:14:10,120 --> 00:14:13,120
it's just like, here's an event, it's got a hash in it. Here's an event, it's got a


144
00:14:13,120 --> 00:14:15,920
hash in it. Here's an event, it's got a hash in it. Here's an event, it's got a hash in


145
00:14:15,920 --> 00:14:22,480
it. Now, when I go to make a commit, I can just pull the most recent time event, put


146
00:14:22,480 --> 00:14:28,760
the hash of that time event into my thing, and then submit the hash of my commit so that


147
00:14:28,760 --> 00:14:33,600
it shows up in the next time event. And this doesn't necessarily make the time events big


148
00:14:33,600 --> 00:14:38,680
because I can use a Merkle tree and say, okay, I'm collecting hashes into groups and then


149
00:14:38,680 --> 00:14:43,400
collecting those into groups and collecting those into groups. So even if you had millions or billions


150
00:14:43,400 --> 00:14:48,080
of hashes that were coming into this thing, you could have tree shaped things where you've


151
00:14:48,080 --> 00:14:51,640
got a thousand servers that are collecting events, and then they send theirs to the next


152
00:14:51,640 --> 00:14:57,480
layer that collects them right before it publishes. So the main point is you have some path using


153
00:14:57,480 --> 00:15:04,120
hashes to prove that your commit happens after one of these anchors and before another one


154
00:15:04,120 --> 00:15:09,320
of these anchors. Now your event must have happened in this time window. It can't have


155
00:15:09,320 --> 00:15:15,520
been earlier and it can't have been later. We can use that to put events in order with


156
00:15:15,520 --> 00:15:23,160
the exception that some events are going to end up in this split brain way. And for those


157
00:15:23,160 --> 00:15:28,520
you just need some deterministic rule. So I can say I'm going to put these things in


158
00:15:28,520 --> 00:15:32,680
order by where they appear in the Merkle tree, or I could say I'm going to put them in order


159
00:15:32,680 --> 00:15:38,360
by what is the hash. So if I was trying to order this particular set of events, I could


160
00:15:38,360 --> 00:15:47,960
do one, two, A, B, three, or one, A, B, two, three. And those are equally valid. There


161
00:15:47,960 --> 00:15:51,920
are some advantages to keeping the streaks together. I probably don't want my events


162
00:15:51,920 --> 00:15:59,080
to go one, A, two, B, three, because then I'm going to have split these events and I'm much


163
00:15:59,080 --> 00:16:05,440
more likely to have invalidated B, which was built right off of A. At least this way, if


164
00:16:05,440 --> 00:16:14,360
A was valid, I get that whole chain. All right. So where are we going to get a source of time?


165
00:16:14,360 --> 00:16:18,620
We could use any source of time we wanted. You could have a server run by three box that


166
00:16:18,620 --> 00:16:24,040
just issues these things every few seconds, but we want things that are untrusted. But


167
00:16:24,040 --> 00:16:29,360
fortunately for us, people have spent a lot of time thinking, how do we build a large-scale,


168
00:16:29,360 --> 00:16:34,280
consistent system where it's difficult to mess with when things happen? These are the large


169
00:16:34,280 --> 00:16:41,720
blockchains. So the obvious places to look for a source of time is something like a Bitcoin


170
00:16:41,720 --> 00:16:47,680
or an Ethereum. Mostly we're leaning on Ethereum because the blocks fall much faster. If I


171
00:16:47,680 --> 00:16:53,480
know that this event happened between two Bitcoin blocks, that's just a lot more time


172
00:16:53,480 --> 00:16:57,880
than I know this event happened between two Ethereum blocks. I don't know the exact numbers,


173
00:16:57,880 --> 00:17:03,480
but it's like roughly 10 minutes between Bitcoin blocks and roughly 10 seconds between Ethereum


174
00:17:03,480 --> 00:17:08,480
blocks. If I have to wait for my things to become consistent, I would rather wait 10


175
00:17:08,480 --> 00:17:13,440
seconds than 10 minutes, all else being equal. And since the Ethereum blocks give us this


176
00:17:13,440 --> 00:17:19,840
continuous chain, now I can go back and have my CAS service, collect everything into that


177
00:17:19,840 --> 00:17:26,080
Merkle root, put that Merkle root into a transaction that appears on the Ethereum blockchain. And


178
00:17:26,080 --> 00:17:32,080
now that acts as my source of time. And all of the events in the entire ceramic network


179
00:17:32,080 --> 00:17:38,800
are now in a Merkle tree that is anchored to a Ethereum block and can be put into one


180
00:17:38,800 --> 00:17:43,840
of these total orders. And then it's just a matter of playing all of the events in order.


181
00:17:43,840 --> 00:17:49,880
So if my events are all transactions that are doing some JSON diff, I can just keep applying


182
00:17:49,880 --> 00:17:55,180
the diffs in order. If I have something that went split-brain, I can say, well, this one


183
00:17:55,180 --> 00:18:02,060
was first, so it wins the compare and swap and it gets the diff applied. And this one


184
00:18:02,060 --> 00:18:08,200
showed up second, it loses the compare and swap. I'm not going to apply that diff, but


185
00:18:08,200 --> 00:18:14,320
you're still going to be able to detect, hey, this thing is on a dead branch. And if you


186
00:18:14,320 --> 00:18:20,280
wanted to do something like have a merge algorithm that tries to rebase that data back in, you


187
00:18:20,280 --> 00:18:25,680
could build that. And we're going to probably have more tools in the future to make that


188
00:18:25,680 --> 00:18:32,320
easier. Our algorithm is relatively simple. We think first writer wins. We are going to


189
00:18:32,320 --> 00:18:37,760
take whoever has the lowest block height. And if you tie on black height, then we're


190
00:18:37,760 --> 00:18:42,080
just going to do a random. Whichever event has the lowest hash is going to be the one


191
00:18:42,080 --> 00:18:46,200
that wins. And that's going to start getting things applied to the end of it. And that


192
00:18:46,200 --> 00:18:50,800
will invalidate some other chain. But if I've made a whole bunch of changes in a row and


193
00:18:50,800 --> 00:18:55,520
my first block wins, because everything else is built off of that first block, I can take


194
00:18:55,520 --> 00:19:03,080
that whole chain in. And because the replay is entirely deterministic, every commit is


195
00:19:03,080 --> 00:19:09,080
is every transaction in that chain is either going to emit or is going to abort. So you


196
00:19:09,080 --> 00:19:14,480
can use this to build an arbitrary replicated state machine and build up whatever current


197
00:19:14,480 --> 00:19:21,080
state you need and know that that data is going to be consistent. So this is going to


198
00:19:21,080 --> 00:19:27,360
enable you to build things like names. So we have three IDs, our DID that is built on


199
00:19:27,360 --> 00:19:33,080
top of this. When you're building identities, you're always going to have this choice of


200
00:19:33,080 --> 00:19:39,240
do I want this thing to be local and available or exclusive or chosen? If I just want to


201
00:19:39,240 --> 00:19:44,240
use a DID key, I can generate that locally, but I don't get to pick my name. If I want


202
00:19:44,240 --> 00:19:49,600
something like an email address from Gmail, I can pick what name I want, but I'm going


203
00:19:49,600 --> 00:19:55,360
to need permission from Gmail to get something that is a name within their namespace. Or


204
00:19:55,360 --> 00:19:59,800
I can just do tags. My parents named me Aaron. They didn't have to ask anyone's permission


205
00:19:59,800 --> 00:20:27,860
to...


206
00:20:00,000 --> 00:20:04,200
name me Aaron. Very local, very available. You get to choose what you want.


207
00:20:04,200 --> 00:20:07,800
Unfortunately, there's lots of other people with the same name, and it's very


208
00:20:07,800 --> 00:20:16,920
difficult to Google for me. So, questions. Do people have interests where... I find


209
00:20:16,920 --> 00:20:23,040
that I talk to lots of people, and it's very easy to make the mistake of trying


210
00:20:23,040 --> 00:20:27,480
to do everything as local first software, and saying if you can't do it in a CRDT,


211
00:20:27,480 --> 00:20:31,920
and you can't do it in a duals consistency, you shouldn't do it at all. And I argue


212
00:20:31,920 --> 00:20:35,960
that that's wrong. There's a lot of relatively basic things that people want,


213
00:20:35,960 --> 00:20:41,360
where you want consistency. I came over on an airplane here. I wanted a


214
00:20:41,360 --> 00:20:45,640
guaranteed seat on the airplane. If I showed up and they had said, hey, we know


215
00:20:45,640 --> 00:20:49,680
that you've had this reservation for this flight for months, but somebody else


216
00:20:49,680 --> 00:20:54,240
reserved that seat one second before you, only their laptop was closed, and


217
00:20:54,240 --> 00:20:57,480
they didn't open it till this morning, and it turns out you never actually had


218
00:20:57,480 --> 00:21:03,600
the seat on the airplane. Like, no, I want finality that tells me you have mutual


219
00:21:03,600 --> 00:21:07,280
exclusion rights. You bought this ticket. You bought this seat. You get on the


220
00:21:07,280 --> 00:21:11,680
airplane. But at the same time, the vast majority of what our software is doing


221
00:21:11,680 --> 00:21:18,600
doesn't need that strict linearizability consistency. Most of our software can


222
00:21:18,600 --> 00:21:22,800
actually do things in much fuzzier, much more eventually consistent ways. So I


223
00:21:22,800 --> 00:21:27,760
think we need to be building on databases where consistency is a thing


224
00:21:27,760 --> 00:21:33,520
that you ask for when you need it, or better yet, you give data constraints to


225
00:21:33,520 --> 00:21:39,000
your database, and it uses consistency when it knows that you need it in order


226
00:21:39,000 --> 00:21:43,720
to satisfy your constraints. You can look up confluence invariance if you want to


227
00:21:43,720 --> 00:21:47,520
see a lot of literature on how SQL databases can figure out when they do


228
00:21:47,520 --> 00:21:54,560
and don't need consistency. And let's build databases together where we have


229
00:21:54,560 --> 00:21:59,360
strong consistency and we have pretty good availability when we don't need


230
00:21:59,360 --> 00:22:03,040
that strong consistency by thinking about consistency as a thing that


231
00:22:03,040 --> 00:22:06,760
happens after some amount of time, rather than a thing that happens by choosing


232
00:22:06,760 --> 00:22:10,480
the right database. Thank you.


233
00:22:10,480 --> 00:22:19,760
So you touched on it at the end there about not wanting your flight to get


234
00:22:19,760 --> 00:22:26,960
unbooked on you. Yes. But in general, what do we got to do to, like, on the user


235
00:22:26,960 --> 00:22:31,680
facing metaphors side, like, we get, we know what it's like to have our pull


236
00:22:31,680 --> 00:22:37,000
requests not merged yet, right? But like, do applications need to bring those


237
00:22:37,000 --> 00:22:41,240
metaphors to users or they just need to be, have good judgment about when to go


238
00:22:41,240 --> 00:22:47,320
strongly consistent? So it's some of each. There are some times where you know you


239
00:22:47,320 --> 00:22:55,240
need strong answers and sometimes you don't. If I am revoking a key, I'm pretty


240
00:22:55,240 --> 00:23:01,400
sure that that user wants a real strong consistency event for, I made it so that


241
00:23:01,400 --> 00:23:05,640
this person is no longer allowed to see my photos, and then I added a bunch of


242
00:23:05,640 --> 00:23:12,120
photos to this album. You really want those events in the right order. But for


243
00:23:12,120 --> 00:23:17,240
a lot of things, I think you are going to want to show that to the user. So I had a


244
00:23:17,240 --> 00:23:24,120
Palm Pilot in the 90s, and whenever I created an event, it was a white


245
00:23:24,120 --> 00:23:28,760
background with black text. And when I plugged it into my laptop and I synced


246
00:23:28,760 --> 00:23:34,360
that event, it flipped to a black background with white text. And it just


247
00:23:34,360 --> 00:23:39,720
had visual indications to me for, this is an event that lives locally on your Palm


248
00:23:39,720 --> 00:23:44,520
Pilot, or this is an event that has already been synced to the desktop and


249
00:23:44,520 --> 00:23:48,600
you know you're not going to lose and is out there. I think a calendar app could


250
00:23:48,600 --> 00:23:54,440
pretty easily be like, hey, you have a pending reservation for this room, that's


251
00:23:54,440 --> 00:23:59,720
in gray text, and at some point later, whether that's 20 seconds later because


252
00:23:59,720 --> 00:24:03,960
you're waiting for a block to become final, or whether that's hours later


253
00:24:03,960 --> 00:24:07,000
because you're disconnected from the internet and you just can't get to the


254
00:24:07,000 --> 00:24:13,960
consensus group. Those things can appear as, hey, you have a fuzzy, not final thing


255
00:24:13,960 --> 00:24:17,320
that you have done, and you can see your changes and react with it and have a


256
00:24:17,320 --> 00:24:21,880
good real-time experience, you know. I would much rather edit a Google Doc and


257
00:24:21,880 --> 00:24:26,520
have some of my text indicate to me, this is locally on your machine and not


258
00:24:26,520 --> 00:24:32,600
synced yet, than have it say, oh, you're not online, let me lock that document


259
00:24:32,600 --> 00:24:36,920
for you so you can make no changes to your slides while you're on this plane


260
00:24:36,920 --> 00:24:41,480
for six hours going to a conference. Like, no, actually, it would have been okay


261
00:24:41,480 --> 00:24:45,800
for Google Docs to let me make changes to these slides while I was on the


262
00:24:45,800 --> 00:24:51,160
airplane and sync them later, as long as it communicated to me, you've got data


263
00:24:51,160 --> 00:24:55,880
that exists only on your laptop. So I do think that you want to have these


264
00:24:55,880 --> 00:25:00,440
metaphors at least possible to expose all the way up to the user and give them


265
00:25:00,440 --> 00:25:07,320
guarantees of you have a candidate new truth versus you have a final new truth.


266
00:25:08,840 --> 00:25:13,880
And I would probably avoid going too deep. For developers, I probably want to


267
00:25:13,880 --> 00:25:18,200
give them APIs where they can say, this data is local, or this data is durable,


268
00:25:18,200 --> 00:25:22,600
I at least got it off your machine, but it hasn't been guaranteed consistent


269
00:25:22,600 --> 00:25:27,720
yet. Or this data has reached majority. As long as no node in the network lies,


270
00:25:27,720 --> 00:25:31,720
you can keep going. So that's going to enable you to get more performance if


271
00:25:31,720 --> 00:25:36,120
you just assume the other nodes in the network aren't lying, you probably want


272
00:25:36,120 --> 00:25:40,440
to do optimistic concurrency control on majority, because most things will reach


273
00:25:40,440 --> 00:25:46,280
finality eventually. But you don't want to tell the user, hey, you have this


274
00:25:46,280 --> 00:25:50,920
ticket on this airplane until you know that things have really reached finality.


275
00:25:50,920 --> 00:25:55,080
So the right API to show to a developer may have more layers here. I think the


276
00:25:55,080 --> 00:25:59,400
right API to show to a user is a very strict, this thing is final, or this


277
00:25:59,400 --> 00:26:03,880
thing is not final, and pending changes and conflicts, and we'll see what


278
00:26:03,880 --> 00:26:04,440
happens.


279
00:26:06,600 --> 00:26:13,560
So using Ethereum or Bitcoin or, you know, an equivalent like system, those


280
00:26:13,560 --> 00:26:18,840
providing probabilistic finality, what happens in those environments where


281
00:26:18,840 --> 00:26:24,200
like, there's a reorg on the chain or blocks become orphaned that you've then


282
00:26:24,200 --> 00:26:28,840
anchored your time commits to and you end up in a split brain situation where


283
00:26:28,840 --> 00:26:33,240
like, some events have been anchored to like one part of the chain, and other


284
00:26:33,240 --> 00:26:36,280
events have been anchored to the other part of the chain, like eventually, you


285
00:26:36,280 --> 00:26:39,320
know, that needs to be resolved. How's that work out in the ceramic world?


286
00:26:41,000 --> 00:26:46,120
So if you had data that was anchored to blocks that eventually got unwound,


287
00:26:47,000 --> 00:26:53,640
those events didn't happen. So they are going to be in a state like this blue


288
00:26:53,640 --> 00:26:58,840
event. And it's just going to be like, okay, this thing didn't get anchored.


289
00:27:00,040 --> 00:27:05,160
If your application has application specific logic to try and detect, I have


290
00:27:05,160 --> 00:27:11,160
an unanchored event, you could try and fix that up. We need better APIs at


291
00:27:11,160 --> 00:27:16,840
exposing that and letting you fix things up right now. And then you would have


292
00:27:16,840 --> 00:27:21,160
to create some green event here and be like, okay, this is event three, I'm


293
00:27:21,160 --> 00:27:26,440
going to collect all of my events that got pruned and re merge them together.


294
00:27:26,440 --> 00:27:31,800
All right. I gotta admit, I really enjoyed watching Loki, because it's like,


295
00:27:31,800 --> 00:27:35,560
haha, you're explaining to the society the timelines diverge, and then some of


296
00:27:35,560 --> 00:27:41,240
them get pruned off, and now they're gone. And I'm glad that Marvel is making


297
00:27:41,240 --> 00:27:44,760
entire TV shows to explain consistency models better to people.


298
00:27:44,760 --> 00:27:49,720
A very open question. But if on your airplane, you would have had the time to


299
00:27:49,720 --> 00:27:51,720
add more slides, what would you have done?


300
00:27:55,720 --> 00:28:00,440
If I had more time, I would have talked more about ways in which you can get


301
00:28:00,440 --> 00:28:06,120
those mergings to be more automatic. So right now, most of the documents that


302
00:28:06,120 --> 00:28:11,240
are in Ceramic are a pile of JSON patch. And the transactions are just apply


303
00:28:11,240 --> 00:28:14,920
the patch, apply the patch, apply the patch, here's your JSON object, that's


304
00:28:14,920 --> 00:28:20,040
your current state. You might want to have much richer reactions here, where


305
00:28:20,040 --> 00:28:27,240
you have something like run this particular piece of JavaScript or other


306
00:28:27,240 --> 00:28:32,040
VM byte codes that is going to read the state and mutate things and write new


307
00:28:32,040 --> 00:28:36,200
states and do richer replicated state machines. There's a lot of things that


308
00:28:36,200 --> 00:28:42,840
can be done with CRDTs, where if I don't have this signed event 3, maybe I


309
00:28:42,840 --> 00:28:47,480
could say, oh, I'm going to look at all of the pruned realities, and if I


310
00:28:47,480 --> 00:28:51,720
have deterministic rules for merging them, maybe I can merge them on the


311
00:28:51,720 --> 00:28:56,840
client and say, yeah, I've got a bunch of signed mutations, but you went


312
00:28:56,840 --> 00:29:00,680
down one route that says I made a tweet from my phone, and you went down


313
00:29:00,680 --> 00:29:04,200
another route that says I made a tweet from my tablet, and those are all


314
00:29:04,200 --> 00:29:08,200
different tweets that happened at different times. A client should be


315
00:29:08,200 --> 00:29:14,360
able to do that merge and essentially create this green event 3, even


316
00:29:14,360 --> 00:29:22,920
though that event hasn't been made and signed by the actual owner of that


317
00:29:22,920 --> 00:29:26,920
data stream. So to put an event in a data stream, someone has to sign it,


318
00:29:26,920 --> 00:29:31,160
but you could have virtual events that are made by just the client. So


319
00:29:31,160 --> 00:29:34,200
you could sign it, but you could have virtual events that are made by just


320
00:29:34,200 --> 00:29:40,760
taking all of the tips that have been pruned and say, yeah, I'm going to


321
00:29:40,760 --> 00:29:47,720
run CRDT merge on any of the data that is capable of CRDT merge, rather


322
00:29:47,720 --> 00:29:53,480
than just the I have to get everything into one at a time right now. I


323
00:29:53,480 --> 00:29:58,040
think Ceramic is a little too aggressive of everything needs to be in


324
00:29:58,040 --> 00:30:04,920
this linearizable, ordered set of events. Everything that is in the


325
00:30:04,920 --> 00:30:08,760
candidate tip is basically just a candidate tip. There is no sense of


326
00:30:08,760 --> 00:30:12,120
I'm going to pull information in from candidate tips that never actually


327
00:30:12,120 --> 00:30:15,480
got merged into the chain in order to build a richer state.


328
00:30:15,480 --> 00:30:19,080
There will be a difference between committing the entire state of the


329
00:30:19,080 --> 00:30:24,680
object, which is what you're doing now, or then describe an action to


330
00:30:24,680 --> 00:30:29,560
change the object and then commit that. Most of the ceramic streams that


331
00:30:29,560 --> 00:30:34,840
exist right now, the thing that is in each of the events is essentially a


332
00:30:34,840 --> 00:30:39,880
transaction. It says run this piece of JSON patch on the current state, and


333
00:30:39,880 --> 00:30:42,920
it will turn whatever JSON you had as the previous state into the JSON


334
00:30:42,920 --> 00:30:47,160
that's the new state. So the thing that is being stored is the diffs, and


335
00:30:47,160 --> 00:30:51,480
you just replay all the diffs to get the current state of the object. You


336
00:30:51,480 --> 00:30:55,000
just as easily could make something that worked more like a register that


337
00:30:55,000 --> 00:31:00,120
was last writer wins. So for example, if I was making the current state of


338
00:31:00,120 --> 00:31:07,000
Wikipedia over IPFS stream, I wouldn't put all of the diffs into a giant


339
00:31:07,000 --> 00:31:12,120
JSON patch. I would just be like, my rule here is here's the CID for the


340
00:31:12,120 --> 00:31:17,880
current head of Wikipedia over IPFS, and when I had a new change to it, I


341
00:31:17,880 --> 00:31:21,480
would just create a new event that says, hey, here's the new CID, and would


342
00:31:21,480 --> 00:31:24,680
just essentially hang a Merkle tree off of each event, and then you wouldn't


343
00:31:24,680 --> 00:31:29,960
replay from the beginning. You just say, what is the latest state? So it


344
00:31:29,960 --> 00:31:34,280
makes sense to use this either in the I've got a whole pile of diffs, or in


345
00:31:34,280 --> 00:31:37,960
the I do a roll up, and every time I have a new event, my new event is


346
00:31:38,680 --> 00:31:42,680
here's the root of the new state. And with advanced data layouts, I think


347
00:31:42,680 --> 00:31:46,840
it's going to be relatively cheap to have. Just look at the front thing


348
00:31:46,840 --> 00:31:51,160
that leads to some Merkle tree that's got potentially an entire file system.


349
00:31:51,960 --> 00:31:55,320
I kind of want to make a stream type that's web-native file system, so that


350
00:31:55,320 --> 00:31:59,560
it's really easy to have the mutations be happening as a ceramic stream, and


351
00:31:59,560 --> 00:32:03,080
have all the data be in web-native file system, and get all of that fusion


352
00:32:03,080 --> 00:32:03,480
goodness.


353
00:32:03,480 --> 00:32:10,360
Yeah, that example of making a web-native file system stream is totally


354
00:32:10,360 --> 00:32:13,320
one of those things where we can take advantage of the fact that we all know


355
00:32:13,320 --> 00:32:18,040
how to parse each other's Merkle trees, and do useful stuff to remake these


356
00:32:18,040 --> 00:32:18,920
technologies later.


357
00:32:20,760 --> 00:32:25,640
My question, oh, and I guess maybe a comment on the consistency model there.


358
00:32:25,640 --> 00:32:32,120
The one Fireproof does is every update is like, let's say you update document


359
00:32:32,120 --> 00:32:37,080
XYZ, and somebody updates document ABC. Concurrently, you're going to end up


360
00:32:37,080 --> 00:32:44,600
with two clock routes, two clock heads, and the read on a clock described by


361
00:32:44,600 --> 00:32:49,000
two clock heads is just a totally normal thing to do. So it reads from that


362
00:32:49,000 --> 00:32:52,920
state, it merges dynamically, and the next update will put those clock heads


363
00:32:52,920 --> 00:32:54,120
together into one.


364
00:32:54,120 --> 00:32:55,240
And what do you do if they conflict?


365
00:32:55,960 --> 00:32:57,320
If they're both on the same key?


366
00:32:57,320 --> 00:32:57,560
Yeah.


367
00:32:58,280 --> 00:33:01,640
At this point, I hang on to it and let you deal with it later.


368
00:33:01,640 --> 00:33:05,400
Okay, so it's not last writer wins. It's, ah, you have a conflict, do


369
00:33:05,400 --> 00:33:05,720
something.


370
00:33:05,720 --> 00:33:07,080
Yeah, so it's the CouchDB model.


371
00:33:08,120 --> 00:33:12,840
I mean, Word for CouchDB works for Git. Most developers that I know are much


372
00:33:12,840 --> 00:33:17,400
happier that Git says, hey, two developers changed this file, how would


373
00:33:17,400 --> 00:33:22,840
you like to merge it? Rather than just being like, hmm, developer B's changes


374
00:33:22,840 --> 00:33:26,520
will stay, developer A's changes are gone. Bye. Consistent now.


375
00:33:28,360 --> 00:33:34,680
Okay, so then the question I had is, let's say we, well, okay, you talked


376
00:33:34,680 --> 00:33:38,360
about the consistency monitoring thing, like, like, if you're an app, you want


377
00:33:38,360 --> 00:33:45,720
to tell the user that it's finalized or whatever, how, how do we build that


378
00:33:45,720 --> 00:33:51,880
sort of monitoring, like, in the IDFS immutable distributed way? Or is that


379
00:33:51,880 --> 00:33:53,720
become like more of a service specific thing?


380
00:33:55,160 --> 00:34:00,680
So I think that is always going to be fairly tied in with who is signing the


381
00:34:00,680 --> 00:34:07,640
time events. So if I have something that is, if I can boil my consensus group


382
00:34:07,640 --> 00:34:13,080
down to a signature, so let's say I had the dumbest possible time server, a


383
00:34:13,080 --> 00:34:17,240
single server that you give events to, and it just signs the event with its


384
00:34:17,240 --> 00:34:21,400
key. And I'm trusting that it's not signing conflicting things, but it's


385
00:34:21,400 --> 00:34:27,400
actually going exactly in order. Now I could throw that entire pile of objects


386
00:34:27,400 --> 00:34:33,800
into IPFS and simply by checking the signatures that are on the timestamp


387
00:34:33,800 --> 00:34:37,800
events, I can say, yes, I've got all these things and I've got them all in


388
00:34:37,800 --> 00:34:42,120
order and I can figure out what that dependency graph is. So I don't


389
00:34:42,120 --> 00:34:45,800
actually need to talk to the time server to do that. It's a little bit more


390
00:34:45,800 --> 00:34:49,400
complicated if you're doing something like Ethereum, because the question


391
00:34:49,400 --> 00:34:55,000
that you're always asking is, is this Ethereum block in the thing that is


392
00:34:55,000 --> 00:34:59,720
the current Ethereum head? And because Ethereum could always reorg on you, you


393
00:34:59,720 --> 00:35:05,800
have to go to the Ethereum network and be like, hey, Ethereum network, what is


394
00:35:05,800 --> 00:35:11,240
the current head of the network? And more specifically, is this particular


395
00:35:11,240 --> 00:35:16,120
transaction that appears here, because I've got some timestamp event that


396
00:35:16,120 --> 00:35:22,200
references an Ethereum transaction, that that transaction is in fact in the


397
00:35:22,200 --> 00:35:26,920
chain. And at some point you're going to have to go check that, because that is


398
00:35:26,920 --> 00:35:30,920
the source of truth. Or you can decide that you're more trusting than that and


399
00:35:30,920 --> 00:35:36,680
hit the anchor service and just say, hey, anchor service, did this particular


400
00:35:36,680 --> 00:35:41,080
event get anchored? And it can tell you yes or no. So it's a question of how


401
00:35:41,080 --> 00:35:49,480
much trust do you want to give it? Maybe the right answer is send 9,999 out of


402
00:35:49,480 --> 00:35:54,120
10,000 requests to the anchor service, and then 1 in 10,000 requests actually


403
00:35:54,120 --> 00:35:59,000
go to the chain and check it. Because over time, if it's lying to you,


404
00:35:59,000 --> 00:36:03,000
eventually you're going to catch it and be like, haha, you're a lying CAS


405
00:36:03,000 --> 00:36:07,240
service. You said this was anchored, but it wasn't. I'm telling all my friends


406
00:36:07,240 --> 00:36:09,480
not to use your CAS service anymore.


407
00:36:12,440 --> 00:36:15,880
So that kind of gets into what my last question was going to be, which is like


408
00:36:15,880 --> 00:36:20,760
at the most consistent, they turn the knob up all the way. I guess what kind


409
00:36:20,760 --> 00:36:28,520
of anomalies would Jepson find? And it sounds like maybe what we'd hit is we're


410
00:36:28,520 --> 00:36:34,040
only as good as the write-ahead log or as the Ethereum chain would choose to


411
00:36:34,040 --> 00:36:34,540
use.


412
00:36:35,320 --> 00:36:44,040
Yeah, so Jepson is trying to test, am I actually doing the strict


413
00:36:44,040 --> 00:36:49,960
serializability thing? And if I'm doing something less than that, the guarantee


414
00:36:49,960 --> 00:36:54,120
we're giving here is sort of common prefix. So to be strict about it, you say,


415
00:36:54,120 --> 00:36:57,240
okay, anything that's not final, we're going to say. That doesn't count.


416
00:36:57,240 --> 00:37:01,720
Jepson, don't look at those. That's sketchy land. We don't want that.


417
00:37:04,680 --> 00:37:10,120
If you counted stuff that is in this blue space as this is a failed


418
00:37:10,120 --> 00:37:17,320
transaction, then you would potentially do very well with that. Now, despite the


419
00:37:17,320 --> 00:37:21,080
fact that the model says you could totally get the guarantee from Jepson


420
00:37:21,080 --> 00:37:27,320
that this was strongly consistent for anything that has been named final, I am


421
00:37:27,320 --> 00:37:32,120
confident that our current implementation would not get the gold star from


422
00:37:32,120 --> 00:37:37,720
Jepson. They would be like, oh, you lost some particular write that you thought


423
00:37:37,720 --> 00:37:43,880
you had, particularly because we are a distributed network. So if I have an


424
00:37:43,880 --> 00:37:49,400
event and I anchor that event and that event exists on my server and no one


425
00:37:49,400 --> 00:37:54,040
else in the ceramic network decides to mirror it, and then I go out of business


426
00:37:54,040 --> 00:37:59,640
and my events all vanished because nobody else cared and nobody else


427
00:37:59,640 --> 00:38:04,920
mirrored it. All of that stuff just did the semi-mutable thing and went from


428
00:38:04,920 --> 00:38:10,920
this state to I don't know. There was some data here. Here's what the hash of


429
00:38:10,920 --> 00:38:16,280
the data was. If you had it, I could tell you you were right. Good luck.


430
00:38:18,760 --> 00:38:21,560
Clearly, Jepson would say this was a violation because I had a bunch of


431
00:38:21,560 --> 00:38:27,640
events that were final but then got mutated in the semi-mutable way from


432
00:38:27,640 --> 00:38:30,200
here's the final state of that data to I don't know.


433
00:38:30,200 --> 00:38:36,280
Right. I mean, that's also a little bit like Jepson saying you did a bad job because you turned off your S3 bucket or something.


434
00:38:36,280 --> 00:38:44,120
Right. But I think we are a little more subject to that than most databases


435
00:38:44,120 --> 00:38:48,680
because while the general idea behind Ceramic is all of the information you


436
00:38:48,680 --> 00:38:52,360
care about you can bring to your server and you can cache and you can provide


437
00:38:52,360 --> 00:38:57,000
availability for the stuff that you care about being available. If you don't do


438
00:38:57,000 --> 00:39:01,640
that I can totally imagine people being like, hey I want to read weather data


439
00:39:01,640 --> 00:39:06,040
from someone's app who's got 10 years worth of weather data for the whole


440
00:39:06,040 --> 00:39:10,920
world. You're only going to sync in the data points that you read once. You're


441
00:39:10,920 --> 00:39:17,400
not going to read the entire history of weather for the world. So you might find


442
00:39:17,400 --> 00:39:22,760
that you have systems that are surprised when all of the sudden your data source


443
00:39:22,760 --> 00:39:25,800
went away because you are relying on somebody's server that you didn't


444
00:39:25,800 --> 00:39:32,520
realize existed. So we have maybe like five more minutes left. If you were an


445
00:39:32,520 --> 00:39:38,520
airline building a booking system using this, if they came to you and


446
00:39:38,520 --> 00:39:42,360
they wanted to start building that today, would you suggest that they run their


447
00:39:42,360 --> 00:39:48,120
own timestamp server because then that puts them on par with


448
00:39:48,120 --> 00:39:50,760
the guarantees they're used to or would you suggest that they go in the Wild


449
00:39:50,760 --> 00:39:59,800
West and use a public blockchain? I think for an airline server they would want


450
00:39:59,800 --> 00:40:28,520
things to be a little bit better installed.


451
00:40:00,000 --> 00:40:07,560
guarantee of running their own timestamp server because they really don't want


452
00:40:07,560 --> 00:40:11,560
þings to revert on them. If there's going to be a reorg they want it to be their


453
00:40:11,560 --> 00:40:19,440
reorg? Yeah, basically, but the airline is in some sense a less


454
00:40:19,440 --> 00:40:26,560
interesting case because it's not as contested of a resource. If you had


455
00:40:26,560 --> 00:40:31,800
several different organizations that were sharing something, so let's say you


456
00:40:31,800 --> 00:40:36,000
had a bunch of airlines that were trying to coordinate on who gets to use which


457
00:40:36,000 --> 00:40:40,160
gate at which time or who gets to use the runway at which time, we don't want


458
00:40:40,160 --> 00:40:44,960
there to be any lines on the runway. We want the planes to not pull out until


459
00:40:44,960 --> 00:40:50,400
they know that they've got a spot on the runway. You could coordinate the scarce


460
00:40:50,400 --> 00:40:55,800
resource of the runway using a system like this and then you're probably going


461
00:40:55,800 --> 00:40:59,520
to want to have a timestamp that is a consensus group between either a bunch


462
00:40:59,520 --> 00:41:05,240
of the airlines or some public source of time. Okay, so in that case you might run


463
00:41:05,240 --> 00:41:09,480
a private blockchain because then when it reorgs at least it's your reorg? Right.


464
00:41:09,480 --> 00:41:16,720
And again, you can get real finality if you have a closed set. If you


465
00:41:16,720 --> 00:41:22,640
know there are exactly 27 airlines that fly out of this airport and we're using


466
00:41:22,640 --> 00:41:28,480
2F plus, we're using 2 thirds plus one of them to consider something final,


467
00:41:28,480 --> 00:41:33,600
you're not going to get reorgs. It's only in a permissionless blockchain where


468
00:41:33,600 --> 00:41:38,720
anyone can join that you can't ever get real finality because what does it mean


469
00:41:38,720 --> 00:41:45,000
to have two-thirds plus one of everyone who's a Bitcoin miner right now? Do you


470
00:41:45,000 --> 00:41:49,240
know that there's not some secret giant mining pool that none of us have ever


471
00:41:49,240 --> 00:41:54,200
heard of that has just been created because, surprise, we want one day to


472
00:41:54,200 --> 00:41:57,600
cause a bunch of reorgs and they've been mining blocks on their own chain for a


473
00:41:57,600 --> 00:42:03,520
year and are gonna undo the entire laxity plan? We can't know that that


474
00:42:03,520 --> 00:42:08,280
scenario doesn't exist in a public blockchain. Whereas in a permissioned


475
00:42:08,280 --> 00:42:13,200
blockchain where you say, look, these are the nodes, we have two-thirds plus one of


476
00:42:13,200 --> 00:42:18,480
them, this is final, you could have real finality. And anchoring obviously gives


477
00:42:18,480 --> 00:42:24,240
you no better finality than the finality of the thing you anchor to, you know? If


478
00:42:24,240 --> 00:42:34,440
you really strongly securely anchored yourself to a flimsy door, like, okay, no


479
00:42:34,440 --> 00:42:37,680
matter how well you anchor to something you're not gonna get more temporal


480
00:42:37,680 --> 00:42:40,400
guarantee than the thing you've anchored yourself to.


481
00:42:40,400 --> 00:42:48,720
Alright, thanks Harry.
