1
00:00:00,000 --> 00:00:09,840
Hey everyone, so by that point you already heard a lot probably about double hashing.


2
00:00:09,840 --> 00:00:17,120
So what I want to tell you about is how we added double hashing to IPNI.


3
00:00:17,120 --> 00:00:24,400
So also I'm a software engineer in Bedrock team in the IPNI part of it and we not just


4
00:00:24,400 --> 00:00:28,440
like maintain a specification, we're also running a service that Masi told about which


5
00:00:28,440 --> 00:00:34,480
is called SID.contact which has, I don't know, like trillion CIDs indexed by that point,


6
00:00:34,480 --> 00:00:40,160
a bit more, and we needed to also migrate this whole dataset to double hashing at scale


7
00:00:40,160 --> 00:00:44,920
without causing any service disruption to the users.


8
00:00:44,920 --> 00:00:49,440
So I will touch a bit on the specification, how it's compatible with the DHT and then


9
00:00:49,440 --> 00:00:55,040
talk a bit about how we dealt with it in SID.contact.


10
00:00:55,040 --> 00:01:01,600
So just to recap how data gets into IPNI, so that happens in four simple steps.


11
00:01:01,600 --> 00:01:07,680
So step number one is storage providers when they have some new data, they announce it


12
00:01:07,680 --> 00:01:11,320
onto libp2p pubsub topic.


13
00:01:11,320 --> 00:01:19,640
So IPNI is listening to this libp2p pubsub and when it sees a new advertisement of content


14
00:01:19,640 --> 00:01:26,000
being available, it would reach out to that storage provider directly and fetch the new


15
00:01:26,000 --> 00:01:27,640
advertisements.


16
00:01:27,640 --> 00:01:29,680
So that's essentially the right path.


17
00:01:29,680 --> 00:01:35,840
Then on the lookup path, when a user wants to find something, they would send a request


18
00:01:35,840 --> 00:01:38,240
to IPNI with a seed or multi-hash.


19
00:01:38,240 --> 00:01:43,800
IPNI would return a bunch of providers where the data can be fetched from and then the


20
00:01:43,800 --> 00:01:48,160
user would go and fetch the cut picture from the provider of their choice.


21
00:01:48,160 --> 00:01:52,760
So that's essentially the flow.


22
00:01:52,760 --> 00:01:58,560
So IPNI is used across both IPFS and Filecoin networks.


23
00:01:58,560 --> 00:02:09,440
It has been default content router alongside DHT since Coupa version 18, as far as I remember.


24
00:02:09,440 --> 00:02:15,320
So also about the privacy, so we kind of separate two main topics here.


25
00:02:15,320 --> 00:02:18,880
So there is a reader privacy and writer privacy.


26
00:02:18,880 --> 00:02:22,080
So this specific presentation is focused on the reader privacy.


27
00:02:22,080 --> 00:02:28,560
So reader privacy, this means that as a user, I don't want neither IPNI like other content


28
00:02:28,560 --> 00:02:33,400
router or some man in the middle who's observing the traffic to be able to figure out what


29
00:02:33,400 --> 00:02:36,120
data I'm looking for or where this...


30
00:02:36,120 --> 00:02:37,600
Just continue.


31
00:02:37,600 --> 00:02:41,960
Yeah, and where the data can be fetched from.


32
00:02:41,960 --> 00:02:47,120
So on the other hand, the writer privacy, this means as a content publisher, I don't


33
00:02:47,120 --> 00:02:50,760
want the content router system to be able to spy on me.


34
00:02:50,760 --> 00:02:55,760
So both of the topics are equally important and we will address both of them eventually.


35
00:02:55,760 --> 00:03:01,720
But right now, our primary focus is the reader privacy and that's what this presentation


36
00:03:01,720 --> 00:03:04,120
is focused on.


37
00:03:04,120 --> 00:03:08,480
So let's see what the IPNI results look like.


38
00:03:08,480 --> 00:03:13,920
So you can just check it in your browser if you want, but I can go through it here.


39
00:03:13,920 --> 00:03:16,120
So at the top, you've got a multi-hash.


40
00:03:16,120 --> 00:03:19,920
So multi-hash is an inner component of the CID.


41
00:03:19,920 --> 00:03:21,840
So that's the digest of the data.


42
00:03:21,840 --> 00:03:25,120
So here it's base 58 formaters.


43
00:03:25,120 --> 00:03:31,200
So and then we send a lookup and the IPNI returns you zero or many provider results.


44
00:03:31,200 --> 00:03:34,180
So and provider results has a few fields.


45
00:03:34,180 --> 00:03:36,760
So let's start from the bottom.


46
00:03:36,760 --> 00:03:39,880
So at the bottom, there is a provider info.


47
00:03:39,880 --> 00:03:44,800
So it consists of the libptp identity of the provider and the addresses where the data


48
00:03:44,800 --> 00:03:46,280
can be fetched from.


49
00:03:46,280 --> 00:03:49,000
So the next one is the metadata.


50
00:03:49,000 --> 00:03:54,800
So metadata defines protocols that the data can be fetched over.


51
00:03:54,800 --> 00:03:58,720
It can be like bit swap, file coin, graph sync, HTTP, and et cetera.


52
00:03:58,720 --> 00:04:01,800
In the future, there is no special formats into it.


53
00:04:01,800 --> 00:04:03,760
It's just a binary field.


54
00:04:03,760 --> 00:04:09,200
And the last one is the context ID. context ID is the internal provider specific field.


55
00:04:09,200 --> 00:04:12,240
So it's not very interesting in the context of that presentation, but that's something


56
00:04:12,240 --> 00:04:19,280
that providers can use as an identify internally.


57
00:04:19,280 --> 00:04:27,560
So if to simplify IPNI, basically, we can imagine that this is just like two endpoints.


58
00:04:27,560 --> 00:04:35,240
So one endpoint is get and get you send multi-hash and results your list of peer IDs and put


59
00:04:35,240 --> 00:04:45,280
you put multi-hash and with a list of peer IDs assigned to it, like a big patch map.


60
00:04:45,280 --> 00:04:50,880
So with the breeder privacy implemented, we don't want neither IPNI, no passive observer,


61
00:04:50,880 --> 00:04:55,320
like spying over the traffic to be able to understand what we are looking for.


62
00:04:55,320 --> 00:05:01,240
So before going forward, I just wanted to explain a bit of the notation that is going


63
00:05:01,240 --> 00:05:02,720
to be used going forward.


64
00:05:02,720 --> 00:05:07,920
So hash over data means obviously hash over data.


65
00:05:07,920 --> 00:05:11,760
Anc means encryption, that the data is encrypted with a key.


66
00:05:11,760 --> 00:05:16,680
Derived key means the key, cryptographic key derived from the data.


67
00:05:16,680 --> 00:05:18,960
Nonce is a cryptographic nonce.


68
00:05:18,960 --> 00:05:21,920
MH stands for multi-hash.


69
00:05:21,920 --> 00:05:25,480
Double vertical line stands for double pipe says for concatenation.


70
00:05:25,480 --> 00:05:28,600
So cat concatenated dog equals cat dog.


71
00:05:28,600 --> 00:05:31,880
And peer ID is the identity of the provider.


72
00:05:31,880 --> 00:05:34,760
Multi-hash is twice.


73
00:05:34,760 --> 00:05:35,760
Okay.


74
00:05:35,760 --> 00:05:42,720
So in context, this provider specific identify that it's not really interesting in the context


75
00:05:42,720 --> 00:05:43,720
of that presentation.


76
00:05:43,720 --> 00:05:46,680
Metadata is IPNI metadata.


77
00:05:46,680 --> 00:05:50,640
So let's see like for our cat picture, what we can do.


78
00:05:50,640 --> 00:05:57,080
So first of all, like as a step one, so you kind of in order for data to be discoverable,


79
00:05:57,080 --> 00:06:02,840
you need to put it on ping on some IPFS node, as well as you need to make the routing system


80
00:06:02,840 --> 00:06:06,480
aware that that content is located at that node.


81
00:06:06,480 --> 00:06:12,040
So you would get a CAD of that content first, get a multi-hash of it.


82
00:06:12,040 --> 00:06:13,680
So then you would do two things.


83
00:06:13,680 --> 00:06:17,160
You would calculate a hash over multi-hash.


84
00:06:17,160 --> 00:06:22,040
And that's where the name double hashing comes from, but as was explained before, so because


85
00:06:22,040 --> 00:06:24,800
multi-hash is already a hash over the content.


86
00:06:24,800 --> 00:06:28,400
And at the same time, you would derive key from that multi-hash cryptographic key.


87
00:06:28,400 --> 00:06:33,560
And then you would take the identity of the provider that has that content and you would


88
00:06:33,560 --> 00:06:37,400
encrypt that identity with the key derived from the original multi-hash.


89
00:06:37,400 --> 00:06:43,880
So then you would take the pair of the hashed multi-hash and the encrypted payload and put


90
00:06:43,880 --> 00:06:49,200
it into IPNI and you would put the CAD picture onto IPFS nodes.


91
00:06:49,200 --> 00:06:52,960
So this is pretty simple.


92
00:06:52,960 --> 00:06:58,840
So also IPNI has some extra data associated with the provider record, not just identity


93
00:06:58,840 --> 00:06:59,840
of the provider.


94
00:06:59,840 --> 00:07:05,640
So let's see how the full flow works on the IPNI side.


95
00:07:05,640 --> 00:07:08,040
There are a few steps there.


96
00:07:08,040 --> 00:07:13,840
So essentially the main punchline is that we want IPNI to be really fast.


97
00:07:13,840 --> 00:07:16,440
It to be like just a single lookup request.


98
00:07:16,440 --> 00:07:20,120
And after that lookup request, you can get the results and start reaching out to the


99
00:07:20,120 --> 00:07:21,120
providers.


100
00:07:21,120 --> 00:07:27,880
So, and even with a writer privacy, reader privacy implemented, we want that to be continued


101
00:07:27,880 --> 00:07:29,500
to be true.


102
00:07:29,500 --> 00:07:34,080
So basically when a user wants to look up something, they would calculate a hash, they


103
00:07:34,080 --> 00:07:35,880
would send the hash to IPNI.


104
00:07:35,880 --> 00:07:40,740
So IPNI would do like a quick index lookup and find the disencrypted records that have


105
00:07:40,740 --> 00:07:42,600
been put into it.


106
00:07:42,600 --> 00:07:44,840
Hash them, send them back to user.


107
00:07:44,840 --> 00:07:49,400
So at that point, user can decrypt them using the original multi-hash value and can start


108
00:07:49,400 --> 00:07:51,760
reaching out to the providers.


109
00:07:51,760 --> 00:08:00,240
So they can use, for example, lippity-p, what's the stream, multi-stream select, and basically


110
00:08:00,240 --> 00:08:03,800
reach out and negotiate protocol.


111
00:08:03,800 --> 00:08:09,280
So additionally, so as we mentioned, one of the previous slides, IPNI has some metadata.


112
00:08:09,280 --> 00:08:13,000
And metadata, but we also don't store it in open in IPNI.


113
00:08:13,000 --> 00:08:19,920
We also encrypt them, encrypt it in exactly the same way as we do it with the provider


114
00:08:19,920 --> 00:08:21,120
identities.


115
00:08:21,120 --> 00:08:25,760
So now instead of deriving key from multi-hash, we derive key from the provider identity,


116
00:08:25,760 --> 00:08:29,440
can get in it with a context ID and encrypt metadata with it.


117
00:08:29,440 --> 00:08:37,000
So if user wants to basically fetch metadata, they can do that and they would do like another


118
00:08:37,000 --> 00:08:40,640
round trip to IPNI and would fetch that metadata.


119
00:08:40,640 --> 00:08:47,480
And by doing that, like a few times, they would assemble the full resulting IPNI payload.


120
00:08:47,480 --> 00:08:52,240
So having said all of that, so the whole, we can see what essentially are the indexes


121
00:08:52,240 --> 00:08:53,560
that are stored in IPNI.


122
00:08:53,560 --> 00:08:55,720
There's just like two indexes.


123
00:08:55,720 --> 00:09:01,320
One is like hash of the multi-hash map to nonce, concatenated with the encrypted peer


124
00:09:01,320 --> 00:09:02,520
IDs.


125
00:09:02,520 --> 00:09:10,360
And second one is hash over the peer ID concatenated with context ID, mapped to nonce with, and


126
00:09:10,360 --> 00:09:13,000
concatenated with encrypted metadata.


127
00:09:13,000 --> 00:09:16,880
So this essentially the two indexes that we store.


128
00:09:16,880 --> 00:09:21,920
We use exactly the same functions across both DHT and IPNI, the same for encryption, same


129
00:09:21,920 --> 00:09:25,240
magic values, same hashing.


130
00:09:25,240 --> 00:09:30,880
So these are the functions that we use like shard 256 for hashing, ASGCM with 12 byte


131
00:09:30,880 --> 00:09:33,240
nodes, nonce for encryption.


132
00:09:33,240 --> 00:09:38,800
So with the multi-hashes, when we double hash them, we always make sure that it has the


133
00:09:38,800 --> 00:09:39,880
correct context.


134
00:09:39,880 --> 00:09:46,440
That's what tells the indexer that this is a double hash and double hash can be treated


135
00:09:46,440 --> 00:09:47,560
differently.


136
00:09:47,560 --> 00:09:51,000
And as I mentioned, DHT and IPNI have like fully compatible formats.


137
00:09:51,000 --> 00:09:57,240
So in fact, if you look into DHT and IPNI record, so DHT has some more data in it.


138
00:09:57,240 --> 00:10:04,920
So I want to go like through what the fields mean, but basically the IPNI, the IPNI record


139
00:10:04,920 --> 00:10:07,160
is subset of the DHT.


140
00:10:07,160 --> 00:10:12,040
So basically we can take the records, for example, from DHT, put them into IPNI, and


141
00:10:12,040 --> 00:10:15,520
that's what would just walk out of the box.


142
00:10:15,520 --> 00:10:18,280
So this is it from the specification point of view.


143
00:10:18,280 --> 00:10:22,680
Are there any questions at this point?


144
00:10:22,680 --> 00:10:26,080
Okay, cool.


145
00:10:26,080 --> 00:10:30,000
So now I want to talk about the seed.contact.


146
00:10:30,000 --> 00:10:35,800
So seed.contact, I hope the data about how much stuff we ingest is accurate.


147
00:10:35,800 --> 00:10:41,760
So basically we ingest about like 5 billion multi-hashes per day, about 2,500 requests


148
00:10:41,760 --> 00:10:44,640
per second, 100% uptime.


149
00:10:44,640 --> 00:10:47,680
We use it for both IPFS and Filecoin.


150
00:10:47,680 --> 00:10:55,160
So we do have some IPFS nodes advertising into IPNI and basically Kuber uses seed.contact


151
00:10:55,160 --> 00:11:02,680
for lookups and it's default contact router since Kuber version 18.


152
00:11:02,680 --> 00:11:09,120
So again, our goal when introducing the IPNI was to, when introducing double hashing, was


153
00:11:09,120 --> 00:11:15,840
to not to cause any service disruption and continue serving users for both double hashed


154
00:11:15,840 --> 00:11:19,240
and not double hashed paths.


155
00:11:19,240 --> 00:11:26,280
So this is how simplified version of seed.contact looks under the hood.


156
00:11:26,280 --> 00:11:31,760
So if we start from the right, these are the storage providers.


157
00:11:31,760 --> 00:11:37,320
Then in the middle, basically seed.contact is based on the IPNI implementation called


158
00:11:37,320 --> 00:11:38,800
store the index.


159
00:11:38,800 --> 00:11:44,520
And we have like a few instances running, so we call them with Viking names.


160
00:11:44,520 --> 00:11:46,680
So there are three of them at the minute in production.


161
00:11:46,680 --> 00:11:48,500
So Kep, Hadido, and Odin.


162
00:11:48,500 --> 00:11:53,760
So each of these instances is backed by the key value data store.


163
00:11:53,760 --> 00:11:54,760
We use Pebble.


164
00:11:54,760 --> 00:11:59,560
So Pebble is the inner component of the commercial product called CockroachDB.


165
00:11:59,560 --> 00:12:03,440
So it's basically the key value store that's used there under the hood.


166
00:12:03,440 --> 00:12:09,760
So we have like running multiple instances of indexer for two purposes.


167
00:12:09,760 --> 00:12:12,060
It's basically, one is experimentation.


168
00:12:12,060 --> 00:12:16,160
So for example, if you want to introduce a new data store, we would set up a new instance


169
00:12:16,160 --> 00:12:21,240
and see how this data store performs or like tune parameters and et cetera.


170
00:12:21,240 --> 00:12:22,800
And we also do sharding.


171
00:12:22,800 --> 00:12:30,920
So we shard by providers so we can tell which indexer would be processing which provider.


172
00:12:30,920 --> 00:12:33,440
So this is done by the assigner service.


173
00:12:33,440 --> 00:12:39,000
So basically, assigner service, when it sees a new announcement on libpwpubsub topic, it


174
00:12:39,000 --> 00:12:40,000
would pick it up.


175
00:12:40,000 --> 00:12:42,320
It then would check, okay, is this a new provider?


176
00:12:42,320 --> 00:12:43,940
I already know about it.


177
00:12:43,940 --> 00:12:45,940
If I already know about it, that's fine.


178
00:12:45,940 --> 00:12:50,400
If it's new, it would just explicitly assign this provider to one of the indexes.


179
00:12:50,400 --> 00:12:53,320
So this is essentially the write path.


180
00:12:53,320 --> 00:12:57,720
On the read path, when the request comes in, it hits our proxy server, which is called


181
00:12:57,720 --> 00:13:02,920
indexer, and the sole responsibility of it to do a scatter gather request across all


182
00:13:02,920 --> 00:13:04,520
the instances.


183
00:13:04,520 --> 00:13:09,680
So indexer would just scatter the request, gather the results, compose them over and


184
00:13:09,680 --> 00:13:10,800
send it back.


185
00:13:10,800 --> 00:13:15,300
So all read requests are going through the indexer.


186
00:13:15,300 --> 00:13:20,840
So basically, when moving to double hashing, we want to take that opportunity to re-architect


187
00:13:20,840 --> 00:13:27,800
stuff a bit with scalability in mind, because we see increased amount of the traffic coming


188
00:13:27,800 --> 00:13:28,800
to std.contact.


189
00:13:28,800 --> 00:13:33,060
We expect it to be even more requests, more writes.


190
00:13:33,060 --> 00:13:37,320
So we want to make sure that we are scalable.


191
00:13:37,320 --> 00:13:38,920
So we've done a few changes.


192
00:13:38,920 --> 00:13:41,080
So they're highlighted with the red color.


193
00:13:41,080 --> 00:13:44,080
So let me start with the DHStore.


194
00:13:44,080 --> 00:13:46,560
So we introduced the service called DHStore.


195
00:13:46,560 --> 00:13:49,920
So essentially what it means, DHStore stands for double hash store.


196
00:13:49,920 --> 00:13:54,640
So essentially what now store the index is not backed by a local database, but instead


197
00:13:54,640 --> 00:13:57,520
it writes into the remote service.


198
00:13:57,520 --> 00:14:06,000
And DHStore is so simple, it just stores binary keys mapped to binary values without knowing


199
00:14:06,000 --> 00:14:08,760
any context about it.


200
00:14:08,760 --> 00:14:17,960
So store the index now becomes a service that traverses the chains and writes them into


201
00:14:17,960 --> 00:14:18,960
the remote store.


202
00:14:18,960 --> 00:14:22,760
But also unhooked store the index from the lookup path.


203
00:14:22,760 --> 00:14:28,800
So now the request from the index star, the read requests won't be routed into store the


204
00:14:28,800 --> 00:14:29,800
index.


205
00:14:29,800 --> 00:14:32,600
So it's not on the read path anymore.


206
00:14:32,600 --> 00:14:38,320
So also we introduced, we hooked up DHStore directly to index star.


207
00:14:38,320 --> 00:14:43,440
So as I mentioned before, so when we double hash the multi hash, we add the special codex


208
00:14:43,440 --> 00:14:44,440
to it.


209
00:14:44,440 --> 00:14:49,040
So when a double hash request hits index star, we know that it's been double hashed and then


210
00:14:49,040 --> 00:14:55,280
we can route it straight to the DHStore and a result straight from it, which is going


211
00:14:55,280 --> 00:14:57,920
to be like instant.


212
00:14:57,920 --> 00:15:03,840
And we also want to support regular queries and we introduced a new service DHFind.


213
00:15:03,840 --> 00:15:07,200
So DHFind stands for obviously double hash find.


214
00:15:07,200 --> 00:15:13,320
And when a regular request comes in, DHFind would take it, would implement the whole like


215
00:15:13,320 --> 00:15:14,320
reader privacy workflow.


216
00:15:14,320 --> 00:15:21,360
It would go to DHStore, it would fetch all the data, decrypt it, assemble the payload


217
00:15:21,360 --> 00:15:22,360
and return to the user.


218
00:15:22,360 --> 00:15:27,800
And in fact, if you hit DHFind, so basically the result you see is exactly similar to what


219
00:15:27,800 --> 00:15:35,580
you would have seen if you hit the not double hashed indexer.


220
00:15:35,580 --> 00:15:46,560
So if you get results from DHStore directly, so now instead of like seeing open results,


221
00:15:46,560 --> 00:15:49,720
like playing payload, you would see something like that.


222
00:15:49,720 --> 00:15:53,960
So basically you would see a multi hash and then you would see a number of binary blobs


223
00:15:53,960 --> 00:16:01,840
and the re-specification, how you decrypt them and convert them into provider identities.


224
00:16:01,840 --> 00:16:04,000
What eventually we want to do?


225
00:16:04,000 --> 00:16:09,960
We want to get rid of the not double hash path completely.


226
00:16:09,960 --> 00:16:17,200
So we want all the requests to be privacy preserving, but that would take some time.


227
00:16:17,200 --> 00:16:21,400
That's why at the minute we are going to be supporting this service DHFind.


228
00:16:21,400 --> 00:16:22,400
It's super simple.


229
00:16:22,400 --> 00:16:29,200
It's essentially just a wrapper around a client and it scales really well.


230
00:16:29,200 --> 00:16:32,700
So if we see more demand for regular queries, we would scale it up.


231
00:16:32,700 --> 00:16:36,000
If not, scale it down until we retire it completely.


232
00:16:36,000 --> 00:16:41,800
We also, with the writer privacy upgrade, we want to remove the right now store the


233
00:16:41,800 --> 00:16:45,920
index it does encryption on behalf of the client because advertisement chains, they


234
00:16:45,920 --> 00:16:52,040
still contain the raw data, but we don't want to be the case like in the future.


235
00:16:52,040 --> 00:16:57,560
Basically once we introduce the protocol upgrade for the right, we would also remove the encryption


236
00:16:57,560 --> 00:17:01,400
on behalf of the client.


237
00:17:01,400 --> 00:17:03,200
And that's basically it.


238
00:17:03,200 --> 00:17:10,240
If we look into the future, this is how simplified architecture could look like.


239
00:17:10,240 --> 00:17:15,440
That store the index, traverses chains of advertisements that are encrypted, it writes


240
00:17:15,440 --> 00:17:16,440
them into DHStore.


241
00:17:16,440 --> 00:17:23,520
DHStore is super simple and it can be sharded very well, so we can scale it up to the demand.


242
00:17:23,520 --> 00:17:30,440
And then indexed arguments would scatter across multiple DHStore instances.


243
00:17:30,440 --> 00:17:38,120
So like spoiler alert, so we have not migrated this in production yet.


244
00:17:38,120 --> 00:17:40,360
Our indexes are still catching up.


245
00:17:40,360 --> 00:17:45,680
So in the migration strategy, basically we've chosen to re-ingest everything from a scratch.


246
00:17:45,680 --> 00:17:51,480
So basically instead of migrating the existing data, we just redone it from scratch.


247
00:17:51,480 --> 00:17:58,200
So indexes are currently running, double-hatched indexes are currently running in production.


248
00:17:58,200 --> 00:18:02,720
However the results from it are not hooked up to the output of IndexStar, but instead


249
00:18:02,720 --> 00:18:06,520
we use it to collect some metrics about double-hatched coverage.


250
00:18:06,520 --> 00:18:11,640
So that would tell us when actually we are ready to move to production.


251
00:18:11,640 --> 00:18:19,920
And we collect a few metrics, so we have huge dashboards, and this is some of the double-hatching


252
00:18:19,920 --> 00:18:22,340
specific metrics.


253
00:18:22,340 --> 00:18:31,320
So our criteria is to be able to provide similar coverage, similar or better coverage, and


254
00:18:31,320 --> 00:18:37,320
latencies that we are currently getting from the unhatched seed.contact.


255
00:18:37,320 --> 00:18:38,520
So we measure a few things.


256
00:18:38,520 --> 00:18:40,120
So the first thing is provider coverage.


257
00:18:40,120 --> 00:18:46,240
So provider coverage tells how many providers that are known by our double-hatched indexer


258
00:18:46,240 --> 00:18:51,600
versus how many providers are known by seed.contact.


259
00:18:51,600 --> 00:18:56,120
Like run up a seed.contact knows about 600-ish active providers, I think.


260
00:18:56,120 --> 00:19:00,760
So basically we are almost double-hatched indexer knows like 99% of them.


261
00:19:00,760 --> 00:19:04,280
So the second thing is called distance.


262
00:19:04,280 --> 00:19:11,620
So distance tells how many advertisements need to be ingested by double-hatched indexer


263
00:19:11,620 --> 00:19:14,880
in order to catch up with the regular indexer.


264
00:19:14,880 --> 00:19:20,400
So at the moment this kind of number fluctuates, and at the moment it's like a million, but


265
00:19:20,400 --> 00:19:24,480
that's kind of fluctuates, and it should be like lower than a million, maybe like a few


266
00:19:24,480 --> 00:19:27,880
hundred thousand ideally.


267
00:19:27,880 --> 00:19:29,920
Then the read coverage.


268
00:19:29,920 --> 00:19:34,380
So this one is the most, I think the most important one.


269
00:19:34,380 --> 00:19:42,440
So what we do, so we have this service called dhfind running, and it's hooked up to production


270
00:19:42,440 --> 00:19:46,000
traffic, but it doesn't return any results.


271
00:19:46,000 --> 00:19:50,280
Instead of returning like real payloads, it immediately returns 404.


272
00:19:50,280 --> 00:19:54,280
But under the hood in the background, it still performs the lookup and reports metric about


273
00:19:54,280 --> 00:19:55,280
it.


274
00:19:55,280 --> 00:19:58,960
So it doesn't affect any kind of like latency, it doesn't affect the results returned to


275
00:19:58,960 --> 00:20:22,600
the user, but we can...


276
00:20:00,000 --> 00:20:04,740
can collect data about how many find requests can be satisfied by hasht�


277
00:20:04,740 --> 00:20:09,300
encrypted data. So and obviously that needs to be like as close to 100% as


278
00:20:09,300 --> 00:20:14,860
possible. So as indexes still catching up, so this number also fluctuates a bit,


279
00:20:15,020 --> 00:20:22,020
but I captured that screenshots today and it was like 32% of the lookup requests


280
00:20:22,020 --> 00:20:28,540
can be satisfied by double hasht� indexes. We kind of expect it to catch up. I


281
00:20:28,540 --> 00:20:33,620
don't want to be overly optimistic, but like very, very soon, within like a few


282
00:20:33,620 --> 00:20:40,620
weeks maybe. And the final one is obviously not found requests. This is the


283
00:20:40,620 --> 00:20:47,460
amount of requests that we cannot find in the double hasht� index. We've had


284
00:20:47,460 --> 00:20:53,500
quite a few bumps on the road while introducing that, so double hashting to


285
00:20:53,500 --> 00:20:58,580
seed.contact. Some of them were a bit embarrassing, but still, so we've done


286
00:20:58,580 --> 00:21:04,820
like a few improvements. So first of all, we use Pebble a lot and Pebble is


287
00:21:04,820 --> 00:21:08,900
underlying key-value storage for CockroachDB. So CockroachDB is a


288
00:21:08,900 --> 00:21:12,260
commercial product. It comes with a bunch of stuff available out of the box, a


289
00:21:12,260 --> 00:21:16,740
bunch of metrics, guidances, and etc. But for Pebble it's not the case. So we had


290
00:21:16,740 --> 00:21:22,820
to instrument it and when we ingest the data, we kind of, we sometimes see like


291
00:21:22,820 --> 00:21:26,900
horrible latencies. Some writes could take, I don't know, randomly like tens of


292
00:21:26,900 --> 00:21:30,980
seconds to complete. We are not sure what's going on, so we instrumented a


293
00:21:30,980 --> 00:21:35,980
database, started collecting a bunch of database-specific metrics such as like


294
00:21:35,980 --> 00:21:42,300
read amplification and a bunch of others. And we tuned it up and now it forms


295
00:21:42,300 --> 00:21:47,380
really well for our workloads. So this is one of the improvements we've done. So we


296
00:21:47,380 --> 00:21:51,060
introduced the S3 mirroring. This is something my colleague Andrew is going


297
00:21:51,060 --> 00:21:56,140
to be talking about today. So the punchline is one of the embarrassing


298
00:21:56,140 --> 00:21:59,940
bugs that we've hit. Instead of calculating two hashes, we've been


299
00:21:59,940 --> 00:22:05,140
calculating three hashes and over the multi-hash. And when we discovered that,


300
00:22:05,140 --> 00:22:08,540
obviously there is no way back. There is no way to unhash the stuff that we


301
00:22:08,540 --> 00:22:14,100
already done. And the most kind of like, the thing that takes the most time is to


302
00:22:14,100 --> 00:22:18,140
actually fetch the advertisement chains from the providers. So we introduced like


303
00:22:18,140 --> 00:22:22,940
S3 mirroring. This means that advertisements now can be stored on S3


304
00:22:22,940 --> 00:22:27,380
and instead of like reaching out to remote storage provider, you would just


305
00:22:27,380 --> 00:22:32,140
go and fetch it from the S3 like storage bucket. So that's significantly


306
00:22:32,140 --> 00:22:38,140
improved the speed that we can like bootstrap a new indexer. The thing that


307
00:22:38,140 --> 00:22:44,380
Masi talked about is a very important update, the nd.json. So with double


308
00:22:44,380 --> 00:22:49,580
hashing in APNI, you might require to do like a multiple round trips in order to


309
00:22:49,580 --> 00:22:54,820
assemble the full payload. And for example, some multi-hashes which are


310
00:22:54,820 --> 00:22:58,700
specifically hot, they can have like hundreds of records associated with


311
00:22:58,700 --> 00:23:03,220
them. And like decrypting each record and then doing a round trip fetching


312
00:23:03,220 --> 00:23:07,100
metadata for each record might take like a long time. That's why like nd.json


313
00:23:07,100 --> 00:23:11,700
specifically is a must-have for reader privacy. So now instead of like


314
00:23:11,700 --> 00:23:15,940
waiting for all for decrypting all the records, we can just decrypt them as they


315
00:23:15,940 --> 00:23:21,460
come in. And basically that significantly reduces time to first byte for the


316
00:23:21,460 --> 00:23:28,180
users. And yeah, so we've done like a number of other optimizations to like


317
00:23:28,180 --> 00:23:32,380
the whole store the index implementation. Like we've, I don't know, download speed


318
00:23:32,380 --> 00:23:38,420
improved like from 2 to 20x. Writer output includes improved around like


319
00:23:38,420 --> 00:23:43,980
5x. And yeah, we've done like a lot of things to the latest versions of store


320
00:23:43,980 --> 00:23:50,860
the index. Also like I want to like quickly mention what kind of like


321
00:23:50,860 --> 00:23:57,940
configuration we're running on. So these two services that we introduced, we like


322
00:23:57,940 --> 00:24:01,420
DHStore, we're running at the minute just a single instance of it. So it's a


323
00:24:01,420 --> 00:24:08,220
memory optimized instance in AWS, 60 gigs RAM, 6 vCPUs. We like at the minute


324
00:24:08,220 --> 00:24:13,500
10 terabytes GP3 volume is enough for us. And we found a sweet spot with


325
00:24:13,500 --> 00:24:18,300
GP3 volume configuration. So that allows us to like ingest the amounts of data


326
00:24:18,300 --> 00:24:22,620
that needs to be ingested. So in for DHFinds it's like super, it's super


327
00:24:22,620 --> 00:24:26,100
lightweight. At the moment we're running five of them, which seems to be enough.


328
00:24:26,100 --> 00:24:30,080
But kind of we can scale it up or down. So and they're super tiny, just one


329
00:24:30,080 --> 00:24:37,820
gigabyte from and one and a half vCPUs. So but again we can scale them up and


330
00:24:37,820 --> 00:24:42,700
scale them down. So and obviously but just want to reiterate we are not in the


331
00:24:42,700 --> 00:24:47,020
end of our journey yet. So the current digestion speeds looks promising but we


332
00:24:47,020 --> 00:24:52,860
still have not not still fully connected it to production. But we kind of like


333
00:24:52,860 --> 00:25:01,420
roughly there not to be super optimistic. And yeah basically all services, all


334
00:25:01,420 --> 00:25:06,260
not just store the index, but the whole thing is open source and we also open


335
00:25:06,260 --> 00:25:10,340
source not just the services themselves, but also all the Terraform


336
00:25:10,340 --> 00:25:16,140
configurations, all the all customization and etc that we used to bootstrap


337
00:25:16,140 --> 00:25:21,300
seed.contact. So if someone can just bootstrap like mirror from it by just


338
00:25:21,300 --> 00:25:25,860
running our Terraform and applying the customization and just yeah it's it's


339
00:25:25,940 --> 00:25:33,060
very doable. So if you want to get in touch it's IP and I Slack channel and


340
00:25:33,060 --> 00:25:40,100
go up IP and I GitHub. Yeah I think that's it so any any questions?


341
00:25:51,700 --> 00:25:57,940
Yeah so why are you using Pebble instead of a real database like I know like


342
00:25:57,940 --> 00:26:04,660
something like FoundationDB or you know whatever is like a big big data store?


343
00:26:04,660 --> 00:26:08,900
So it's a great question. It's not our first kind of iteration it's like I


344
00:26:08,900 --> 00:26:14,300
think it's a third or fourth database that we tried with and Pebble is like


345
00:26:14,300 --> 00:26:21,460
super simple and also it offers it has a merge function basically it doesn't


346
00:26:21,460 --> 00:26:26,300
flush to the disk for every write as far as I remember. You can specify the kind


347
00:26:26,300 --> 00:26:31,100
of like a merge technique so when you perform so the it has very very good


348
00:26:31,100 --> 00:26:36,780
write throughput. So we've done some testing against other data stores that


349
00:26:36,780 --> 00:26:41,900
we use and Pebble performed like on magnitude better than than others and


350
00:26:41,900 --> 00:26:47,180
yeah I think it was Masi who discovered it first and specifically because it's


351
00:26:47,180 --> 00:26:51,380
used as a key value search for CockroachDB and we've done some testing


352
00:26:51,380 --> 00:27:03,460
it's the results were really good. Just a quick thing to add the storage in IP and


353
00:27:03,460 --> 00:27:09,980
I just uses vanilla key value store. We don't need any SQL engine nothing like


354
00:27:09,980 --> 00:27:17,020
that right. No matter how fast a real database implementation is the overhead


355
00:27:17,020 --> 00:27:26,060
of a SQL engine is non-zero. Yeah so that is why you mentioned FoundationDB


356
00:27:26,060 --> 00:27:32,260
specifically because it's a key value store just distributed so it's also very


357
00:27:32,260 --> 00:27:39,420
fast. There's like others like TKV so it's just things to try I guess.


358
00:27:39,820 --> 00:27:44,380
Cool thank you great question so yeah so that's the one we tried and we're


359
00:27:44,380 --> 00:27:49,340
really happy with performance maybe we can find a better one in the future.


360
00:27:56,500 --> 00:28:02,700
This is a more forward-looking question but how do we think about like


361
00:28:02,700 --> 00:28:08,460
debug ability in the future so like we might write bugs how do we know if we've


362
00:28:08,460 --> 00:28:13,940
made a mistake if everything's encrypted and we can't see the providers or the


363
00:28:13,940 --> 00:28:19,140
or know what SIDs are being looked up if we get bug reports. That's a great


364
00:28:19,140 --> 00:28:23,500
question and so depends where the bug is so the worst ones are in the bugs


365
00:28:23,500 --> 00:28:27,260
encrypted code. If you get the cryptography wrong or messed up the


366
00:28:27,260 --> 00:28:30,940
record format somewhere I guess you would have to just rebuild the index


367
00:28:30,940 --> 00:28:34,260
because it's you cannot decrypt stuff because encrypted and you don't have the


368
00:28:34,260 --> 00:28:38,380
original key so that's the worst kind of surface where you can could find a bug.


369
00:28:38,380 --> 00:28:44,660
If there is a bug in a service logic somewhere then it's we can just


370
00:28:44,660 --> 00:28:49,500
can handle it as a normal bug so the main point is to not to have the


371
00:28:49,500 --> 00:28:52,660
bugs in the crypto codes like we had like for example three hashes instead of


372
00:28:52,660 --> 00:28:58,540
two and as long as there are no bugs there then it should be like similar to


373
00:28:58,540 --> 00:29:05,220
the current debugging experience. Just relating to David's question would it


374
00:29:05,220 --> 00:29:14,940
make sense to have endpoints or services in IPNI or in general even across the


375
00:29:14,940 --> 00:29:22,900
DHT that allow a user to explicitly submit the original CID or you know the


376
00:29:22,900 --> 00:29:27,300
key that we can then decrypt information with so that we can do debugging.


377
00:29:27,300 --> 00:29:33,260
Because when we have an encrypted world we have basically two sides of


378
00:29:33,260 --> 00:29:39,740
the system one is just implementation bugs for that you know yep it's software


379
00:29:39,740 --> 00:29:46,220
industry you write tests and whatever the other category of bugs are the


380
00:29:46,220 --> 00:29:50,020
interaction bugs that we won't have any visibility into because the information


381
00:29:50,020 --> 00:29:55,780
is encrypted so I wonder if it would make sense to have a sort of endpoint a


382
00:29:55,780 --> 00:30:00,660
sort of well-known protocol by which you can go and submit a CID which then


383
00:30:00,660 --> 00:30:04,980
reverses back to the current state which is I can use the CID look up the


384
00:30:04,980 --> 00:30:09,300
information see what you are getting and then debug the system as if I am sitting


385
00:30:09,300 --> 00:30:13,460
you know I was curious what's your take on it what do you think?


386
00:30:13,460 --> 00:30:17,700
Yeah it would make total sense in fact it's like what DHFind does right so you


387
00:30:17,700 --> 00:30:23,220
kind of submit thing and open it returns you like a payload. Also we do have a


388
00:30:23,220 --> 00:30:27,700
client so if anyone ever used store the index so the index has like a client


389
00:30:27,700 --> 00:30:31,460
that they can take and then you create it you give it CID it returns you


390
00:30:31,460 --> 00:30:35,580
provider records we do have exactly the same one with the same API but they just


391
00:30:35,580 --> 00:30:40,900
does the implements a double hashing workflow so but we can use that for


392
00:30:40,900 --> 00:30:47,500
debugging too so if you give it open CID and see where things get broken if


393
00:30:47,500 --> 00:30:50,060
hopefully they don't.


394
00:30:50,060 --> 00:30:53,020
I wanted to ask real quick for other index


395
00:30:53,020 --> 00:30:58,860
providers or operators who are operating their own index instances do we have any


396
00:30:58,860 --> 00:31:04,660
thoughts on how we help them to achieve having a reader privacy on their


397
00:31:04,660 --> 00:31:05,820
instances?


398
00:31:05,820 --> 00:31:11,140
Oh there's a great question so the way we approach migration as I


399
00:31:11,140 --> 00:31:14,340
mentioned we decided to re-ingest everything from scratch there were like


400
00:31:14,340 --> 00:31:20,660
a few few reasons for that so we switched to a new GP3 volume so we


401
00:31:20,660 --> 00:31:26,820
wanted to find a sweet spot like basically what kind of how many IOPS


402
00:31:26,820 --> 00:31:32,220
how many throughput is going to be satisfactory for our workload so we


403
00:31:32,220 --> 00:31:38,620
wanted we knew that we can't implement migration code for every


404
00:31:38,620 --> 00:31:44,340
possible type of storage so and when others need to migrate to we want to


405
00:31:44,340 --> 00:31:49,760
provide like a simple path for them to do that and we also wanted to update the


406
00:31:49,760 --> 00:31:55,140
index counts so if you look into the output of CID.context it returns you


407
00:31:55,140 --> 00:32:00,300
like number of indexes for each of the providers and in order to have the


408
00:32:00,300 --> 00:32:06,080
correct values basically you need to we had to re-ingest the whole thing


409
00:32:06,080 --> 00:32:12,460
because we started counting them like halfway through the lifecycle and


410
00:32:12,460 --> 00:32:16,980
yeah so and in order to speed up the whole ingestion process now we have like


411
00:32:16,980 --> 00:32:21,900
S3 mirror that's again Andrew is going to be talking about and the punchline


412
00:32:21,900 --> 00:32:26,780
is that you can have like if you don't have to go to the remote providers then


413
00:32:26,780 --> 00:32:32,460
you can like rebuild index from S3 like much much faster so I'm not ready to say


414
00:32:32,460 --> 00:32:36,980
like how exactly how long is going to take but maybe like 5, 10 times


415
00:32:36,980 --> 00:32:41,400
faster depending where you're hosting your services so for others so


416
00:32:41,400 --> 00:32:46,660
maybe I don't know this is something we can open to others to to use as well


417
00:32:46,660 --> 00:32:50,500
but Andrew is going to be talking about that later on today. Cool awesome thank


418
00:32:50,500 --> 00:33:17,860
you very much.
