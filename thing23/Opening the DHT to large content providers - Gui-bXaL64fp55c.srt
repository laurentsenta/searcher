1
00:00:00,000 --> 00:00:13,560
Hi everyone, so as Massie said I'm going to talk about mostly the DHT and how we can open


2
00:00:13,560 --> 00:00:16,840
it to larger content providers.


3
00:00:16,840 --> 00:00:23,040
Maybe you know it's been a pain for large content providers to provide a lot of content


4
00:00:23,040 --> 00:00:30,040
to the DHT because the implementation hasn't been that efficient and one motivation on


5
00:00:30,040 --> 00:00:37,200
why we want large content providers to advertise the data on the DHT is to get them away from


6
00:00:37,200 --> 00:00:42,960
BitSwap because some of the large content providers are only providing on BitSwap and


7
00:00:42,960 --> 00:00:48,400
BitSwap is very chatty, it's a lot of spam across the network, the only way to get some


8
00:00:48,400 --> 00:00:55,920
content is to go through BitSwap and so that's to get them to the DHT so we can get all IPFS


9
00:00:55,920 --> 00:00:59,920
less chatty and more resource efficient.


10
00:00:59,920 --> 00:01:04,800
So how does the DHT provide process work?


11
00:01:04,800 --> 00:01:08,000
Because that's what we want to improve.


12
00:01:08,000 --> 00:01:16,760
So when nodes want to advertise that it has some content to the DHT, what we do is we


13
00:01:16,760 --> 00:01:24,360
need to find the 20 closest peers to this CID and then ask them to store a provider


14
00:01:24,360 --> 00:01:31,400
record which is basically a pointer mapping the CID to the peer ID of the provider such


15
00:01:31,400 --> 00:01:38,520
that anyone that is looking for this piece of content, this CID, can look up the 20 closest


16
00:01:38,520 --> 00:01:45,880
peers and at least one of them will be there online and will say okay this peer ID is providing


17
00:01:45,880 --> 00:01:47,520
this content.


18
00:01:47,520 --> 00:01:53,560
So now in terms of performance, in order to find the 20 closest peers, usually in the


19
00:01:53,560 --> 00:02:01,800
IPFS network we need around like 3 or 4 hops, the way it works is we have concurrency parameters


20
00:02:01,800 --> 00:02:08,320
so we can have at least, so we can have at most 10 in-flight messages at once which is


21
00:02:08,320 --> 00:02:17,400
I won't just ask, so when I'm looking in my routing table for the closest peer to a CID,


22
00:02:17,400 --> 00:02:23,040
I will not send just one message, I will send a message to 10 of the closest peers I know


23
00:02:23,040 --> 00:02:28,560
and get back an answer and iteratively get closer and closer.


24
00:02:28,560 --> 00:02:35,480
So it means that the overall number of messages that I send to discover the 20 closest peers


25
00:02:35,480 --> 00:02:38,800
will be around 30 at the moment.


26
00:02:38,800 --> 00:02:46,120
Note that when I say 20 it's the replication factor which sometimes is referred to as K


27
00:02:46,120 --> 00:02:53,560
but K has multiple meanings in academia so I'm going to refer to this parameter as rep


28
00:02:53,560 --> 00:02:56,440
for the rest of the presentation.


29
00:02:56,440 --> 00:03:01,960
And then once we've found these 20 closest peers we want to allocate the provider record


30
00:03:01,960 --> 00:03:06,000
to these peers so that's additional messages.


31
00:03:06,000 --> 00:03:12,040
So now looking at it in terms of number of connections to open and number of messages


32
00:03:12,040 --> 00:03:19,560
to send, we get that we need to open approximately 35 connections, so new connections when we


33
00:03:19,560 --> 00:03:26,000
allocate one provider record, I mean when we advertise one CID which means allocating


34
00:03:26,000 --> 00:03:29,400
20 provider records.


35
00:03:29,400 --> 00:03:34,760
So most likely for the first hop of the DHT resolution we will already be connected to


36
00:03:34,760 --> 00:03:38,400
the peers so that's good, we don't need to open new connections.


37
00:03:38,400 --> 00:03:44,160
And then when iteratively getting closer we need to open some new connections, so approximately


38
00:03:44,160 --> 00:03:47,840
15, so that's just back of the envelope calculation.


39
00:03:47,840 --> 00:03:54,560
And then to find the 20 closest peers we need to send 20 messages, yeah we need to open


40
00:03:54,560 --> 00:03:57,720
the connection to these 20 peers.


41
00:03:57,720 --> 00:04:06,600
And the number of messages, so it's approximately 70, so 50 for the lookup and we need to send


42
00:04:06,600 --> 00:04:11,680
the provider records, so 20 times which is 20 messages.


43
00:04:11,680 --> 00:04:14,520
Now how does it look for large content providers?


44
00:04:14,520 --> 00:04:22,780
So let's say a large content provider wants to provide 1 billion CID to the DHT.


45
00:04:22,780 --> 00:04:33,520
Now every single CID must be reprovided periodically, so after 48 hours all the provider records


46
00:04:33,520 --> 00:04:41,240
will be garbage collected, so we get rid of them and just providers need to republish.


47
00:04:41,240 --> 00:04:48,160
And the reprovide interval has been set to 22 hours based on measurements because what


48
00:04:48,160 --> 00:04:52,380
we want, the first priority is that the content is still available.


49
00:04:52,380 --> 00:04:58,360
So even though the provider record will be garbage collected after 48 hours, if all of


50
00:04:58,360 --> 00:05:03,120
the peers that are storing my provider record go away because of the churn, because you


51
00:05:03,120 --> 00:05:08,800
open your laptop, you get some content allocated, then you close the laptop, we need to make


52
00:05:08,800 --> 00:05:13,260
sure that the content is reprovided every 22 hours.


53
00:05:13,260 --> 00:05:20,040
And so it means that in terms of number of open connection, so now if we want to provide


54
00:05:20,040 --> 00:05:27,720
1 billion CID, if it's 35 connections we need to open for each CID, that's 35 billion connections


55
00:05:27,720 --> 00:05:37,600
we need to open in 22 hours, so that's like 450 connections per second, so that's a lot.


56
00:05:37,600 --> 00:05:43,240
And number of messages sent, it's approximately double this number.


57
00:05:43,240 --> 00:05:51,760
So that's why the DHT is currently very friendly for large content providers and this is a


58
00:05:51,760 --> 00:05:53,320
major problem.


59
00:05:53,320 --> 00:05:59,320
So there are ways around this, such as using the accelerated DHT client, and I'll come


60
00:05:59,320 --> 00:06:03,360
at this solution at the end of the presentation.


61
00:06:03,360 --> 00:06:10,080
So what can we do to optimize now this reprovide operation because we need to reprovide every


62
00:06:10,080 --> 00:06:12,160
22 hours.


63
00:06:12,160 --> 00:06:18,600
So now you're going to tell me, okay, we need to reprovide like 1 billion CID, but we only


64
00:06:18,600 --> 00:06:26,960
have 20,000 peers in the DHT, which means that I could just open a connection to these


65
00:06:26,960 --> 00:06:33,000
20,000 peers and send them all of the CIDs instead of looking for the same peers over


66
00:06:33,000 --> 00:06:36,760
and over again, closing the connection, opening again.


67
00:06:36,760 --> 00:06:41,960
And so that's, so we can just use the pigeonhole principle because we have much more CIDs than


68
00:06:41,960 --> 00:06:43,640
DHT servers.


69
00:06:43,640 --> 00:06:49,720
And it means that we're going to allocate multiple CIDs on the very same DHT servers.


70
00:06:49,720 --> 00:06:55,840
So what we can do is just group the CIDs we want to provide by XOR distance, which means


71
00:06:55,840 --> 00:07:03,240
that yeah, all of the CIDs that are close together will be allocated on the very same


72
00:07:03,240 --> 00:07:06,900
DHT server and just sequentially.


73
00:07:06,900 --> 00:07:13,520
So we open the connection to this DHT server and send it all of the provider records that


74
00:07:13,520 --> 00:07:17,920
we allocated them at once.


75
00:07:17,920 --> 00:07:22,820
And so in order to optimize this, we can just sweep the key space.


76
00:07:22,820 --> 00:07:27,160
So from the key 0, 0, 0, 0 to the key 1, 1, 1, 1, 1.


77
00:07:27,160 --> 00:07:33,240
And when we get to a peer, we just say, okay, here are all the CIDs I want you to advertise


78
00:07:33,240 --> 00:07:39,580
for me, and then go to the next peer and continue to just sweep this way.


79
00:07:39,580 --> 00:07:46,360
And this way we can minimize the number of connections to open and of messages to send.


80
00:07:46,360 --> 00:07:53,400
And so we should get approximately, the number of connection to open should be approximately


81
00:07:53,400 --> 00:07:59,320
the number of DHT server peers.


82
00:07:59,320 --> 00:08:04,120
So now let's get to the details of how it works.


83
00:08:04,120 --> 00:08:09,800
So I have on the right hand side, a binary trie, which is a prefix tree, which is commonly


84
00:08:09,800 --> 00:08:13,380
used to represent distance in the DHT.


85
00:08:13,380 --> 00:08:18,040
So in the rightmost column, we have the peer IDs.


86
00:08:18,040 --> 00:08:26,080
So those are the peers, the nodes, and all of the other, let's say, cells in this graph


87
00:08:26,080 --> 00:08:32,920
are just, let's say, intermediary cells to represent distance.


88
00:08:32,920 --> 00:08:37,820
So just to draw the binary trie.


89
00:08:37,820 --> 00:08:46,040
And so here in this example, we have keys or peer IDs that are represented by a bit


90
00:08:46,040 --> 00:08:53,160
string of length eight, and in Kademia we have a bit string of length 256, just because


91
00:08:53,160 --> 00:08:58,160
it wouldn't be readable to have such a deep tree.


92
00:08:58,160 --> 00:09:06,760
So yeah, so this representation is optimized when we are looking for a key.


93
00:09:06,760 --> 00:09:14,320
So in a database, when we are doing XOR distance arithmetic, this binary trie is the optimized


94
00:09:14,320 --> 00:09:18,840
data structure to look up and add and so on.


95
00:09:18,840 --> 00:09:24,640
And that's the visual representation that will help us understand what's happening.


96
00:09:24,640 --> 00:09:31,480
And so that's also what we're using to group the CIDs that are close together.


97
00:09:31,480 --> 00:09:36,320
So now we define, so in this binary trie, a key space region.


98
00:09:36,320 --> 00:09:49,280
So we define a key space region as a region where all, so let's say the intermediary nodes


99
00:09:49,280 --> 00:09:52,320
are prefixes of the keys.


100
00:09:52,320 --> 00:10:05,040
And if a prefix has at least, so the replication number children, which we'll go through an


101
00:10:05,040 --> 00:10:12,440
example then, it is considered as a region.


102
00:10:12,440 --> 00:10:17,120
And we are interested in finding the smallest number of possible regions that are fully


103
00:10:17,120 --> 00:10:19,160
covering the key space.


104
00:10:19,160 --> 00:10:25,240
So now if we go through an example, we get that the replication factor will be three


105
00:10:25,240 --> 00:10:26,880
for this example.


106
00:10:26,880 --> 00:10:38,200
And so if we take the first, so the topmost yellow rectangle, we have that in the rightmost


107
00:10:38,200 --> 00:10:41,920
column, we have four peers.


108
00:10:41,920 --> 00:10:45,320
And so they all start with the prefix 00.


109
00:10:45,320 --> 00:10:51,520
And so we can see that the prefix 00 has four children or four nodes within it.


110
00:10:51,520 --> 00:10:54,920
And so it's defined as a proper region.


111
00:10:54,920 --> 00:11:02,560
Now going to the second region, we can see that the prefix 011 has exactly three nodes


112
00:11:02,560 --> 00:11:03,560
within it.


113
00:11:03,560 --> 00:11:04,960
So it could be a region.


114
00:11:04,960 --> 00:11:11,360
However, the prefix 010, which is just above, has only two nodes in it, which is not enough


115
00:11:11,360 --> 00:11:12,600
to make a region.


116
00:11:12,600 --> 00:11:18,720
And so we need to combine them together to make sure that, so here the prefix 01 has


117
00:11:18,720 --> 00:11:22,520
more than three nodes within it.


118
00:11:22,520 --> 00:11:24,720
And so you can apply the same logic.


119
00:11:24,720 --> 00:11:32,320
So going below, so it means that the region will always approximately have the same size,


120
00:11:32,320 --> 00:11:37,000
which is going to be slightly more than the replication factor, but it's not necessarily


121
00:11:37,000 --> 00:11:39,200
prefixes with the same length.


122
00:11:39,200 --> 00:11:42,240
So we can see then there's the prefix 1000.


123
00:11:42,240 --> 00:11:51,520
So it's a prefix of length four, whereas before we had prefixes of length two.


124
00:11:51,520 --> 00:11:59,360
And so this region will be useful in order to understand where we need to store our CIDs.


125
00:11:59,360 --> 00:12:02,100
So a CID will only be stored.


126
00:12:02,100 --> 00:12:08,520
So all of the replica of the CIDs will always be stored in the very same region.


127
00:12:08,520 --> 00:12:14,640
Now how do we explore a region and make sure that it is a region?


128
00:12:14,640 --> 00:12:19,920
So what we need to do is to look up a random key within the target region.


129
00:12:19,920 --> 00:12:27,560
If some of the return peers do not match the region's prefix, it means that the region


130
00:12:27,560 --> 00:12:29,760
has been fully explored.


131
00:12:29,760 --> 00:12:37,880
And otherwise, if all of the peers are within the region, we must explore the neighboring


132
00:12:37,880 --> 00:12:43,120
region, which means that we take the prefix, flip the last bit and generate, so a new key


133
00:12:43,120 --> 00:12:48,800
there and explore the new sub-region until the region is fully explored.


134
00:12:48,800 --> 00:12:52,720
We have enough peers in this sub-region.


135
00:12:52,720 --> 00:12:59,880
And now once we have this region, what we want to do is sweep all of the region from


136
00:12:59,880 --> 00:13:04,520
the key space, let's say from left to right or top to bottom, doesn't really matter, but


137
00:13:04,520 --> 00:13:10,120
from the 000 to the 111.


138
00:13:10,120 --> 00:13:18,000
And once we are in a region, we need to re-provide all of the CIDs that are matching this region.


139
00:13:18,000 --> 00:13:24,840
And so it means that once the sweep is over, I have re-provided all of my CIDs.


140
00:13:24,840 --> 00:13:31,680
So it means that the sweep needs to take approximately re-provide interval, so 24 hours to complete


141
00:13:31,680 --> 00:13:36,720
or can take up to 24 hours to complete.


142
00:13:36,720 --> 00:13:41,880
In order to do this, we need to store the peers in a binary try so that it's easier


143
00:13:41,880 --> 00:13:48,200
to keep track of the regions and the seeds will be stored in the binary try as well in


144
00:13:48,200 --> 00:13:51,640
order to understand in which region they belong.


145
00:13:51,640 --> 00:14:02,080
And so it's also optimized for faster insert or delete of the CIDs.


146
00:14:02,080 --> 00:14:07,940
Now how does re-providing work within a region?


147
00:14:07,940 --> 00:14:15,760
So within a region, we need to define a temporary key value store that will map peer IDs to


148
00:14:15,760 --> 00:14:16,760
keys.


149
00:14:16,760 --> 00:14:23,840
So we basically want to list all of the keys that must be allocated for every peer ID in


150
00:14:23,840 --> 00:14:24,840
this region.


151
00:14:24,840 --> 00:14:31,480
So we just iterate on the seeds that belong to this region and map them to, so here, as


152
00:14:31,480 --> 00:14:36,480
we want to replicate them 20 times, we map each key to 20 peer IDs.


153
00:14:36,480 --> 00:14:41,120
And then once we have this map, we just iterate over the peer IDs of the region.


154
00:14:41,120 --> 00:14:47,200
We open a new connection and then we allocate whatever number of CIDs we have to allocate


155
00:14:47,200 --> 00:14:48,200
them.


156
00:14:48,200 --> 00:14:54,000
So it means that we open the connection once and then send multiple messages, one for each


157
00:14:54,000 --> 00:14:57,680
CID we want to re-provide.


158
00:14:57,680 --> 00:15:08,200
And so the number of workers can easily be limited in the implementation.


159
00:15:08,200 --> 00:15:10,640
Now how do we schedule this?


160
00:15:10,640 --> 00:15:17,200
So ideally we do not really want to re-provide everything at once, which is what the accelerated


161
00:15:17,200 --> 00:15:23,440
DHT client is doing, just because it's causing a rush hour and you do everything at once,


162
00:15:23,440 --> 00:15:25,680
then you can shut up your node.


163
00:15:25,680 --> 00:15:30,600
I mean, in some application this would be needed, but usually it's more resource efficient


164
00:15:30,600 --> 00:15:36,160
to spread the use of resource over time.


165
00:15:36,160 --> 00:15:41,360
So each region, so if you take the very same region with the very same prefix, should be


166
00:15:41,360 --> 00:15:46,040
republished within the re-provide interval.


167
00:15:46,040 --> 00:15:54,320
Otherwise maybe the data will provide the record, wouldn't be available anymore.


168
00:15:54,320 --> 00:16:01,760
And so as the regions may grow or shrink, the delays within the region may need to be


169
00:16:01,760 --> 00:16:03,600
adjusted.


170
00:16:03,600 --> 00:16:10,000
And we need to have a scheduler to keep track on when to re-provide each region.


171
00:16:10,000 --> 00:16:16,760
And also it's important, like one important role of the scheduler is what if, so I was


172
00:16:16,760 --> 00:16:22,720
providing in a sweep and over and over again, and then my nodes goes offline.


173
00:16:22,720 --> 00:16:29,480
And then when it goes online again, I need to catch up for the things that I didn't re-provide.


174
00:16:29,480 --> 00:16:36,400
And so I will try to re-provide as fast as possible until I get back on track.


175
00:16:36,400 --> 00:16:43,160
So now concretely, how does it work with, so yeah, let's take how it works from the


176
00:16:43,160 --> 00:16:44,160
start.


177
00:16:44,160 --> 00:16:52,120
So now I have no CIDs that are tracked and the IPFS implementation is asking me, so DHT


178
00:16:52,120 --> 00:16:58,480
implementation to provide a first CID.


179
00:16:58,480 --> 00:17:02,320
So the first provide of the CID isn't timely.


180
00:17:02,320 --> 00:17:04,360
No, no, sorry.


181
00:17:04,360 --> 00:17:11,320
It is timely because when I want to publish some file, so I advertise it on the DHT and


182
00:17:11,320 --> 00:17:15,940
then maybe I immediately want to send it to a friend and I want this friend to be able


183
00:17:15,940 --> 00:17:17,720
to get it from the DHT.


184
00:17:17,720 --> 00:17:20,840
So I need to provide the CID immediately.


185
00:17:20,840 --> 00:17:24,920
But then the re-provide isn't timely because I have 22 hours to re-provide.


186
00:17:24,920 --> 00:17:30,480
So it doesn't matter if I re-provide it after 15 hours or 20 hours or 22 hours.


187
00:17:30,480 --> 00:17:37,420
So that doesn't matter if it takes more, even if it takes seconds or minutes to re-provide.


188
00:17:37,420 --> 00:17:42,960
And so it means that what I'm going to do is when I receive a provide, I will just do


189
00:17:42,960 --> 00:17:46,760
the lookup and allocate to the 20 closest peers.


190
00:17:46,760 --> 00:17:54,920
But then I will not wait necessarily 22 hours to re-provide because the CID will belong


191
00:17:54,920 --> 00:17:58,560
to a region that will be scheduled to be re-provided at a given time.


192
00:17:58,560 --> 00:18:04,140
And so I will just put this new CID with the region and republish it at the time that was


193
00:18:04,140 --> 00:18:07,160
planned for this region.


194
00:18:07,160 --> 00:18:13,240
So now what happens when a region is shrinking, which means that if a region has, so in our


195
00:18:13,240 --> 00:18:16,380
example, we got like a replication factor of three.


196
00:18:16,380 --> 00:18:19,620
So we needed each region to have at least three peers.


197
00:18:19,620 --> 00:18:25,520
Now if peers go offline and a region only has two peers left, what happens?


198
00:18:25,520 --> 00:18:32,040
So in this case, if we have a region with less than the replication factor of peers,


199
00:18:32,040 --> 00:18:35,380
it must be merged with its neighboring region.


200
00:18:35,380 --> 00:18:40,720
So the region, so if you take the prefix identifying the region, you flip the last bit, you get


201
00:18:40,720 --> 00:18:46,840
the neighbor, and you just migrate the regions together to a larger region.


202
00:18:46,840 --> 00:18:56,240
So it means that the time the content will be re-provided will be slightly changed because


203
00:18:56,240 --> 00:19:02,280
both of the neighboring region each had their time at which the content would have been


204
00:19:02,280 --> 00:19:03,280
re-provided.


205
00:19:03,280 --> 00:19:11,280
So what you can do is just take the soonest time to re-provide all of the content.


206
00:19:11,280 --> 00:19:13,040
And same for region expansion.


207
00:19:13,040 --> 00:19:19,880
So if a region grows and at the point it can be split into two distinct regions, so two


208
00:19:19,880 --> 00:19:28,480
regions that have at least a rep peer inside, then we can just re-provide both regions for


209
00:19:28,480 --> 00:19:36,200
the first time at the time that was planned for the region to be re-provided.


210
00:19:36,200 --> 00:19:43,280
And then the scheduler will make sure to just space the re-provide time so that not everything


211
00:19:43,280 --> 00:19:47,560
happens at once.


212
00:19:47,560 --> 00:19:56,600
So now I'm just going to give an intuition of why this is correct, or the re-provide


213
00:19:56,600 --> 00:19:59,960
scheme works, is when we have...


214
00:20:00,000 --> 00:20:09,960
the first provide... we just need to find a region where this CID belongs, we add the region to the scheduler


215
00:20:09,960 --> 00:20:16,400
and it will be scheduled to be re-provided every 22 hours. So that's for the very first CID.


216
00:20:16,400 --> 00:20:23,560
And then when new CIDs are added, they are added either to a new region where there was no CID,


217
00:20:23,560 --> 00:20:28,600
so a new region is defined and added to the scheduler, or they are added to an existing region.


218
00:20:28,600 --> 00:20:37,160
And then it's just basically induction, so if the region grows, so...


219
00:20:37,160 --> 00:20:44,760
I mean, you're going to add the CIDs to the already existing region, and now the regions are going to shrink or expand


220
00:20:44,760 --> 00:20:54,560
depending on the peer turn in the network, and so that's how we define that the re-provide happens.


221
00:20:54,560 --> 00:21:03,560
So now, let's come to the performance evaluation and try to measure the performance.


222
00:21:03,560 --> 00:21:13,840
So if, again, taking a large content provider, we're providing 1 billion CIDs every 22 hours using IPFS realistic settings,


223
00:21:13,840 --> 00:21:22,760
so we measured approximately 20,000 DHT server nodes, and we have a replication factor of 20.


224
00:21:22,760 --> 00:21:33,200
So it makes us... So to know the number of regions, it's approximately the number of peers over twice the replication factor,


225
00:21:33,200 --> 00:21:43,200
because, yeah, on average, I mean, an upper bound of the region size is twice the replication factor.


226
00:21:43,200 --> 00:21:47,160
So we got approximately 500 regions.


227
00:21:47,160 --> 00:21:57,000
In order to explore a region, we need approximately 55... We need to open approximately 55 connections and to send 70 messages.


228
00:21:57,000 --> 00:22:07,360
So the number of connections to be opened in order to re-provide the 1 billion CID is expected to be around 28,000,


229
00:22:07,360 --> 00:22:13,200
whereas it was 35 billion for the Vanilla provide operation.


230
00:22:13,200 --> 00:22:18,880
So we got an improvement of the order of 1 million, which is huge in the number of connections.


231
00:22:18,880 --> 00:22:25,760
And then in the number of messages to be sent, so we'll need to send approximately...


232
00:22:25,760 --> 00:22:31,520
So 500 times 70, which is the number of regions, times the number of messages to explore a region,


233
00:22:31,520 --> 00:22:38,760
plus, as we need to provide 1 billion CIDs and each one of them to 20 peers,


234
00:22:38,760 --> 00:22:43,560
we need to send 20 million messages to provide CIDs.


235
00:22:43,560 --> 00:22:45,560
And so that's kind of the limit.


236
00:22:45,560 --> 00:22:51,760
So we can approximate the number of messages to be sent to just the number of replica provider records.


237
00:22:51,760 --> 00:23:01,320
And so when we compare it with the Vanilla provide operation, we got an improvement of 3.5x,


238
00:23:01,320 --> 00:23:09,160
which is also good, but not as impressive as the number of connections to be opened.


239
00:23:09,160 --> 00:23:19,640
So now we can compare it with the provide many from the accelerated DHT client, or full RT, which is the same thing.


240
00:23:19,640 --> 00:23:30,040
So what the accelerated DHT client does well is that it will group the CIDs by XOR distance when we're providing,


241
00:23:30,040 --> 00:23:37,320
which means that, okay, it's re-providing everything at once, which is good and bad.


242
00:23:37,320 --> 00:23:43,080
It's good because you can group the CIDs and minimize the number of connections to be open,


243
00:23:43,080 --> 00:23:53,600
but it's bad because it will cause a rush hour, so you just re-provide everything at once and generate a lot of network traffic.


244
00:23:53,600 --> 00:23:59,960
Also, another problem of the accelerated DHT client is that it may contain stale entries in the routing table,


245
00:23:59,960 --> 00:24:04,840
because it is made...


246
00:24:04,840 --> 00:24:12,240
So you do not make a lookup before allocating the CID to the 20 closest peers.


247
00:24:12,240 --> 00:24:16,840
What you do is you run a crawler, I think, every hour.


248
00:24:16,840 --> 00:24:26,240
And so the crawler is going to give you an image of the network, but maybe you're going to re-provide one, like, 15 minutes after,


249
00:24:26,240 --> 00:24:32,760
and the network has totally changed, and you don't have a clue about this, so you're going to allocate to the wrong peers,


250
00:24:32,760 --> 00:24:36,960
or peers that aren't online anymore.


251
00:24:36,960 --> 00:24:41,440
And running this crawler in order to refresh the routing table is also expensive,


252
00:24:41,440 --> 00:24:50,560
because you need to basically connect to every single peers every refresh interval, which I think is one hour.


253
00:24:50,560 --> 00:24:57,240
And also, it's not very optimized, because the CIDs are grouped...


254
00:24:57,240 --> 00:25:07,520
The CIDs group have a constant size, which means that it isn't optimized because of the try, because...


255
00:25:07,520 --> 00:25:16,400
Yeah, if you order them in a linear way and group them with a constant size,


256
00:25:16,400 --> 00:25:26,960
you will not respect the try and the prefixes of them, and so you will end up allocating them on the wrong peers,


257
00:25:26,960 --> 00:25:32,080
and opening more connections.


258
00:25:32,080 --> 00:25:43,080
So in order to get there and be able to implement this, we need to tackle a few other problems first, as always.


259
00:25:43,080 --> 00:25:50,400
So now, the way the DHT works is when you want to provide something, there is just a provide interface to the DHT,


260
00:25:50,400 --> 00:25:56,000
or to content routers in general, which I think isn't good,


261
00:25:56,000 --> 00:26:02,960
because it means that in the current situation, the IPFS implementation, so Kubo or any other implementation,


262
00:26:02,960 --> 00:26:06,640
need to handle the reprovide themselves.


263
00:26:06,640 --> 00:26:14,760
Which means that if Kubo wants to track some content, it must have the timer and have knowledge about the DHT


264
00:26:14,760 --> 00:26:20,400
or any other content routing system, such as IPNI, know which one it is using,


265
00:26:20,400 --> 00:26:23,960
and know how to reprovide using this content routing system.


266
00:26:23,960 --> 00:26:31,440
And what we should do instead is having each content routing system have its own reprovide mechanism,


267
00:26:31,440 --> 00:26:37,560
because they all work differently, IPNI is totally different than the DHT for reproviding.


268
00:26:37,560 --> 00:26:44,880
But we should just have a simple interface, such as, okay, start providing content, stop providing content,


269
00:26:44,880 --> 00:26:55,560
such that the IPFS implementation doesn't need to have any knowledge about how reproviding works


270
00:26:55,560 --> 00:27:01,040
in a specific content routing system, and can just say, okay, please start providing for me,


271
00:27:01,040 --> 00:27:04,480
and I don't need it anymore, you can stop providing it.


272
00:27:04,480 --> 00:27:16,240
And so it means that we need to change interfaces in how it works, but yeah, I think this will be done in the coming month.


273
00:27:16,240 --> 00:27:24,680
So now, so yeah, that was the explanation of how we can minimize the reprovide cost for anyone.


274
00:27:24,680 --> 00:27:27,960
I mean, it works especially good for the large content providers,


275
00:27:27,960 --> 00:27:35,680
but if you are advertising some content on your local Kugoo node, it's also an optimization for you.


276
00:27:35,680 --> 00:27:43,440
And it enables large content providers to use the DHT and slowly get away from the BitSwap broadcast, which is good.


277
00:27:43,440 --> 00:27:51,240
And we want to ship this along with the DoubleHash DHT later this year,


278
00:27:51,240 --> 00:28:05,880
so that, yeah, if you switch to the DoubleHash DHT, you have some extra benefits that you can reprovide in a much more efficient way.


279
00:28:05,880 --> 00:28:15,520
And if you're curious about it, there's the Notion page that you can check out, in which the updates will be happening.


280
00:28:15,520 --> 00:28:18,240
So yeah, that was it for me.


281
00:28:18,240 --> 00:28:23,720
Thank you.


282
00:28:23,720 --> 00:28:27,320
Yep, Thomas?


283
00:28:27,320 --> 00:28:32,080
Isn't the current solution where you have to reprovide sort of like a stopgap mechanism,


284
00:28:32,080 --> 00:28:39,800
where you make sure that you spend some cost or you have some cost with wanting to provide content to the network?


285
00:28:39,800 --> 00:28:42,000
Yeah, but this solution will not change this.


286
00:28:42,000 --> 00:28:46,160
You will still need to reprovide every 22 hours.


287
00:28:46,160 --> 00:28:51,480
It's just you reprovide in a more efficient way.


288
00:28:51,480 --> 00:28:59,440
So is the main performance bottleneck we're addressing here is the connection establishment?


289
00:28:59,440 --> 00:29:00,760
Yeah, I think so.


290
00:29:00,760 --> 00:29:05,360
I mean, I haven't tried to advertise that much content myself,


291
00:29:05,360 --> 00:29:17,920
but just looking at the back of the envelope calculation, opening 450,000 connections per second seems a lot to me.


292
00:29:17,920 --> 00:29:22,640
Yeah, I mean, you're going to have to send the messages either way at that rate, right?


293
00:29:22,640 --> 00:29:25,600
Yeah, I mean, yeah, you need to send a lot of messages anyway,


294
00:29:25,600 --> 00:29:33,840
but if you minimize the number of lookups you do, you reduce the number of messages you send by 3.5x already.


295
00:29:33,840 --> 00:29:38,800
Okay, so it's not necessarily the number of...


296
00:29:38,800 --> 00:29:43,880
It's not just the connection establishment, then, it's the numbers, the lookups that are expensive, right?


297
00:29:43,880 --> 00:29:46,920
Yeah, because...


298
00:29:46,920 --> 00:29:50,760
I'm just curious, because I know that, for example, we've been talking a lot about QUIC,


299
00:29:50,760 --> 00:29:56,960
and that actually has a zero round-trip time connection establishment.


300
00:29:56,960 --> 00:29:59,120
Yeah, so...


301
00:29:59,120 --> 00:30:03,520
But if it's actually the cost of doing the lookup, that makes more sense.


302
00:30:03,520 --> 00:30:06,240
You're going to have to be sending a lot more messages to do the lookup.


303
00:30:06,240 --> 00:30:12,720
Yeah, exactly. So if we look just in the number of messages, we still get the 3.5x improvement.


304
00:30:12,720 --> 00:30:14,240
Right, right.


305
00:30:14,240 --> 00:30:17,920
And just because you do much less lookups.


306
00:30:17,920 --> 00:30:24,160
And basically, the only messages you get to send is the provider record.


307
00:30:24,160 --> 00:30:25,720
Interesting talk. Thank you, Guy.


308
00:30:25,720 --> 00:30:32,560
So I had a question. In the last slide, you mentioned this is going to be rolled out with the double hashing.


309
00:30:32,560 --> 00:30:34,280
Why the two are coupled?


310
00:30:34,280 --> 00:30:37,680
I see three distinct things that you pointed out.


311
00:30:37,680 --> 00:30:40,320
One is the interface changes.


312
00:30:40,320 --> 00:30:45,000
The other one is the core work here, which is just making provide more efficient.


313
00:30:45,000 --> 00:30:49,320
So I'm curious why they're bundled up with the double hashing as well.


314
00:30:49,320 --> 00:30:53,400
I'd say mostly for practical reasons, because both need some refactor.


315
00:30:53,400 --> 00:30:58,680
And so, once we can refactor...


316
00:30:58,680 --> 00:31:02,840
and include both features at once, then it's just easier.


317
00:31:02,840 --> 00:31:07,080
What's next? So when can I see this in Kubo, for example?


318
00:31:07,080 --> 00:31:09,080
What's the plans for things like load testing?


319
00:31:09,080 --> 00:31:13,920
Because I'm curious to see how it actually behaves in the wild.


320
00:31:13,920 --> 00:31:18,320
So, yeah, next is the implementation,


321
00:31:18,320 --> 00:31:25,400
which will happen once the double hash implementation is done.


322
00:31:25,400 --> 00:31:32,200
And then, yeah, probably we'll be integrating to Kubo at the same time as the double hash.


323
00:31:32,200 --> 00:31:37,840
In terms of testing, this can be done on the provider side only, right?


324
00:31:37,840 --> 00:31:38,840
Yeah, it's just...


325
00:31:38,840 --> 00:31:41,600
So we can test it ourselves before the release?


326
00:31:41,600 --> 00:31:43,800
Yeah, it's just a client change.


327
00:31:43,800 --> 00:31:47,920
So it's a client optimization. We don't need to push it everywhere.


328
00:31:47,920 --> 00:31:50,880
It's just if we ship it in a new Kubo version,


329
00:31:50,880 --> 00:31:54,800
people running this Kubo version can start benefiting from it.


330
00:31:54,800 --> 00:31:57,600
Yeah, but in the testing phase,


331
00:31:57,600 --> 00:32:01,000
we can test it ourselves without having everyone to upgrade, right?


332
00:32:01,000 --> 00:32:02,000
Yeah, exactly.


333
00:32:02,000 --> 00:32:03,000
Okay, that's great.


334
00:32:03,000 --> 00:32:04,000
Thank you.


335
00:32:04,000 --> 00:32:05,000
Thank you.


336
00:32:05,000 --> 00:32:06,000
Thank you.


337
00:32:06,000 --> 00:32:07,000
Thank you.


338
00:32:07,000 --> 00:32:08,000
Thank you.


339
00:32:08,000 --> 00:32:09,000
Thank you.
