1
00:00:00,000 --> 00:00:09,880
So yeah, in this talk, we're going to talk about the foundations for open world compute,


2
00:00:09,880 --> 00:00:15,960
Homestar, which is our implementation of IPVM, as like Kubo is for IPFS.


3
00:00:15,960 --> 00:00:21,920
So we'll talk about that and what that means and what are the issues with open world compute.


4
00:00:21,920 --> 00:00:27,040
So I'm Zeeshan Lekhani, I work at Fission as a staff applied researcher and engineer.


5
00:00:27,040 --> 00:00:30,520
I also started a thing with a couple of friends of mine called Papers We Love.


6
00:00:30,520 --> 00:00:31,520
Anyone heard of that?


7
00:00:31,520 --> 00:00:32,520
Yeah?


8
00:00:32,520 --> 00:00:33,520
Oh, cool, good to hear.


9
00:00:33,520 --> 00:00:34,520
Oh, nice.


10
00:00:34,520 --> 00:00:40,560
Yeah, so we've been running conferences and trying to get that community together for


11
00:00:40,560 --> 00:00:43,200
a good amount of years now.


12
00:00:43,200 --> 00:00:48,960
And I also am attempting some sort of part-time PhD at CMU in programming languages, specifically


13
00:00:48,960 --> 00:00:51,880
a thing called polarized subtyping and functional languages.


14
00:00:51,880 --> 00:00:54,920
If you're interested in that stuff, we can talk offline.


15
00:00:54,920 --> 00:00:58,280
And that's my daughter, who is an awesome, awesome kid.


16
00:00:58,280 --> 00:01:00,640
And she's always happy.


17
00:01:00,640 --> 00:01:03,640
So that's awesome.


18
00:01:03,640 --> 00:01:08,760
So the framing on this, we'll talk about what IPVM is at a glance for people who may not


19
00:01:08,760 --> 00:01:09,760
know.


20
00:01:09,760 --> 00:01:13,680
And we'll talk about some of the early things there.


21
00:01:13,680 --> 00:01:16,960
Actually the way we wanted to do these talks was Brooke, Brooklyn, who works at Fission


22
00:01:16,960 --> 00:01:20,760
CTO, was going to give the community talk first.


23
00:01:20,760 --> 00:01:23,800
And then I was going to come in with the implementation.


24
00:01:23,800 --> 00:01:25,280
Everything did not work out this way.


25
00:01:25,280 --> 00:01:29,880
She is giving a talk later today about IPVM as a whole.


26
00:01:29,880 --> 00:01:33,160
We'll cover some similar things, and she'll also talk about more of the community standards


27
00:01:33,160 --> 00:01:34,160
and specifications.


28
00:01:34,160 --> 00:01:37,560
I'll talk a lot about workflows.


29
00:01:37,560 --> 00:01:43,960
Some of our research in building a workflow system to work with IPFS and do computes.


30
00:01:43,960 --> 00:01:44,960
So I'll talk about that.


31
00:01:44,960 --> 00:01:48,280
I don't know if people are familiar with a lot of the microservice kind of world of workflow


32
00:01:48,280 --> 00:01:50,760
compute that's been happening.


33
00:01:50,760 --> 00:01:54,960
We'll talk about WASM and WIT, which are WebAssembly interface types.


34
00:01:54,960 --> 00:01:58,720
So with all my friends, that's pretty good.


35
00:01:58,720 --> 00:02:01,800
What it means to try to do an open world compute system.


36
00:02:01,800 --> 00:02:07,240
And I think we're just starting out in the implementation of IPVM with Homestar.


37
00:02:07,240 --> 00:02:10,080
But I hope throughout the rest of this conference and the Young Conference, we'll get a really


38
00:02:10,080 --> 00:02:14,480
good sense and see how do people want to work with plugins in a system like this.


39
00:02:14,480 --> 00:02:19,480
How can we run compute anywhere in a decentralized way?


40
00:02:19,480 --> 00:02:24,880
So as mentioned, and I said Brooke is speaking later on this today, and we're also going


41
00:02:24,880 --> 00:02:27,720
to do two talks at the Young Conference.


42
00:02:27,720 --> 00:02:28,720
I forget which day.


43
00:02:28,720 --> 00:02:29,720
I think it's tomorrow.


44
00:02:29,720 --> 00:02:36,080
She'll be talking about more stuff there, and I'll do more of a code rundown about some


45
00:02:36,080 --> 00:02:41,080
Rust stuff we're doing at Fission and how that relates to Homestar, our implementation.


46
00:02:41,080 --> 00:02:46,280
But Brooke has given a lot of talks on the interplanetary ritual machine.


47
00:02:46,280 --> 00:02:49,840
There's a lot of great talks already there that really cover a lot of the ground of what


48
00:02:49,840 --> 00:02:52,000
these things are.


49
00:02:52,000 --> 00:02:57,400
Some of the quotes that come out of her talks that I think really gets at the large scale,


50
00:02:57,400 --> 00:03:03,520
the large distributed scale that IPVM is, is nothing less than connecting all of the


51
00:03:03,520 --> 00:03:05,880
world's users and services.


52
00:03:05,880 --> 00:03:06,880
Pretty easy.


53
00:03:06,880 --> 00:03:09,200
Just a little bit of engineering, I think.


54
00:03:09,200 --> 00:03:15,720
The HCP storage and compute equivalent, open, interoperable, and everywhere, must be substantially


55
00:03:15,720 --> 00:03:18,080
better than Web 2.0.


56
00:03:18,080 --> 00:03:23,040
So when I started at Fission and we started working on the implementation in late January


57
00:03:23,040 --> 00:03:29,360
for this, these quotes, particularly the first two, I was like, oh, I have a lot of work


58
00:03:29,360 --> 00:03:30,360
ahead of me.


59
00:03:30,360 --> 00:03:32,840
A lot to be done, a lot to think about.


60
00:03:32,840 --> 00:03:35,920
And of course, this is all tied into UCANS.


61
00:03:35,920 --> 00:03:40,280
I will talk a little bit about that today as part of the capabilities.


62
00:03:40,280 --> 00:03:45,480
Brooke also has a talk on UCANS today at the other track.


63
00:03:45,480 --> 00:03:49,080
I won't go into depth, but I'm guessing people are pretty familiar with UCANS.


64
00:03:49,080 --> 00:03:52,400
We can talk more about that offline as well.


65
00:03:52,400 --> 00:03:57,640
What is an IPVM or IPVM?


66
00:03:57,640 --> 00:04:00,760
This is the beginnings of the specification for it.


67
00:04:00,760 --> 00:04:05,800
Content address execution to content address data and IPFS.


68
00:04:05,800 --> 00:04:09,960
That's the small goal of the large goal of the open world.


69
00:04:09,960 --> 00:04:12,320
These high-level attributes are still in place.


70
00:04:12,320 --> 00:04:17,720
We have WASM and IPFS, even though things that we'll be running or that we want to interop


71
00:04:17,720 --> 00:04:19,960
with won't just be WASM.


72
00:04:19,960 --> 00:04:24,560
We have declarative invocation, which is actually part of the UCAN invocation spec.


73
00:04:24,560 --> 00:04:26,240
Captured results or receipts, as we'll call them.


74
00:04:26,240 --> 00:04:27,240
We'll talk about that.


75
00:04:27,240 --> 00:04:29,080
A distributed scheduler.


76
00:04:29,080 --> 00:04:34,680
Things like matchmaking, how to determine who of nodes of machines should run a certain


77
00:04:34,680 --> 00:04:35,680
kind of compute.


78
00:04:35,680 --> 00:04:37,520
We just had a talk by Bacalao.


79
00:04:37,520 --> 00:04:43,040
They have certain kinds of things they want to run with data science workloads and, as


80
00:04:43,040 --> 00:04:44,920
we saw, stable diffusion.


81
00:04:44,920 --> 00:04:46,320
Not everybody's going to be able to run that.


82
00:04:46,320 --> 00:04:51,720
We want to be able to distribute the work or the data to the work where nodes can actually


83
00:04:51,720 --> 00:04:57,880
do that, can actually enact what needs to be done within a workflow.


84
00:04:57,880 --> 00:04:59,440
Memoization and adaptive optimization.


85
00:04:59,440 --> 00:05:03,520
We'll actually demo some of the incremental nature that we get by running compute and


86
00:05:03,520 --> 00:05:04,520
IPFS.


87
00:05:04,520 --> 00:05:08,320
Manage effects, which I'll talk about very little in this talk, but kind of where we're


88
00:05:08,320 --> 00:05:09,320
really going.


89
00:05:09,320 --> 00:05:15,160
I think a big difference, as we did put a blog post out recently comparing to Bacalao


90
00:05:15,160 --> 00:05:19,640
and other groups working on IPFS and compute, and the one big difference, I think, that


91
00:05:19,640 --> 00:05:26,480
at least comes out in my mind as we're starting out on this is thinking a lot about how people


92
00:05:26,480 --> 00:05:30,720
program, how do they work with effects, where determinism, item potency, all these things


93
00:05:30,720 --> 00:05:31,720
come into play.


94
00:05:31,720 --> 00:05:33,840
And I'll talk about that.


95
00:05:33,840 --> 00:05:37,160
Also ambient computing, HTTP compute, and a lot of stretch goals.


96
00:05:37,160 --> 00:05:41,320
So IPVM is open world compute for IPFS.


97
00:05:41,320 --> 00:05:45,120
Whereas Brooke has called it the long fabled execution layer.


98
00:05:45,120 --> 00:05:50,160
And I guess when she gave the talk about this last summer, it was more of a fable, and now


99
00:05:50,160 --> 00:05:51,720
it's becoming more real.


100
00:05:51,720 --> 00:05:54,600
So that's a good sign.


101
00:05:54,600 --> 00:05:56,920
As I mentioned, Homestar is an IPVM implementation.


102
00:05:56,920 --> 00:06:01,660
It's written in Rust, and we've been working on that, and I'll link it at the end.


103
00:06:01,660 --> 00:06:06,280
But also we'll have the ability to execute WASM on the browser.


104
00:06:06,280 --> 00:06:10,720
But a lot of the tools obviously we're using is WASM, IPFS.


105
00:06:10,720 --> 00:06:16,040
The system is heavily built around WASM time and their work and the bytecode alliance,


106
00:06:16,040 --> 00:06:19,720
IPLD, and UCAN, as I mentioned.


107
00:06:19,720 --> 00:06:26,000
So when we think about compute, at least when I started working on this stuff, thinking


108
00:06:26,000 --> 00:06:35,800
about workflows, I guess in the Web 2.0 space, workflow computing has gotten very popular.


109
00:06:35,800 --> 00:06:39,080
So has anyone heard of Uber's Cadence?


110
00:06:39,080 --> 00:06:40,080
Workflow engine?


111
00:06:40,080 --> 00:06:41,080
It's a different crowd.


112
00:06:41,080 --> 00:06:45,720
It's the used AWS all services crowd.


113
00:06:45,720 --> 00:06:51,920
So a lot of the big companies, Datadog, DoorDash, AWS, has their own stuff.


114
00:06:51,920 --> 00:06:54,060
But Uber did a thing called Cadence.


115
00:06:54,060 --> 00:06:59,920
Another company called Temporal came out of Uber with another workflow engine, which Datadog


116
00:06:59,920 --> 00:07:02,560
uses Temporal today, plus a lot of other companies.


117
00:07:02,560 --> 00:07:07,120
But all of these companies, all of these companies came out of projects at Microsoft called Durable


118
00:07:07,120 --> 00:07:11,160
Functions, Durable Tasks, and Durable Entities.


119
00:07:11,160 --> 00:07:12,160
You see Temporal there.


120
00:07:12,160 --> 00:07:16,800
They had this number of how many people, how many executions they've been running for different


121
00:07:16,800 --> 00:07:17,800
jobs.


122
00:07:17,800 --> 00:07:21,720
That number kept going up, so I didn't wait anymore when I took the photo of it.


123
00:07:21,720 --> 00:07:24,960
So I don't know if they're just lying, but that was pretty good.


124
00:07:24,960 --> 00:07:29,320
And then you have Cadence, as I mentioned, fault-tolerant stateful code platforms.


125
00:07:29,320 --> 00:07:33,640
So these things are pretty common in the microservice world today.


126
00:07:33,640 --> 00:07:38,780
You give them, you write a series of jobs in YAML or use tooling, typically in clients


127
00:07:38,780 --> 00:07:43,880
like Go, because microservices in Go go hand in hand in a lot of cases.


128
00:07:43,880 --> 00:07:47,200
And then you can define patterns.


129
00:07:47,200 --> 00:07:51,560
So in Durable Functions, as it has above, you have sequential chains, you have aggregates,


130
00:07:51,560 --> 00:07:54,960
you have things that can work in parallel and then become sequential.


131
00:07:54,960 --> 00:08:00,720
So these concepts of workflow compute are pretty normal.


132
00:08:00,720 --> 00:08:05,280
And they are all the rage these days in a lot of the big scale space.


133
00:08:05,280 --> 00:08:10,520
But one of the terms that really comes out of these things and all these companies that


134
00:08:10,520 --> 00:08:15,800
got spawned from Microsoft have come to talk about is this term called virtual resiliency,


135
00:08:15,800 --> 00:08:20,440
which came out of this Ambrosia paper, also in and around related to Durable Functions.


136
00:08:20,440 --> 00:08:25,120
But virtual resiliency is analogous to what we call virtual memory.


137
00:08:25,120 --> 00:08:31,840
And the big thing, and you'll see this term a lot in the workflow documentation, is failure


138
00:08:31,840 --> 00:08:32,840
oblivious code.


139
00:08:32,840 --> 00:08:34,240
Has anyone ever heard of this concept?


140
00:08:34,240 --> 00:08:35,240
No?


141
00:08:35,240 --> 00:08:36,240
Cool.


142
00:08:36,240 --> 00:08:37,240
So new stuff.


143
00:08:37,240 --> 00:08:38,240
Great.


144
00:08:38,240 --> 00:08:42,000
So failure oblivious code is really trying to work with distributed compute at a layer


145
00:08:42,000 --> 00:08:46,160
where the user doesn't have to get all the failures and have to deal with all the mess.


146
00:08:46,160 --> 00:08:49,520
Something else is executing and understanding how things should work and deals with things


147
00:08:49,520 --> 00:08:55,960
like replayability, incremental changes.


148
00:08:55,960 --> 00:08:59,680
So I would say the Microsoft paper, and in case, a really great paper by John and Goldstein


149
00:08:59,680 --> 00:09:07,600
and co., really gets at how to deal with things like non-determinism originating from outside


150
00:09:07,600 --> 00:09:09,720
of a resilient component.


151
00:09:09,720 --> 00:09:15,280
And so that paper really captures essentially how do you deal with work?


152
00:09:15,280 --> 00:09:19,280
How do you deal with it where it doesn't have to flow all the way to the user?


153
00:09:19,280 --> 00:09:24,280
And so a big goal for us, and again, differences with some of the other groups working on IPFS


154
00:09:24,280 --> 00:09:25,480
compute, I think is this.


155
00:09:25,480 --> 00:09:31,040
We are really trying to gauge at an abstraction to deal with virtual resiliency and failure


156
00:09:31,040 --> 00:09:32,520
oblivious code.


157
00:09:32,520 --> 00:09:36,360
And Brooke will talk about that at the high level as well in her talk.


158
00:09:36,360 --> 00:09:40,860
The issue with a lot of these companies like Cadence and Temporal and people using these


159
00:09:40,860 --> 00:09:49,360
workflow systems is they will have talks and documentation, long sets of documentation


160
00:09:49,360 --> 00:09:51,960
to digest about what is idempotency.


161
00:09:51,960 --> 00:09:56,360
Can actually someone tell me what idempotency means?


162
00:09:56,360 --> 00:09:57,360
Go ahead.


163
00:09:57,360 --> 00:09:58,360
Yeah, essentially.


164
00:09:58,360 --> 00:10:02,600
It's about replayability in particular for that.


165
00:10:02,600 --> 00:10:03,600
That's really good.


166
00:10:03,600 --> 00:10:06,200
Can anyone tell me what determinism means?


167
00:10:06,200 --> 00:10:07,200
Right.


168
00:10:07,200 --> 00:10:13,640
Are there differences between idempotency and determinism?


169
00:10:13,640 --> 00:10:16,640
Yeah, what are they?


170
00:10:16,640 --> 00:10:17,640
Right.


171
00:10:17,640 --> 00:10:18,640
That's right.


172
00:10:18,640 --> 00:10:19,640
Okay, good.


173
00:10:19,640 --> 00:10:20,640
Very good.


174
00:10:20,640 --> 00:10:21,640
Very good.


175
00:10:21,640 --> 00:10:32,640
And so when you think about these things, we understand these things at a pretty good


176
00:10:32,640 --> 00:10:33,640
level.


177
00:10:33,640 --> 00:10:38,320
We're not saying, hey, all you people working on different services and microservices running


178
00:10:38,320 --> 00:10:42,840
these workflows, you should all know what idempotency means and what determinism means.


179
00:10:42,840 --> 00:10:43,840
What are the differences?


180
00:10:43,840 --> 00:10:47,800
And please, oh, please, make sure your code is deterministic so we can replay it easily.


181
00:10:47,800 --> 00:10:50,280
And if it's not, we'll try our best.


182
00:10:50,280 --> 00:10:51,960
And these things are really hard to think about.


183
00:10:51,960 --> 00:10:55,160
These are classic distributed system problems.


184
00:10:55,160 --> 00:10:59,920
And yet these workflow engines that people are using every day, the only thing they really


185
00:10:59,920 --> 00:11:04,160
kind of try to aim at the problem is documentation and some other specific things.


186
00:11:04,160 --> 00:11:09,080
So one example that was told to me is somebody who works at, I won't name the company for


187
00:11:09,080 --> 00:11:13,280
this one, I should say that, but who worked at a certain company where they were trying


188
00:11:13,280 --> 00:11:14,280
to create a workflow.


189
00:11:14,280 --> 00:11:19,240
And so a big use case for workflows is all different kind of RPC calls right across microservices.


190
00:11:19,240 --> 00:11:21,960
And so you would think in a workflow, you want to get this replayability.


191
00:11:21,960 --> 00:11:24,480
You have one step, call one RPC service.


192
00:11:24,480 --> 00:11:26,600
You have another step, call another.


193
00:11:26,600 --> 00:11:32,000
The problem you'll see is engineers who actually just copy around templates and say, well,


194
00:11:32,000 --> 00:11:36,000
I have code that has like 50 RPC executions in one step.


195
00:11:36,000 --> 00:11:39,520
Which happens, well, what happens when one of those RPC executions fail?


196
00:11:39,520 --> 00:11:44,360
Well, the whole thing will replay again, over and over and over again, because one service


197
00:11:44,360 --> 00:11:47,840
essentially killed your other 49, right?


198
00:11:47,840 --> 00:11:51,480
So getting these things right, how should people do steps in a workflow?


199
00:11:51,480 --> 00:11:52,680
How should we engage compute?


200
00:11:52,680 --> 00:11:56,640
There's a lot of like devX experience and ideas to think about that make this pretty


201
00:11:56,640 --> 00:11:58,760
hard.


202
00:11:58,760 --> 00:12:02,840
So also in Temporal, they talk about, you know, the word honest is on you, right?


203
00:12:02,840 --> 00:12:06,160
They really try to force idempotency, but they can't restrict you.


204
00:12:06,160 --> 00:12:11,160
So there's a lot of discussion there to be like, how do we enforce these kinds of things?


205
00:12:11,160 --> 00:12:14,560
And then of course, they also said application developers are responsible for developing


206
00:12:14,560 --> 00:12:15,560
worker programs.


207
00:12:15,560 --> 00:12:19,560
It is on you to do this work.


208
00:12:19,560 --> 00:12:23,040
And they also say in their docs, activity code, which is actually their kind of key


209
00:12:23,040 --> 00:12:26,480
for doing like scheduled tasks, can be non-deterministic.


210
00:12:26,480 --> 00:12:28,040
We recommend that it be idempotent.


211
00:12:28,040 --> 00:12:31,840
Again, this idea is that we can recommend all these things, but we know that programmers


212
00:12:31,840 --> 00:12:34,260
will do whatever the hell they want.


213
00:12:34,260 --> 00:12:37,040
So how do you try to enforce these things?


214
00:12:37,040 --> 00:12:39,080
You have type systems, you have all these other kinds of things.


215
00:12:39,080 --> 00:12:41,200
And it's one of the things that we've been building out IPVM.


216
00:12:41,200 --> 00:12:45,200
We're really trying to think about how can we enforce certain properties so people can


217
00:12:45,200 --> 00:12:47,400
get certain kinds of guarantees when they run compute.


218
00:12:47,400 --> 00:12:51,040
And that, to me, is the real hard problem as you start thinking about interop.


219
00:12:51,040 --> 00:12:55,560
All right, so the modes and kind of working with IPVM and what we're thinking about.


220
00:12:55,560 --> 00:12:56,560
I talk a lot about replayability.


221
00:12:56,560 --> 00:13:02,320
This is something that really does come in IPFS for free with the incremental nature


222
00:13:02,320 --> 00:13:04,880
of it with using content addressing.


223
00:13:04,880 --> 00:13:07,840
I'll show actually an example of that.


224
00:13:07,840 --> 00:13:08,840
We want to think about things.


225
00:13:08,840 --> 00:13:14,240
We want to have properties around things like updating DNS records where you have workflows


226
00:13:14,240 --> 00:13:19,720
that are essentially atomic transactions or something like software transactional memory.


227
00:13:19,720 --> 00:13:23,600
People familiar with STM at all?


228
00:13:23,600 --> 00:13:24,600
Philip wrote Haskell.


229
00:13:24,600 --> 00:13:26,440
That's too easy.


230
00:13:26,440 --> 00:13:30,880
So Haskell probably is one of the bigger languages that actually has an implementation of software


231
00:13:30,880 --> 00:13:33,200
transactional memory.


232
00:13:33,200 --> 00:13:36,960
And the idea is, again, in software thinking about transactions in this way where if you


233
00:13:36,960 --> 00:13:39,800
have to roll something back, you roll something back.


234
00:13:39,800 --> 00:13:45,280
One of the new kind of hot programming languages of today is Verse, which is coming out of


235
00:13:45,280 --> 00:13:48,840
Epic, which has the dream team of all the great programming language researchers, including


236
00:13:48,840 --> 00:13:51,200
Simon Peyton Jones working on it.


237
00:13:51,200 --> 00:13:56,080
And they're basically looking at a verse for a multiverse compute language that will be


238
00:13:56,080 --> 00:14:01,800
at its core all based on software transactional memory to do multiple, multiple concurrent


239
00:14:01,800 --> 00:14:06,720
modules running all at once and kind of building that into their core for multiverse programming.


240
00:14:06,720 --> 00:14:13,240
So when we think about large scale compute on IPFS, how do people enact these kinds of


241
00:14:13,240 --> 00:14:18,000
workflows and make sure they get the guarantees that transactions give them?


242
00:14:18,000 --> 00:14:19,000
Manage effects.


243
00:14:19,000 --> 00:14:21,160
Brooks talked a lot about this.


244
00:14:21,160 --> 00:14:23,680
This comes also a lot from the functional programming world.


245
00:14:23,680 --> 00:14:27,040
And we're still just working on what this is going to look like.


246
00:14:27,040 --> 00:14:30,920
But again, these concepts, idempotency, determinism, oh, now we have to have a side effect.


247
00:14:30,920 --> 00:14:32,760
Now we have to make a call over the network.


248
00:14:32,760 --> 00:14:34,960
How do we move that around within an execution flow?


249
00:14:34,960 --> 00:14:35,960
How do we do that?


250
00:14:35,960 --> 00:14:37,840
I'm giving you more questions than answers.


251
00:14:37,840 --> 00:14:39,800
We have ideas on how this is going to work.


252
00:14:39,800 --> 00:14:47,360
But how to get people to understand that certain things that can be deterministic, you will


253
00:14:47,360 --> 00:14:49,360
get the results that you expect.


254
00:14:49,360 --> 00:14:52,280
Versus if you add some side effects, we can try to manage them.


255
00:14:52,280 --> 00:14:54,160
But you might get an email that happens twice.


256
00:14:54,160 --> 00:14:57,360
Because we can maybe only give you at least one semantics.


257
00:14:57,360 --> 00:15:01,840
And so I don't know how programmers are putting together all these tasks and workflows together.


258
00:15:01,840 --> 00:15:07,400
How much they're really thinking about how these things interplay.


259
00:15:07,400 --> 00:15:09,800
And then, of course, capabilities are a big mode of things.


260
00:15:09,800 --> 00:15:12,200
And capabilities, we've looked at it in kind of two different circles.


261
00:15:12,200 --> 00:15:16,880
You have what we've been using for UCAN, which is around delegation and authorization, which


262
00:15:16,880 --> 00:15:19,160
is a big part of IPVM.


263
00:15:19,160 --> 00:15:23,960
But also capabilities of different kinds of nodes running different kinds of tasks.


264
00:15:23,960 --> 00:15:25,760
Do you run WASM deterministic tasks?


265
00:15:25,760 --> 00:15:30,120
Do you run WASM with WASI to do more side effecting kind of WASM code that you want


266
00:15:30,120 --> 00:15:33,920
to do using something from the host?


267
00:15:33,920 --> 00:15:35,800
Do you run Docker jobs?


268
00:15:35,800 --> 00:15:37,240
Do you run these kinds of jobs?


269
00:15:37,240 --> 00:15:41,920
How do we know who should run a certain kind of workflow based on various steps of data


270
00:15:41,920 --> 00:15:42,920
within a workflow?


271
00:15:42,920 --> 00:15:43,920
Right?


272
00:15:43,920 --> 00:15:45,160
So based on tasks.


273
00:15:45,160 --> 00:15:50,280
So now, essentially, what we're working on with IPVM are ways to bootstrap all this information.


274
00:15:50,280 --> 00:15:51,620
You come up online.


275
00:15:51,620 --> 00:15:53,760
You have your capabilities of what you run.


276
00:15:53,760 --> 00:15:54,760
What kind of nodes you are.


277
00:15:54,760 --> 00:15:55,760
What your GPU is.


278
00:15:55,760 --> 00:15:59,280
You can't just run stable diffusion if you don't have enough GPU.


279
00:15:59,280 --> 00:16:04,000
So these things are very important to determine who should run a workflow.


280
00:16:04,000 --> 00:16:05,640
And that's where we come in with affinities.


281
00:16:05,640 --> 00:16:10,240
So there are certain kinds of clusters, certain kinds of nodes that have affinities.


282
00:16:10,240 --> 00:16:12,120
And that's work that we're determining.


283
00:16:12,120 --> 00:16:16,320
How do you bootstrap this and have the ability to say, no, you should run this, or we're


284
00:16:16,320 --> 00:16:20,960
going to run and delegate parts of these things.


285
00:16:20,960 --> 00:16:23,280
So really what this is all going through, all these things.


286
00:16:23,280 --> 00:16:24,280
We talk about workflows.


287
00:16:24,280 --> 00:16:25,280
We talk about tasks.


288
00:16:25,280 --> 00:16:26,840
We're going to talk a little bit about WASM now.


289
00:16:26,840 --> 00:16:32,120
When I was writing this talk, I was listening to this Yes song a ton, which is Wondrous


290
00:16:32,120 --> 00:16:34,360
Stories, which is really, really good.


291
00:16:34,360 --> 00:16:39,320
And really what comes into play for all this is the goal, the dream that all the Java application


292
00:16:39,320 --> 00:16:42,520
programmers had back in the day, which is write once, run anywhere.


293
00:16:42,520 --> 00:16:44,480
I still think it is the dream today.


294
00:16:44,480 --> 00:16:48,160
And that is a big reason we have gone into WASM really heavily.


295
00:16:48,160 --> 00:16:52,840
And so as part of the work, and actually code that's already in place, we've been really


296
00:16:52,840 --> 00:16:54,840
focused on WASM with interface types.


297
00:16:54,840 --> 00:16:57,400
It's kind of a big community standard of the Bytecode Alliance.


298
00:16:57,400 --> 00:16:58,880
We're all in.


299
00:16:58,880 --> 00:17:03,680
This does cause some chaos with things like not following some of the bits that companies


300
00:17:03,680 --> 00:17:05,920
like Wasm or others might do.


301
00:17:05,920 --> 00:17:09,600
But if you're going to have open world computing, you need to have some standards.


302
00:17:09,600 --> 00:17:15,680
And so we're focusing really heavily on WIP-Bindgen and WASM tools and these interface types.


303
00:17:15,680 --> 00:17:17,320
And are people familiar with the component model?


304
00:17:17,320 --> 00:17:18,920
It's gone through a lot of change.


305
00:17:18,920 --> 00:17:19,920
A little bit?


306
00:17:19,920 --> 00:17:20,920
Cool.


307
00:17:20,920 --> 00:17:26,400
So basically from their words, but you want to define a portable, low to run time efficient


308
00:17:26,400 --> 00:17:35,240
binary format, cross language composition, support ritualizability, static analyzability,


309
00:17:35,240 --> 00:17:39,480
and ability to work with language and not stay with things defined by WASI.


310
00:17:39,480 --> 00:17:40,920
It's unique value proposition.


311
00:17:40,920 --> 00:17:45,960
This is essentially keeping to WASM's idea of sandboxing, isolation, and computes.


312
00:17:45,960 --> 00:17:48,400
And defining, you know, this is actually the good and bad, right?


313
00:17:48,400 --> 00:17:51,360
They've been working on this component model very incrementally.


314
00:17:51,360 --> 00:17:55,520
You know, a lot of things, as I've been working in WASM for the first time, as I've been telling


315
00:17:55,520 --> 00:18:00,040
a lot of people, it's really great, but it's still pretty far away from like everyday users


316
00:18:00,040 --> 00:18:01,600
getting used to it, right?


317
00:18:01,600 --> 00:18:05,400
And I'll show, as I get to the demo here in a minute, I'll show you like some WASM code


318
00:18:05,400 --> 00:18:07,600
and how it's not so easy to kind of work with.


319
00:18:07,600 --> 00:18:12,360
But it obviously gives us a lot of things that we want to use.


320
00:18:12,360 --> 00:18:17,400
And so a big thing that's part of HomeStar today and the implementation is we really set


321
00:18:17,400 --> 00:18:19,960
out, well, we have two kinds of things we're all in on.


322
00:18:19,960 --> 00:18:24,800
We're all in on WASM component types or interface types, and we're all in on IPLD.


323
00:18:24,800 --> 00:18:27,120
So we should have a way to go back and forth between them, right?


324
00:18:27,120 --> 00:18:31,760
So today HomeStar implements an interpreter that actually, if you think about it this


325
00:18:31,760 --> 00:18:38,400
way, what you actually really get is way more types, right, on the WIT end.


326
00:18:38,400 --> 00:18:41,120
And we can convert all these things back and forth, right?


327
00:18:41,120 --> 00:18:45,960
So actually we're getting a more interesting type system for free by having the ability


328
00:18:45,960 --> 00:18:47,480
to go in both ways.


329
00:18:47,480 --> 00:18:48,680
And that's a really cool concept.


330
00:18:48,680 --> 00:18:51,880
We'll just show in code in the conference tomorrow.


331
00:18:51,880 --> 00:18:56,120
Another big part of this, as the demo is going to come up right now, is promise pipelining.


332
00:18:56,120 --> 00:18:58,720
So this comes out of the e-writes Mark Miller work.


333
00:18:58,720 --> 00:19:03,160
This is kind of stuff that Brooks touted and touted for a long time.


334
00:19:03,160 --> 00:19:07,160
But how do we show, how do we kind of line up things in the execution graph where instructions,


335
00:19:07,160 --> 00:19:11,200
which are the actual things that run within a task, are essentially promised and we can


336
00:19:11,200 --> 00:19:14,680
actually create a DAG essentially of how we're going to run those things.


337
00:19:14,680 --> 00:19:17,720
And so this is actually a DAG that we produced today out of the code, for example.


338
00:19:17,720 --> 00:19:19,520
This is a very sequential one.


339
00:19:19,520 --> 00:19:23,080
As you can see, we have these three different batches that run.


340
00:19:23,080 --> 00:19:26,800
And each SID is referenced as a task, essentially.


341
00:19:26,800 --> 00:19:30,080
It's tied together as this one has to come after that one and after that one, right?


342
00:19:30,080 --> 00:19:33,160
There's a causality that is in place.


343
00:19:33,160 --> 00:19:37,960
But you can also have parallel ones that all may be having the same input SID from patch


344
00:19:37,960 --> 00:19:38,960
zero.


345
00:19:38,960 --> 00:19:40,480
And now those can run in parallel.


346
00:19:40,480 --> 00:19:43,800
So we have a scheduler that can actually run those things in parallel, while other ones


347
00:19:43,800 --> 00:19:47,000
have to have different dependencies.


348
00:19:47,000 --> 00:19:48,680
And then we get receipts out of this, right?


349
00:19:48,680 --> 00:19:50,680
So with receipts, we have receipts at the different levels.


350
00:19:50,680 --> 00:19:55,840
And actually, as we create the graph, we can go back from the bottom of the graph to actually


351
00:19:55,840 --> 00:19:57,960
say, well, I already have the receipt for that third one.


352
00:19:57,960 --> 00:19:59,760
I don't have to run the whole execution.


353
00:19:59,760 --> 00:20:00,760
But if I have a third one, I don't have to run the execution.


354
00:20:00,760 --> 00:20:01,760
And this is a very, very simple thing.


355
00:20:01,760 --> 00:20:02,760
And it's a very, very simple thing to show.


356
00:20:02,760 --> 00:20:03,760
But it's a very, very simple thing to show.


357
00:20:03,760 --> 00:20:04,760
And so I'm going to show you some examples of how we can show this.


358
00:20:04,760 --> 00:20:05,760
So here's a graph that I've created.


359
00:20:05,760 --> 00:20:06,760
It's a graph that I've created.


360
00:20:06,760 --> 00:20:07,760
And I've got a SID of SID.


361
00:20:07,760 --> 00:20:08,760
And I've got a SID of SID.


362
00:20:00,000 --> 00:20:02,200
If I have to keep going back through, I'll go back through.


363
00:20:02,200 --> 00:20:03,200
Okay?


364
00:20:03,200 --> 00:20:06,200
And then you see that if I have to go all the way to the top.


365
00:20:06,200 --> 00:20:08,200
And then essentially just kind of re-showing the case.


366
00:20:08,200 --> 00:20:12,200
What we do with receipts actually is we actually have a local cache using GossipSub.


367
00:20:12,200 --> 00:20:16,200
It's a peer with members in our mesh, send the data to everybody.


368
00:20:16,200 --> 00:20:20,200
And then we also put it through the P2P for other nodes that are not part of the mesh


369
00:20:20,200 --> 00:20:22,200
to have access to it later when they have to go get it.


370
00:20:22,200 --> 00:20:24,200
Obviously, they pay a higher cost for that.


371
00:20:24,200 --> 00:20:27,200
But essentially, we have this distributed cache that we can use.


372
00:20:27,200 --> 00:20:28,200
Okay.


373
00:20:28,200 --> 00:20:30,200
So not too much time left.


374
00:20:30,200 --> 00:20:35,200
Let me now break for the demo of some of this work.


375
00:20:35,200 --> 00:20:36,200
All right.


376
00:20:36,200 --> 00:20:41,200
So we have, I think, like everybody doing demos, we have cats.


377
00:20:41,200 --> 00:20:42,200
Right?


378
00:20:42,200 --> 00:20:44,200
Is that like a thing?


379
00:20:44,200 --> 00:20:48,200
So we have this app that Brian Ginsberg on our Fission team helped me do the front end for this.


380
00:20:48,200 --> 00:20:49,200
He's really great.


381
00:20:49,200 --> 00:20:50,200
Thank you, Brian.


382
00:20:50,200 --> 00:20:51,200
Okay.


383
00:20:51,200 --> 00:20:54,200
So we are going to run up a little flow here.


384
00:20:54,200 --> 00:21:00,200
So I'm going to run up a Homestar node right now.


385
00:21:00,200 --> 00:21:01,200
Okay.


386
00:21:01,200 --> 00:21:03,200
It is up and running.


387
00:21:03,200 --> 00:21:04,200
I'm going to refresh this.


388
00:21:04,200 --> 00:21:05,200
We have a WebSocket connection.


389
00:21:05,200 --> 00:21:06,200
Cool.


390
00:21:06,200 --> 00:21:10,200
And just to show you, like, these are running, you know, somewhat legit, but somewhat fake demo to workflows.


391
00:21:10,200 --> 00:21:15,200
So we have two workflows we're going to run that does some image processing here.


392
00:21:15,200 --> 00:21:16,200
We're going to do one.


393
00:21:16,200 --> 00:21:17,200
Actually, I have this.


394
00:21:17,200 --> 00:21:18,200
Let me do this.


395
00:21:18,200 --> 00:21:19,200
That doesn't work on that.


396
00:21:19,200 --> 00:21:20,200
Okay.


397
00:21:20,200 --> 00:21:23,200
So we have different we have these promises which are being awaited.


398
00:21:23,200 --> 00:21:25,200
So you see these await okays.


399
00:21:25,200 --> 00:21:27,200
And people can look at the spec more in detail here.


400
00:21:27,200 --> 00:21:34,200
But these await okays are essentially saying I'm waiting for another task to complete within this workflow before I do any work.


401
00:21:34,200 --> 00:21:35,200
And this is essentially sequential.


402
00:21:35,200 --> 00:21:40,200
And we're going to do crop, rotate, and blur on one side.


403
00:21:40,200 --> 00:21:45,200
And we'll have a second workflow that's going to reuse crop, rotate, but then do something different.


404
00:21:45,200 --> 00:21:46,200
Okay.


405
00:21:46,200 --> 00:21:48,200
It will do gray scaling.


406
00:21:48,200 --> 00:21:49,200
All right.


407
00:21:49,200 --> 00:21:50,200
Cool.


408
00:21:50,200 --> 00:21:52,200
So, hey, we're going to start running some stuff.


409
00:21:52,200 --> 00:21:56,200
This cat is going to get processed.


410
00:21:56,200 --> 00:21:59,200
And boom, we have a crop.


411
00:21:59,200 --> 00:22:01,200
We have a rotate.


412
00:22:01,200 --> 00:22:05,200
And see before the other one runs, I'm going to oh, my node crashed.


413
00:22:05,200 --> 00:22:06,200
Right?


414
00:22:06,200 --> 00:22:08,200
And I'm not going to start VS code.


415
00:22:08,200 --> 00:22:10,200
Oh, I got to blur.


416
00:22:10,200 --> 00:22:11,200
Okay.


417
00:22:11,200 --> 00:22:12,200
Let me rerun this.


418
00:22:12,200 --> 00:22:21,200
I'll do this a little faster so we can really see incremental compute at its play.


419
00:22:21,200 --> 00:22:23,200
Demo gods, right?


420
00:22:23,200 --> 00:22:25,200
Do that one more time.


421
00:22:25,200 --> 00:22:26,200
Run up.


422
00:22:26,200 --> 00:22:27,200
Okay.


423
00:22:27,200 --> 00:22:30,200
Nix, don't run Nix.


424
00:22:30,200 --> 00:22:31,200
All right.


425
00:22:31,200 --> 00:22:32,200
For James.


426
00:22:32,200 --> 00:22:33,200
All right.


427
00:22:33,200 --> 00:22:38,200
So we'll run up that one more time.


428
00:22:38,200 --> 00:22:39,200
Waiting for Rust to compile.


429
00:22:39,200 --> 00:22:40,200
There we go.


430
00:22:40,200 --> 00:22:41,200
Okay.


431
00:22:41,200 --> 00:22:44,200
So just to show it again, see if I can beat the timer on this.


432
00:22:44,200 --> 00:22:53,200
So we're doing our images, crop, rotate.


433
00:22:53,200 --> 00:22:56,200
I think I killed it in time.


434
00:22:56,200 --> 00:22:57,200
Yeah.


435
00:22:57,200 --> 00:22:58,200
Okay.


436
00:22:58,200 --> 00:22:59,200
Cool.


437
00:22:59,200 --> 00:23:01,200
So I'm going to wait just to show you what's going on here, too.


438
00:23:01,200 --> 00:23:03,200
We have these nice little icons here that we ran these two jobs.


439
00:23:03,200 --> 00:23:08,200
We're waiting for now the client to say, hey, I'm not getting a pong from you anymore.


440
00:23:08,200 --> 00:23:10,200
So we're going to fail out.


441
00:23:10,200 --> 00:23:12,200
It's doing that here shortly.


442
00:23:12,200 --> 00:23:15,200
It's going to tell me that blur is not going to work.


443
00:23:15,200 --> 00:23:16,200
There you go.


444
00:23:16,200 --> 00:23:17,200
Boom.


445
00:23:17,200 --> 00:23:18,200
I get an X, right?


446
00:23:18,200 --> 00:23:19,200
I don't know where you are right now.


447
00:23:19,200 --> 00:23:20,200
Okay.


448
00:23:20,200 --> 00:23:21,200
Let's rerun it.


449
00:23:21,200 --> 00:23:24,200
Server's back up.


450
00:23:24,200 --> 00:23:28,200
Whenever Rust tells me it is.


451
00:23:28,200 --> 00:23:29,200
There we go.


452
00:23:29,200 --> 00:23:30,200
Okay.


453
00:23:30,200 --> 00:23:31,200
Cool.


454
00:23:31,200 --> 00:23:32,200
And we're peered.


455
00:23:32,200 --> 00:23:33,200
Okay.


456
00:23:33,200 --> 00:23:34,200
Great.


457
00:23:34,200 --> 00:23:35,200
And so I'll go back here.


458
00:23:35,200 --> 00:23:36,200
So we want to say, hey, can we rerun this thing again?


459
00:23:36,200 --> 00:23:40,200
Oh, it's much faster for those.


460
00:23:40,200 --> 00:23:43,200
And then we're just going to compute blur, right?


461
00:23:43,200 --> 00:23:44,200
As we see over there.


462
00:23:44,200 --> 00:23:45,200
Okay.


463
00:23:45,200 --> 00:23:51,200
Now, workflow two already uses two of the same computations that we already have.


464
00:23:51,200 --> 00:23:52,200
Those happen really fast.


465
00:23:52,200 --> 00:23:53,200
Now we're going to grayscale.


466
00:23:53,200 --> 00:23:54,200
Boom.


467
00:23:54,200 --> 00:23:55,200
Okay.


468
00:23:55,200 --> 00:23:56,200
Demo works, right?


469
00:23:56,200 --> 00:24:06,200
So, you know, the key in this is that, like, in this code, and again, I'll go through the code in the young conference a little bit more.


470
00:24:06,200 --> 00:24:12,200
This is like legitimately running a scheduler, legitimately running things in parallel, creating a graph, creating a DAG.


471
00:24:12,200 --> 00:24:18,200
We're using, you know, the Tokyo runtime to do a lot of work here as well for doing how we schedule parallel things.


472
00:24:18,200 --> 00:24:24,200
And so, you know, maybe it's not the coolest demo in terms of just, like, what we're doing with the images.


473
00:24:24,200 --> 00:24:34,200
But really working across compute and thinking about how to really get developers to get the guarantees and decide what they really want to do with code is really where we're going with stuff, right?


474
00:24:34,200 --> 00:24:39,200
And I think, again, highlighting the differences from a lot of the other groups working in this space.


475
00:24:39,200 --> 00:24:40,200
Yeah.


476
00:24:40,200 --> 00:24:41,200
Demo.


477
00:24:41,200 --> 00:24:42,200
Good.


478
00:24:42,200 --> 00:24:43,200
Okay.


479
00:24:43,200 --> 00:24:44,200
Cool.


480
00:24:44,200 --> 00:24:45,200
All right.


481
00:24:45,200 --> 00:24:46,200
Actually, I'll go back to that for a second.


482
00:24:46,200 --> 00:25:03,200
So in the world part of this, and I think the part I really wanted to get at as we're a community here, is, you know, in the system of running steps, running compute, doing this open world, interoperable, you know, form of computing, how do we do a plug-in system that really works where you can bring in a bunch of people and say, you know, we're going to do this.


483
00:25:03,200 --> 00:25:12,200
You know, it really works where you can bring in your Docker task, and maybe you have certain, you know, we can say, like, Docker tasks are all, like, side-effecting, but they might not touch that much of the host.


484
00:25:12,200 --> 00:25:14,200
They might not touch that much of a network.


485
00:25:14,200 --> 00:25:24,200
How can you tell us, hey, I have a certain kind of compute or a binary that I run, whatever the case might be, and I want to make it somewhat restrictive, or I have certain properties that I want you to do?


486
00:25:24,200 --> 00:25:26,200
Do you annotate it?


487
00:25:26,200 --> 00:25:28,200
You know, I've gone back and forth with Brooke on these things.


488
00:25:28,200 --> 00:25:43,200
Do people come up with annotations for a task, but what happens is a big issue that the Ambrosia paper talks about, and people I know at Microsoft have talked about when they work with durable entities, is they would have an annotation system, and then the people would write the code inside of that, inside of whatever's being annotated.


489
00:25:43,200 --> 00:25:47,200
They say it's monotonic, or they say it's deterministic, but the actual code isn't.


490
00:25:47,200 --> 00:25:52,200
You know, somebody told me once, a developer was telling them, oh, I get a user record from the database.


491
00:25:52,200 --> 00:25:54,200
That's deterministic.


492
00:25:54,200 --> 00:25:55,200
What about the timestamp?


493
00:25:55,200 --> 00:25:57,200
That's not, right?


494
00:25:57,200 --> 00:26:00,200
And so, because the user record is just a view, right?


495
00:26:00,200 --> 00:26:04,200
And so how do we really come up with the right ways to handle those foundations?


496
00:26:04,200 --> 00:26:10,200
I think that's the really hard part, the real hard computer science part that really comes into play with all this stuff.


497
00:26:10,200 --> 00:26:14,200
So just to kind of wrap up, we have a lot of components.


498
00:26:14,200 --> 00:26:17,200
I put a lot of work into our Figma for doing this.


499
00:26:17,200 --> 00:26:19,200
I wanted to show it.


500
00:26:19,200 --> 00:26:20,200
And things that we're working on.


501
00:26:20,200 --> 00:26:21,200
So we're thinking about workflows.


502
00:26:21,200 --> 00:26:23,200
We're working through, like, what we're looking at the runtime.


503
00:26:23,200 --> 00:26:29,200
I've talked about bootstrapping, discovery.


504
00:26:29,200 --> 00:26:30,200
I can use the highlighter part.


505
00:26:30,200 --> 00:26:31,200
Isn't that cool?


506
00:26:31,200 --> 00:26:38,200
Discovery, scheduling, parallel versus sequential execution with promise pipelining, registration capabilities,


507
00:26:38,200 --> 00:26:41,200
all these kind of things, matchmaking affinities, receipt storage.


508
00:26:41,200 --> 00:26:47,200
And the stuff that's, like, not transparent is stuff that we're really close to already finishing today.


509
00:26:47,200 --> 00:26:49,200
And so obviously stuff that we're still working on.


510
00:26:49,200 --> 00:26:51,200
I talk about this plug-in substrate, right?


511
00:26:51,200 --> 00:26:53,200
That, to me, is the real interesting part as a community.


512
00:26:53,200 --> 00:26:54,200
We really come together.


513
00:26:54,200 --> 00:26:59,200
Because we don't want everybody forking a Homestar node and writing their own piece of this, right?


514
00:26:59,200 --> 00:27:02,200
Even though we'll support WASM and some other things as first class.


515
00:27:02,200 --> 00:27:07,200
We want to be able to have something that just fits in with the substrate.


516
00:27:07,200 --> 00:27:10,200
We talk about the WASM part specifically.


517
00:27:10,200 --> 00:27:16,200
Interpreter, SID ahead-of-time validation, WASI components for doing some stuff internally, browser support.


518
00:27:16,200 --> 00:27:20,200
We want to obviously execute WASM in the browser.


519
00:27:20,200 --> 00:27:23,200
And then, you know, Brooke will talk more about managed effects.


520
00:27:23,200 --> 00:27:29,200
These are a lot of the ones that we're thinking about building in-house as part of the kind of first class system for Homestar.


521
00:27:29,200 --> 00:27:31,200
And, yeah, the end is only the beginning.


522
00:27:31,200 --> 00:27:36,200
You know, I hope the next few days at the conference, people, as we show some of the code and go through stuff,


523
00:27:36,200 --> 00:27:40,200
that, A, IPVM implementation of it is really happening.


524
00:27:40,200 --> 00:27:44,200
And that there are a lot of really great open questions that we should have some powwows on.


525
00:27:44,200 --> 00:27:46,200
So, awesome.


526
00:27:46,200 --> 00:27:47,200
Thank you.


527
00:27:47,200 --> 00:27:54,200
Thank you, Zeeshan.


528
00:27:54,200 --> 00:27:56,200
Amazing presentation.


529
00:27:56,200 --> 00:27:58,200
We have time for one question.


530
00:27:58,200 --> 00:28:01,200
There's only one question.


531
00:28:01,200 --> 00:28:04,200
Somebody else can go.


532
00:28:04,200 --> 00:28:05,200
I just had a quick question.


533
00:28:05,200 --> 00:28:08,200
If you go back to the slide with the architecture diagrams.


534
00:28:08,200 --> 00:28:09,200
Yeah, let me go to...


535
00:28:09,200 --> 00:28:11,200
I think it's 24, I think.


536
00:28:11,200 --> 00:28:12,200
Oh, just like the last...


537
00:28:12,200 --> 00:28:13,200
Yeah.


538
00:28:13,200 --> 00:28:14,200
Okay.


539
00:28:14,200 --> 00:28:18,200
So, how do you define the run time of actually running those things that is kind of outside of the VM, in a sense?


540
00:28:18,200 --> 00:28:22,200
Like, where do you draw the boundary in the run time?


541
00:28:22,200 --> 00:28:27,200
Because there are, for example, things around the network and so on that might be outside of the VM that you sort of plug in.


542
00:28:27,200 --> 00:28:28,200
Yeah.


543
00:28:28,200 --> 00:28:29,200
How do you draw those boundaries?


544
00:28:29,200 --> 00:28:35,200
To me, it's more about the run time essentially is what the node interacts with.


545
00:28:35,200 --> 00:28:36,200
So, for me, it's like...


546
00:28:36,200 --> 00:28:39,200
Well, I would say even the run time is something that...


547
00:28:39,200 --> 00:28:42,200
When you think about discoverability, it's also networking-based.


548
00:28:42,200 --> 00:28:48,200
So, the run time is what essentially is also understanding of how to bootstrap, how to get on the network, those kinds of things.


549
00:28:48,200 --> 00:28:54,200
So, the networking part of it is obviously a separate piece, but the run times have to know about that interaction point.


550
00:28:54,200 --> 00:28:57,200
So, I put it in the bucket for part of the run time.


551
00:28:57,200 --> 00:28:59,200
That's how I decided to find it.


552
00:28:59,200 --> 00:29:01,200
Yeah, discoverability.


553
00:29:01,200 --> 00:29:06,200
One thing is we already have a pretty good story for how to build the bootstrapping layers.


554
00:29:06,200 --> 00:29:09,200
But hard parts of evaluation, when you think about...


555
00:29:09,200 --> 00:29:12,200
As we start thinking about more and more kinds of workflows, you really start getting into...


556
00:29:12,200 --> 00:29:17,200
Okay, you have enough GPU to run a certain kind of compute.


557
00:29:17,200 --> 00:29:18,200
Now I'm going to give you the workflow.


558
00:29:18,200 --> 00:29:19,200
You can run it.


559
00:29:19,200 --> 00:29:21,200
Maybe the data is also more local to you.


560
00:29:21,200 --> 00:29:22,200
So, here you go.


561
00:29:22,200 --> 00:29:23,200
Run it.


562
00:29:23,200 --> 00:29:28,200
And then maybe at the point of you running it, you're like, I got really busy, and I don't have enough CPU or GPU anymore.


563
00:29:28,200 --> 00:29:31,200
So, now I have to move that workflow somewhere else.


564
00:29:31,200 --> 00:29:33,200
So, it's a classic routing problem.


565
00:29:33,200 --> 00:29:38,200
I used to work at Comcast where we did this all in network with semantics on the packets.


566
00:29:38,200 --> 00:29:44,200
But I think one thing I really could think about is how to move around workflows and still get the guarantees.


567
00:29:44,200 --> 00:29:54,200
I mean, you hope with workflow systems, like when you look at temporal and cadence that people are using at big scale companies and stuff that AWS does, it's all tied to SLAs.


568
00:29:54,200 --> 00:29:59,200
So, if you want people to have a compute system, they're probably also going to bring their SLAs to you.


569
00:29:59,200 --> 00:30:01,200
And that's a problem.


570
00:30:01,200 --> 00:30:03,200
Performance matters.


571
00:30:03,200 --> 00:30:06,200
Movement of data and movement of code matters.


572
00:30:06,200 --> 00:30:09,200
If you can cache more things, that matters.


573
00:30:09,200 --> 00:30:15,200
And so, I think that's a big  for me, when I think of workflow engines, I think of at that scale that the big companies go.


574
00:30:15,200 --> 00:30:17,200
And that's what we have to build for.


575
00:30:17,200 --> 00:30:19,200
Yeah, I'm around, so you can find me.


576
00:30:19,200 --> 00:30:20,200
Cool.


577
00:30:20,200 --> 00:30:38,200
Thank you.
