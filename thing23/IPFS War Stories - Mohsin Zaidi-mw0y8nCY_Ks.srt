1
00:00:00,000 --> 00:00:09,040
Good afternoon everyone. My name is Mohsen. I'm one of the maintainers of the Ceramic


2
00:00:09,040 --> 00:00:16,400
Network. I've met a bunch of you in the past. It's nice to see everyone again. In this talk


3
00:00:16,400 --> 00:00:24,400
I wanted to present some of our struggles using IPFS for our production systems. Struggles.


4
00:00:24,400 --> 00:00:33,600
We've used vanilla IPFS, JS, and Kubo. And that's important because we've now got a larger team, but


5
00:00:33,600 --> 00:00:37,760
before we didn't have the bandwidth to piece together components that would work the best.


6
00:00:37,760 --> 00:00:42,880
So we relied more on vanilla components so that we could focus on building our own protocols.


7
00:00:44,640 --> 00:00:51,040
Some of this mix and match that is now possible wasn't available, so we can do more things now


8
00:00:51,040 --> 00:00:57,120
that we couldn't in the past. Some of the problems I'm going to discuss have been fixed, either by us


9
00:00:57,120 --> 00:01:03,040
or for us, with a lot of help and support from Protocol Labs. So I see Steve here,


10
00:01:03,040 --> 00:01:07,840
I don't know if Lidl's here, Gus is here. People who've helped us a lot, so it's very appreciated.


11
00:01:09,120 --> 00:01:13,760
Some of these things could be things we've just been doing wrong, and we'd love feedback.


12
00:01:13,760 --> 00:01:20,240
Lastly, I'm likely going to finish ahead of time, so anyone else has more stories,


13
00:01:20,240 --> 00:01:22,000
feel free to share at the end.


14
00:01:26,800 --> 00:01:32,160
All right, so how does Ceramic use IPFS? Ceramic is an advanced streaming protocol with streams


15
00:01:32,160 --> 00:01:40,320
of hash-linked commits that are stored in IPFS. These are IPLD commits, IPLD data structures.


16
00:01:40,320 --> 00:01:47,280
Ceramic also provides additional primitives for identity data composability and for data modeling


17
00:01:47,280 --> 00:01:53,280
across nodes and applications. Stream tips are synchronized by sending requests over PubSub,


18
00:01:53,280 --> 00:01:57,360
and the commits that are discovered via that mechanism are synchronized using Ritzwap.


19
00:01:58,640 --> 00:02:06,320
Ceramic also posts these new tips to Ethereum, bunches them together in a Merkle tree,


20
00:02:06,320 --> 00:02:11,600
and posts it to Ethereum, posts the root CID to Ethereum, posts those commits to IPFS,


21
00:02:11,600 --> 00:02:15,520
and also propagates them through PubSub to the rest of the Ceramic network.


22
00:02:16,720 --> 00:02:20,720
This is critical for Ceramic because it is our source of eventual consistency.


23
00:02:26,960 --> 00:02:31,600
So these were some of the key challenges we faced running IPFS in production. I'm sure a lot of you


24
00:02:31,600 --> 00:02:36,720
could identify with these. These are the things we struggled with the most. I'm going to cover each


25
00:02:36,720 --> 00:02:48,560
of them briefly. The graph on top is the memory utilization for when we were using JS-IPFS


26
00:02:48,560 --> 00:02:54,880
in production. As you can see, it has a very pretty sawtooth pattern of how it would


27
00:02:54,880 --> 00:03:04,320
go up to about 100% and then go back to zero because it would crash. So, the silver lining.


28
00:03:05,280 --> 00:03:14,880
And that would prevent our anchoring process from completing often and would cause a lot of work to


29
00:03:14,880 --> 00:03:22,880
be repeated. We ended up pre-anchoring the whole thing. We had to do a lot of work to get it to


30
00:03:22,880 --> 00:03:29,600
work. We ended up preemptively rebooting. So we would reboot IPFS, rush to do the anchoring,


31
00:03:29,600 --> 00:03:32,880
and then once anchoring was done, we would push everything and then we would reboot it again.


32
00:03:33,520 --> 00:03:42,960
So we ended up doing that to get anchoring to work. The bottom graph is for Kubo. Due to API calls,


33
00:03:42,960 --> 00:03:48,800
number of API calls, or other network conditions, it would also have an unbounded number of


34
00:03:48,800 --> 00:03:55,600
go routines, which would result in out of memory crashes. And Gus was very helpful. We had a long


35
00:03:55,600 --> 00:04:02,160
discussion about this. We did add, we can and we have added rate limits. But ultimately, the best


36
00:04:02,160 --> 00:04:07,360
solution would be to have back pressure because you can't really enforce rate limits. You can't


37
00:04:07,360 --> 00:04:12,800
hard code them because something else could change that would make your hard coded limit not correct.


38
00:04:12,800 --> 00:04:19,200
Sometimes there are timeouts without any other reason, like the memory and CPU will be fine,


39
00:04:19,200 --> 00:04:24,320
it'll still time out for data we know is available on the node. It will not respond with it. It just


40
00:04:24,320 --> 00:04:36,480
refuses to respond with it. These are more probably specific to us. We're heavy users of the POP sub


41
00:04:36,480 --> 00:04:46,000
network. It's hard to scale and be highly available when you have pet servers, which is how POP sub


42
00:04:47,520 --> 00:04:52,320
works with IPFS nodes. Each node has its own identity and it's not interchangeable with other


43
00:04:52,320 --> 00:05:01,200
nodes. You can't just swap an identity. You can't share identity. That makes it hard to spin up more


44
00:05:01,200 --> 00:05:07,280
nodes behind a load balancer and treat it like a cluster that has an identity. There is an IPFS


45
00:05:07,280 --> 00:05:12,480
cluster operator, but that's not what it's meant for. The other thing you can't do is you can't


46
00:05:14,880 --> 00:05:20,480
upgrade a node without downtime. Every time you want to upgrade one of the nodes, it has to go


47
00:05:20,480 --> 00:05:25,280
down so it'll release the repo lock and then you upgrade it and bring it back up. There's always


48
00:05:25,280 --> 00:05:37,920
some amount of downtime. This is another problem we've faced a lot, POP sub connectivity. I know


49
00:05:37,920 --> 00:05:44,800
other people have as well. With JSIPFS, nodes would suddenly just stop receiving POP sub traffic


50
00:05:44,800 --> 00:05:50,640
and we wouldn't know why if they went too long without receiving a message. To solve that, we set


51
00:05:50,640 --> 00:05:57,360
up keepalive lambdas in AWS. Then we just added code to ceramic to just send keepalives every few


52
00:05:57,360 --> 00:06:02,240
minutes within a certain amount of time so that there was some traffic going on POP sub so that


53
00:06:02,240 --> 00:06:09,280
it wouldn't die. That helped a bit. It did not eliminate the issue. For that, we wrote more


54
00:06:09,280 --> 00:06:15,200
automation that would just restart IPFS if there were no POP sub messages received for 30 minutes.


55
00:06:15,200 --> 00:06:26,240
With JSIPFS, well, with that, it definitely helped. Kubo does better, but it still has


56
00:06:27,440 --> 00:06:33,440
times when it just stops receiving POP sub traffic. It happened recently. It's very


57
00:06:33,440 --> 00:06:35,120
infrequent with Kubo, but it does still happen.


58
00:06:35,120 --> 00:06:43,520
This was terribly painful, migrating from JSIPFS to Kubo. We had to write code to migrate the


59
00:06:43,520 --> 00:06:51,760
pin store because JSIPFS had the S3 plugin for the pin store. Kubo did not. Again, some of this


60
00:06:51,760 --> 00:06:55,440
is stuff we could have built. We just were in the middle of migrating. We didn't have the time to


61
00:06:55,440 --> 00:07:02,800
do that. We ended up spending a lot of time on that. We also had to write code to migrate the


62
00:07:02,800 --> 00:07:08,800
pin store. We ended up spending time anywhere migrating the pin store. The block store naming


63
00:07:08,800 --> 00:07:15,040
format was inconsistent between JSIPFS and Kubo. It was like even on the same repo version, it was


64
00:07:15,040 --> 00:07:19,520
inconsistent. We were terrified we would have to migrate the entire block store, which is


65
00:07:19,520 --> 00:07:22,160
probably a terabyte of information in each of our environments.


66
00:07:24,080 --> 00:07:30,000
Thankfully, we moved to repo version 12, which was consistent on Kubo and JSIPFS,


67
00:07:30,000 --> 00:07:34,560
so we didn't have to touch the block store. It had the S3 plugin with some tweaks, so we


68
00:07:34,560 --> 00:07:39,200
got to work. Secure web sockets were not supported in Kubo out of the box.


69
00:07:40,480 --> 00:07:46,400
All our multi-addresses were secure web sockets. We had to change all of our multi-addresses,


70
00:07:46,400 --> 00:07:50,160
ask all of our partners to change their multi-addresses so that we could all use


71
00:07:50,160 --> 00:07:58,880
each other's nodes. It is now supported out of the box in Kubo, but it wasn't back then.


72
00:08:03,840 --> 00:08:13,200
Debugging. Yes. I have looked at Kubo logs many times. I've just looked at the first page and


73
00:08:13,200 --> 00:08:20,800
then closed the tab because it's hard to tell what matters. Logs can be verbose. They can include


74
00:08:20,800 --> 00:08:28,640
errors from other nodes, which has been really weird. There's no easy way to identify sources


75
00:08:28,640 --> 00:08:40,800
of memory growth, no easy way to identify sources of delays. Some takeaways, like building production


76
00:08:40,800 --> 00:08:47,280
databases over vanilla IPFS can get tricky. Definitely dragons, not the cute ones. Some


77
00:08:47,280 --> 00:08:54,240
more problems can definitely now be solved by putting together components in a better way that


78
00:08:54,240 --> 00:09:00,960
wasn't possible before, and also building new components over existing ones, both of which


79
00:09:00,960 --> 00:09:07,040
are avenues we're exploring. So putting together a better combination of packages, and there's been


80
00:09:07,040 --> 00:09:12,880
some great presentations today just about that. Some great new protocols being built that we can


81
00:09:12,880 --> 00:09:20,880
now pick from. And it's great to see that the community is aware of all these shortcomings.


82
00:09:20,880 --> 00:09:25,760
Nobody hides these problems. Everyone's forthcoming about them, and a lot of people


83
00:09:25,760 --> 00:09:32,400
working to fix them, which is great. Now for some fun random weirdness, which I don't know


84
00:09:32,400 --> 00:09:38,240
how many people would have seen these things. My very favorite is the one at the top, is that every


85
00:09:38,240 --> 00:09:45,760
other dagget RPC call would fail. Like every other call, every second call would fail. And that was


86
00:09:45,760 --> 00:09:53,920
because the IPFS HTTP client defaulted to using keepalive true for the node connection pool,


87
00:09:54,480 --> 00:09:59,440
which meant it used reused connections, but Kubo did not support connection reuse. It would kill


88
00:09:59,440 --> 00:10:04,000
the connection as soon as the first one completed, and the second one would try...


89
00:10:04,880 --> 00:10:10,080
So node would try to reuse the connection, and this was also a bit of a weird interaction with


90
00:10:10,080 --> 00:10:16,240
the AWS load balancer, and the IPFS HTTP client would make another dagget call. That would fail


91
00:10:16,240 --> 00:10:21,280
because Kubo had killed the connection from its side, so we had to set keepalive false to


92
00:10:22,160 --> 00:10:25,760
to not kill every other call. That was a very interesting investigation.


93
00:10:25,760 --> 00:10:33,360
IPFS dagget can stall for hours and successfully resolve some connection limit problem, we're not


94
00:10:33,360 --> 00:10:40,880
sure. The third one's also very interesting. You're connected to a peer that you know has the


95
00:10:40,880 --> 00:10:47,920
data you want, but it will time out trying to get that data. And then you give up and you kill that


96
00:10:48,800 --> 00:10:53,680
IPFS CLI call, and then you run it again and it works right away. And we've just seen this


97
00:10:53,680 --> 00:10:58,480
pattern happen so many times, so it's not more like happenstance. It's just...


98
00:11:04,960 --> 00:11:05,360
This is...


99
00:11:05,360 --> 00:11:08,480
What if you turn off the timeouts for like seven minutes,


100
00:11:08,480 --> 00:11:10,320
and it still times out, and then comes back to me?


101
00:11:13,760 --> 00:11:20,720
This is our newest struggle. Some of this is potentially the way Ceramic interacts with IPFS,


102
00:11:20,720 --> 00:11:27,520
so it's not like an IPFS-only thing. A large number of duplicate messages have been propagating


103
00:11:27,520 --> 00:11:33,920
through the Ceramic network, and we've adjusted the seen message cache TTL, so there's a cache


104
00:11:33,920 --> 00:11:38,720
inside pops up for messages that have been seen within a certain window of time. We've adjusted


105
00:11:38,720 --> 00:11:45,600
the TTL, so with Protocol Labs' help, we made the TTL configurable, so we were able to adjust it


106
00:11:45,600 --> 00:11:53,280
to its larger window we set now. And we also found a problem with the cache implementation itself.


107
00:11:53,280 --> 00:11:58,400
It was a first seen cache, so it would see a message once, and then even if it saw it a bunch


108
00:11:58,400 --> 00:12:04,560
of times after, it would take the time it saw it first as the start of its window for the TTL.


109
00:12:04,560 --> 00:12:09,600
That way, you would see a lot more duplicates get through, so we re-implemented that to be


110
00:12:09,600 --> 00:12:15,920
a last seen cache, so now it would be at least X amount of time, which is configurable from the


111
00:12:15,920 --> 00:12:21,520
last time you saw the message before it was let through the cache again. So that helped.


112
00:12:22,800 --> 00:12:29,440
Now we see fewer duplicates getting propagated, but now our nodes are still getting somewhat


113
00:12:29,440 --> 00:12:35,840
overwhelmed, like receiving messages that get validated and parsed and then rejected because


114
00:12:35,840 --> 00:12:40,720
they didn't make it through the cache. But the CPU utilization is at like 50-60% just trying


115
00:12:40,720 --> 00:12:49,280
to reject those messages. But we're looking at this from multiple angles, from the ceramic side


116
00:12:49,280 --> 00:12:51,440
and the PubSub side.


117
00:12:55,040 --> 00:12:57,520
At least we can think of good signatures when you're rejecting.


118
00:12:57,520 --> 00:13:07,840
Yeah, so if anyone else has any stories to share, you're welcome to. I think I finished way ahead


119
00:13:07,840 --> 00:13:10,640
of time, but that was the intention.


120
00:13:10,640 --> 00:13:17,840
Cool, yeah, so this might be a good session for comments more than questions, or maybe answers


121
00:13:17,840 --> 00:13:22,080
from the community. I know a feeling that we've had at Fission sometimes is like,


122
00:13:22,080 --> 00:13:28,880
clearly we're just holding the IPFS wrong, and everybody else here must be doing this


123
00:13:28,880 --> 00:13:36,160
better than we are. So yeah, anybody have anything they want to share? Answers, questions, thoughts?


124
00:13:37,280 --> 00:13:42,720
JS IPFS to Kubo. Why did you start with JS IPFS?


125
00:13:43,680 --> 00:13:50,640
Initially, Ceramic was meant to run in browsers, and then it seemed like a natural transition from


126
00:13:50,640 --> 00:13:55,600
it's in JS, run in the browser, and now we switch to Node.js, run it on the server. It's no longer


127
00:13:55,600 --> 00:14:04,400
supported in the browser, but JS IPFS stayed. And then we used it for a while, I think a couple


128
00:14:04,400 --> 00:14:11,520
of years, until like mid-2022, last year is when we switched to Kubo. So that was the reason. It


129
00:14:11,520 --> 00:14:16,720
was more historical that we picked JS IPFS as the thing.


130
00:14:17,920 --> 00:14:18,560
Awesome, thank you.


131
00:14:18,560 --> 00:14:30,560
I'm interested in your kind of interface with folks in the rest of the IPFS cinematic universe.


132
00:14:30,560 --> 00:14:35,920
So when you were trying to fix this stuff, right? So you mentioned that you worked with Protocol


133
00:14:35,920 --> 00:14:42,800
Labs to fix some of these problems. There's clearly a few things here where like, it shouldn't be down


134
00:14:42,800 --> 00:14:50,000
to you, they're fixes that would make the whole network better. Did you find like it was useful


135
00:14:50,000 --> 00:14:58,720
to be raising bugs, raising issues in the GitHub repo? How did you decide when it was time to like


136
00:14:58,720 --> 00:15:04,080
pick up the flashing red telephone and call someone at Protocol Labs and say, this is fixed? And


137
00:15:04,640 --> 00:15:12,160
is there any ways that you would like that relationship to improve for you and for other


138
00:15:12,160 --> 00:15:17,600
people? Because of course, some of these things just don't scale, right? You can't. And yeah,


139
00:15:18,320 --> 00:15:20,880
I guess those are my three, four questions.


140
00:15:22,720 --> 00:15:31,680
Yeah, I mean, we found that our interface with... So we started out with GitHub issues


141
00:15:32,480 --> 00:15:39,600
and communicating with the team on Slack. And it's always been super responsive,


142
00:15:39,600 --> 00:15:43,280
mostly been super responsive. And when it's not been, it's not been like a,


143
00:15:43,920 --> 00:15:47,920
it's been understandable. There's never been like a inordinate amount of delay getting back to us.


144
00:15:48,960 --> 00:15:55,360
There's always been a way to kind of escalate if we needed to. And kind of GitHub, Slack,


145
00:15:55,360 --> 00:16:01,760
maybe other ways sometimes. But it's always been very responsive, very detailed responses,


146
00:16:01,760 --> 00:16:09,680
timely. We've had to do make fixes ourselves because if it was something Protocol Labs


147
00:16:09,680 --> 00:16:15,440
knew and accepted as a problem and requested for like contributions because they didn't have


148
00:16:15,440 --> 00:16:22,800
like the resources. And I mean, our experience has been good. Running into those problems is


149
00:16:22,800 --> 00:16:28,000
never fun. And solving some of those problems is very much not fun when you don't really know


150
00:16:28,000 --> 00:16:36,720
what's going on. These are complicated systems. But I feel like we've generally had... It's,


151
00:16:37,600 --> 00:16:44,400
I know it's, it's difficult. It's rough for us. So it's easy to feel angry.


152
00:16:45,600 --> 00:16:51,200
And we've felt upset a lot of times, but that's something we worked through.


153
00:16:51,200 --> 00:16:53,280
Has the war been won yet?


154
00:16:53,280 --> 00:16:59,360
It's a good question. A few battles have been won. The war is not won yet. No. Yeah.


155
00:16:59,360 --> 00:17:00,160
You've done some trenches.


156
00:17:00,160 --> 00:17:00,640
Yeah.


157
00:17:00,640 --> 00:17:04,960
All the run. Also, did you get an opportunity to work with Helia?


158
00:17:04,960 --> 00:17:13,840
We have not. I just heard from Daniel before this talk that he's had a great experience with Helia.


159
00:17:13,840 --> 00:17:14,880
Give it another shot.


160
00:17:14,880 --> 00:17:20,960
Yeah. I was telling him it'd be, it's kind of hilarious, a little sob, to go from JS to


161
00:17:20,960 --> 00:17:26,720
Kubo and then back to Helia. Full circle. Yep. Which it's very interesting. I'm definitely


162
00:17:26,720 --> 00:17:32,000
going to give it a shot. If it works well, we could switch to Helia. We need some time


163
00:17:32,000 --> 00:17:37,920
before we implement. We've gotten to the stage where our scale requires implementation of some


164
00:17:37,920 --> 00:17:44,640
custom protocols. And at this point, it will take a little bit of time for us to get there.


165
00:17:44,640 --> 00:17:50,720
In the meantime, to address some of the things we're struggling with, if there are ways to


166
00:17:50,720 --> 00:17:56,240
swap out some components or maybe even Kubo itself with Helia, if that helps get us through


167
00:17:56,240 --> 00:18:00,640
the phase we are in right now, that would be perfect. So definitely going to take a look


168
00:18:00,640 --> 00:18:06,560
at Helia, some of the other protocols that were brought up today. Gold card mirror sounds


169
00:18:06,560 --> 00:18:09,360
very interesting. So we'll definitely take a look at some of these.


170
00:18:09,360 --> 00:18:10,800
Thanks a lot for speaking to us.


171
00:18:10,800 --> 00:18:15,840
Thanks a lot for sharing with all that you have learned and if you could go back and do things.


172
00:18:15,840 --> 00:18:20,720
Or are you starting from scratch today? Are there any recommendations that you have? Or


173
00:18:20,720 --> 00:18:25,520
said another way, how would you want to evolve things in the future with the battle scars and


174
00:18:25,520 --> 00:18:28,640
newest learnings that you have? And totally understandable if you don't have an answer


175
00:18:28,640 --> 00:18:31,120
for that yet. It's something you have to think about. But if there are things that come up


176
00:18:31,120 --> 00:18:32,400
to the top of your head, love to hear them.


177
00:18:32,400 --> 00:18:37,840
Yeah. I mean, off the top of my head, I think I loved the, like to go back in time a little,


178
00:18:37,840 --> 00:18:43,280
I loved the transition from Go IPFS to Kubo. I understand why Protocol Labs did it. It was


179
00:18:43,280 --> 00:18:49,520
becoming the IPFS implementation. Now it is an IPFS implementation, not like everything for


180
00:18:49,520 --> 00:18:55,600
everyone, which it was becoming. Now, since then, it has become easier to mix and match


181
00:18:55,600 --> 00:19:02,800
components. So now people, I would say, should look at picking like the best of class of what


182
00:19:02,800 --> 00:19:08,080
they need versus being like, oh, Kubo should do everything for me or Helios should do everything


183
00:19:08,080 --> 00:19:15,920
for me. So paying more attention to individual components and how to like put them together.


184
00:19:15,920 --> 00:19:19,120
Hopefully that keeps getting easier. I know it is easier now than it was in the past.


185
00:19:20,000 --> 00:19:24,880
If it keeps getting easier, that would be one recommendation would be to pick what you need


186
00:19:24,880 --> 00:19:31,200
versus just picking. Not every team can do that, though. We're probably in the same boat.


187
00:19:31,200 --> 00:19:36,160
We're probably in a position where we can now if we wanted to. But the easier it gets,


188
00:19:36,160 --> 00:19:41,040
the easier smaller teams can just be like, oh, I want these six packages. That's all I need.


189
00:19:41,040 --> 00:19:44,720
And I'll put them together. The easier it gets, the easier it will be for smaller teams to do that.


190
00:19:45,840 --> 00:19:47,040
So that's one of the things I would say.


191
00:19:48,080 --> 00:19:53,040
So Danny kind of asked this and you answered of like, sort of like how was connecting in


192
00:19:53,040 --> 00:19:58,640
and everything else. So Fission and James already kind of said it where, you know,


193
00:19:58,640 --> 00:20:02,000
we spent a long time.


194
00:20:00,000 --> 00:20:29,980
and again, it was go IPFS was in a lot clunkier stake. You know, we found every release and continues to be better, but we were sort of wandering around in the wilderness for six months, nine months before we pierced the veil of PL specifically cake and why we're like, oh, yeah, totally. Yeah. Yeah, like ignore those public channels where everyone's like, why the fuck is that?


195
00:20:30,000 --> 00:20:53,980
I mean, all this stuff doesn't work. Here's the three people to talk to. So I wanted to just like add that for the record. I think there's actually still not good onboarding for IPS operators that the default experiences. I threw it up. I ran it. What that face basically and a bunch of us have now been like, oh, you know, we know where the bodies are buried.


196
00:20:53,980 --> 00:21:11,500
Don't touch that port port. What do you mean? Everybody turns off garbage collection in production, right again? Laughter from everyone, right? But but that's not reflected in public documentation or best practices. So I really want to take this as a call for great.


197
00:21:11,500 --> 00:21:29,500
We should reboot IPS operators. We should continue to make it as an on ramp for folks, and we should continue to work together in looking for saying like, hey, we're about to put two engineers on something for six months.


198
00:21:29,500 --> 00:21:43,500
Before we do this, does someone else want to help? And I see that very like collaboratively sort of thing, right? Like we're actually still sort of in a situation where we're like, well, thank God Gus is running all of those test runners.


199
00:21:43,500 --> 00:22:04,500
And in part, I now understand why we haven't been able to help more. So that was really helpful for me. So, you know, I also picked in what you were saying just recently. So this is the question. Part of my comment was was that you're like, oh, we're about to start doing some, you know, at our scale and for our use case.


200
00:22:04,500 --> 00:22:16,500
We now need to do some custom stuff. So we're gonna like look around for sort of things like that. And I that's the same sort of thing. I'd love for us to be able to be like, like, like actually tell everyone in the community.


201
00:22:16,500 --> 00:22:36,500
Hey, if you're about to embark on that six months to engineer sort of thing. Let's talk to see one who else wants to throw money, resources, requirements, what's whatever into the pot. This is always hard. Because ultimately, it's like, no, we need we need to move quickly for our own resources or like whatever kind of thing like that.


202
00:22:36,500 --> 00:23:01,500
So maybe just like, again, sort of like a public call for like, hey, awesome. At the very least, it would be amazing for you to publish, like, here's what we're trying to do. And we ended up picking up x whenever whatever you decide to do so that we can all learn slash have an opportunity to say like, Oh, yeah, that's great. We'll totally contribute in in x way. Does that make sense?


203
00:23:01,500 --> 00:23:29,500
It does. And it's funny you say that there actually is a write up off what we're planning. Nathaniel has written up it's, it's been shared in some places, I think with the number zero team, or some people has been shared with it had not been broadcasted. But that's a great idea. We'll definitely send it out to more people. So definitely, that's a that's a good point. And the other thing I was going to say is, thank you for starting the IPFS operators challenge. I know you and Brooke did that.


204
00:23:29,500 --> 00:23:51,500
It's been kind of like a godsend because it's a way to pierce the veil. It's not just one issue among I don't know how many issues in Cuba now 1000 1200. It's just it's not one issue among 1200. Now it's like, there's an issue and there's some escalation path at least. So both great points. Thank you.


205
00:23:51,500 --> 00:24:07,500
The other point I wanted to make slightly more generally, so we had the specific instances of rather than getting any kind of back pressure. If you sent too many requests at Kubo, it just fell over and then you rebooted it and now you're back in a healthier state.


206
00:24:07,500 --> 00:24:25,500
And when gossip sub, the slowest node on the network is the one that is verifying too many signatures and suddenly has a queue that is longer than the timeout time. So every message that verifies is out of the timeout window because that's how long the queue is.


207
00:24:25,500 --> 00:24:43,500
So you can have a channel that's got 30 real data center servers on it that is dealing with this flood of traffic because a raspberry pie can't keep up with the channel and just keeps throwing the messages back in five minutes after they happened.


208
00:24:43,500 --> 00:24:55,500
Thinking about ways to make stuff gracefully degrade, I think, would be very helpful of maybe if the raspberry pie can't keep up, it would be nice if the raspberry pie had a bad time.


209
00:24:55,500 --> 00:25:13,500
But the racks of servers in data centers were still happy. And I think there's just a few places in Kubo where taking this approach of, okay, how can some, I mean, I know a lot of people define a distributed system as when a computer you didn't know existed can break your code.


210
00:25:13,500 --> 00:25:28,500
But there's definitely these times where we weren't even being deliberately attacked. It was just a slow node brought the whole channel down. Nonetheless, how do you protect a PubSub channel from someone who's deliberately trying to spam the thing?


211
00:25:28,500 --> 00:25:43,500
Thank you, everyone.
