1
00:00:00,000 --> 00:00:11,520
Next up, we're going to talk about IPNI and what it actually is. This is an acronym that


2
00:00:11,520 --> 00:00:20,400
came up a number of times yesterday in data transfer track, in the opening talks. It is


3
00:00:20,400 --> 00:00:25,760
not something new. The chances are you have already heard about it. It's just a bit of


4
00:00:25,760 --> 00:00:34,080
rename and a bit of restructuring, which I thought it deserves a separate talk to crystallize


5
00:00:34,080 --> 00:00:41,480
what is a protocol, increasingly becoming a protocol, and what is an implementation.


6
00:00:41,480 --> 00:00:50,800
So I put together this talk to talk about that specific protocol versus its implementations,


7
00:00:50,800 --> 00:00:59,280
other plans in terms of protocol perspective, and then walk you through probably the most


8
00:00:59,280 --> 00:01:06,840
developed instance of IPNI, which is SID.contact, that comes after that. So with that in mind,


9
00:01:06,840 --> 00:01:17,600
let's get started. What is IPNI? It's an alternative routing system which is designed to provide


10
00:01:17,600 --> 00:01:30,000
SIDs by the billion, by the bucket load, while maintaining sub-10 millisecond latency-ish.


11
00:01:30,000 --> 00:01:36,920
It is a protocol that is totally different from the way by which DHT, for example, advertises


12
00:01:36,920 --> 00:01:43,020
content and looks up information, and it makes very distinct design decisions to make this


13
00:01:43,020 --> 00:01:52,560
statement become true. Because the fact remains that right now, we cannot provide content


14
00:01:52,560 --> 00:01:59,480
at the rate that is being generated in the DHT. So we need some sort of other mechanism


15
00:01:59,480 --> 00:02:07,080
to enable implementers, enable people that are building stuff on top of this simple yet


16
00:02:07,080 --> 00:02:12,760
extremely powerful ecosystem we call content address data to find their information. And


17
00:02:12,760 --> 00:02:25,320
that's where IPNI comes in. So why is it needed? I sort of touched on that a little bit earlier.


18
00:02:25,320 --> 00:02:33,880
We really need to understand why we need something like IPNI when there's already a routing system.


19
00:02:33,880 --> 00:02:39,360
As I touched on in my opening talk, there is a gap in the networks. There was a gap


20
00:02:39,360 --> 00:02:46,000
in the networks. There was Filecoin and there was IPFS. The amount of information on Filecoin


21
00:02:46,000 --> 00:02:53,520
is significantly larger than the amount of information on IPFS. Even ignoring that, in


22
00:02:53,520 --> 00:03:01,600
the IPFS world, we see sort of optimizations that are a result of a routing system not


23
00:03:01,600 --> 00:03:07,600
sort of being able to handle it, which is I'm only going to advertise my root CID, for


24
00:03:07,600 --> 00:03:11,720
example. I'm not going to advertise the rest of it. So we are starting to see this type


25
00:03:11,720 --> 00:03:18,440
of optimization on the IPFS world, which is pragmatic, but really in the long run is going


26
00:03:18,440 --> 00:03:24,440
to impact the user experience. And it's going to impact this idea of parallelism when it


27
00:03:24,440 --> 00:03:30,560
comes to retrieval, the idea of replication, the idea of caching. It has a huge ripple


28
00:03:30,560 --> 00:03:37,560
effect if we don't provide alternatives that truly allow us to advertise all the CIDs and


29
00:03:37,560 --> 00:03:45,640
you know, so it is pretty important. Large-scale content providers have totally different requirements


30
00:03:45,640 --> 00:03:55,440
than me running an IPFS node on my laptop, traveling to work on the train and back. Their


31
00:03:55,440 --> 00:04:04,000
addresses might change. The large-scale content providers, they do not want to re-advertise


32
00:04:04,000 --> 00:04:10,040
content every time they redeploy a node, for example. They want to be able to horizontally


33
00:04:10,040 --> 00:04:18,040
scale as the traffic for looking up information goes up and down because they want to be as


34
00:04:18,040 --> 00:04:26,000
efficient as possible, both in terms of performance as well as cost. And at the same time, they


35
00:04:26,000 --> 00:04:33,560
want to provide fast lookup. And when we are talking about content address data and content


36
00:04:33,560 --> 00:04:38,800
address data, looking up content address data, we're not really talking about matching the


37
00:04:38,800 --> 00:04:47,040
web2 performance. We absolutely talk about smashing it. Because when we are talking about


38
00:04:47,040 --> 00:04:51,760
a peer-to-peer network, on the paper, it has to be much, much faster than a centralized


39
00:04:51,760 --> 00:04:59,560
network. So what can we do to enable this type of interaction? And the last one, but


40
00:04:59,560 --> 00:05:09,120
not least, let's ignore scaling issues with existing content routing systems. Let's assume


41
00:05:09,120 --> 00:05:16,200
that the cost of connection is zero and we have unlimited connections to everybody. We


42
00:05:16,200 --> 00:05:23,880
are talking about, right now, we have about 10 to the 12th number of CIDs. That's in trillions.


43
00:05:23,880 --> 00:05:29,200
Where we want to get to is 10 to the 15th. Two orders of magnitude larger than that.


44
00:05:29,200 --> 00:05:35,040
Even at 10 to the 12th, if we do the rough calculation, with the size of network that


45
00:05:35,040 --> 00:05:41,160
we have now, which is about 20,000 nodes-ish, assuming all of them are well-connected and


46
00:05:41,160 --> 00:05:47,720
well-behaved, we're talking about gigabytes of data stored on each of these nodes for


47
00:05:47,720 --> 00:05:53,080
content routing alone. And as a person who is running IPFS on his machine on the way


48
00:05:53,080 --> 00:05:58,080
to work, sitting on a train, that's a bit of a, you know, I'm not sure if I really want


49
00:05:58,080 --> 00:06:10,320
to do this. So, this is another motivation here. The other one is the storage space,


50
00:06:10,320 --> 00:06:15,440
the cost of storage keeps going down. So, if we have an alternative solution that just


51
00:06:15,440 --> 00:06:21,320
has a simple machine, has a simple protocol, connects terabytes of data to this server-like


52
00:06:21,320 --> 00:06:28,520
thing, and, you know, hopefully over time this is not going to cost as much running


53
00:06:28,520 --> 00:06:34,880
because storage prices keep going down. And what we can do when the storage prices go


54
00:06:34,880 --> 00:06:43,560
down means we can reduce this notion of having to trust a central party by simply replicating


55
00:06:43,560 --> 00:06:50,320
the data. So, there are ways there. So, just to backtrack, there's a few design decisions


56
00:06:50,320 --> 00:06:56,960
here. The need to enable content routing for the rate of growth in content addressable


57
00:06:56,960 --> 00:07:05,560
data, the need for faster lookup, the need for flexibility on how you handle this content


58
00:07:05,560 --> 00:07:09,840
address data for the large-scale providers that have totally different requirements than


59
00:07:09,840 --> 00:07:18,400
a typical, you know, small node in an IPFS network. And more importantly, as the storage


60
00:07:18,400 --> 00:07:25,040
gets cheaper, we can replicate these and reduce this notion of single point of trust, which


61
00:07:25,040 --> 00:07:37,040
is something that we all strive for in this community. So, that is why IPNI exists.


62
00:07:37,040 --> 00:07:43,720
There's been a number of words in the past year or so that sort of use for explaining


63
00:07:43,720 --> 00:07:48,920
IPNI or pointing at IPNI interchangeably. I wanted to clarify that a little bit. So,


64
00:07:48,920 --> 00:07:59,560
IPNI really isn't network indexer. IPNI isn't a stored index, nor C.contact, nor an index


65
00:07:59,560 --> 00:08:06,600
provider. Great. A lot of information, but what is it really? So, if I want to restructure


66
00:08:06,600 --> 00:08:11,800
this, I want to say that all these things are members of IPNI. So, if you can think


67
00:08:11,800 --> 00:08:18,800
of network indexers as the serving nodes in the IPNI ecosystem, these are the nodes that


68
00:08:18,800 --> 00:08:24,960
are big nodes with lots of storage attached, and their job is to just provide a lookup


69
00:08:24,960 --> 00:08:31,560
fast and ingest by the bucket load. Stored index is just an implementation, and hopefully


70
00:08:31,560 --> 00:08:39,160
not the only one soon, of index providers. This is the server-side implementation.


71
00:08:39,160 --> 00:08:45,080
SID.contact, we're going to cover that in the next talk in a minute. And index providers.


72
00:08:45,080 --> 00:08:51,880
Index providers are just nodes that are implementing the protocol by which to provide information


73
00:08:51,880 --> 00:08:59,320
to the IPNI. So, in the Kubo world, you have the DHT client, for example, or its extensions


74
00:08:59,320 --> 00:09:07,120
that Guy kindly covered for us today. And in the IPNI world, you have the index providers,


75
00:09:07,120 --> 00:09:14,280
which are following a specific protocol in order to advertise content into the interplanetary


76
00:09:14,280 --> 00:09:20,280
network indexer. What does the protocol look like? So, let's


77
00:09:20,280 --> 00:09:27,640
dive a little bit deeper. We've got indexer nodes in the middle that are ingesting advertisements.


78
00:09:27,640 --> 00:09:33,520
The story begins on the left-hand side, where you have the index providers that are structuring


79
00:09:33,520 --> 00:09:41,680
their data in a specific way that is made to be super-efficient for ingestion, super-efficient


80
00:09:41,680 --> 00:09:51,000
for change, may it be address, may it be retrieval protocol, and may it be change in the subset


81
00:09:51,000 --> 00:09:57,960
of CIDs that are provided. So, this is important. We'll get back to that in a minute.


82
00:09:57,960 --> 00:10:04,000
And then these index providers, once they structure their data into this special way,


83
00:10:04,000 --> 00:10:09,720
they make an announcement to the network to say, hey, I have something. That announcement


84
00:10:09,720 --> 00:10:14,600
is just a message to say, there is something new, come get it. That's totally different


85
00:10:14,600 --> 00:10:21,280
from the way that DHT works. So, here you have this triggered passive interaction. I


86
00:10:21,280 --> 00:10:26,560
trigger something, and the network indexer comes and gets it at its own leisure, wherever


87
00:10:26,560 --> 00:10:34,160
it sees fit. So, then once the announcements are published into a network or explicitly,


88
00:10:34,160 --> 00:10:39,200
network indexers are going to reach to the provider, assuming they haven't seen the advertisement


89
00:10:39,200 --> 00:10:46,560
before, and they say, give me the data. They get the data, they structure it in a two-layer


90
00:10:46,560 --> 00:10:55,920
data store. We'll talk about that in a bit, some more, and then expose a simple REST API,


91
00:10:55,920 --> 00:11:03,360
which is just get CID or multi-hash. You call this endpoint, and you get the results back.


92
00:11:03,360 --> 00:11:07,840
So I want to talk a little bit about that structure on the left, because I think it's


93
00:11:07,840 --> 00:11:12,880
important to understand why and how it is different from the DHT. And then I'm going


94
00:11:12,880 --> 00:11:19,920
to talk internally about how this is structured inside the network indexer, though outside


95
00:11:19,920 --> 00:11:24,960
the specification, this is outside the specification, but this is typically how we see the data


96
00:11:24,960 --> 00:11:31,040
being structured on the nodes for scalability reasons. So on the left-hand side, the structure


97
00:11:31,040 --> 00:11:38,240
by which the content is advertised by the providers looks a lot like a blockchain. You


98
00:11:38,240 --> 00:11:44,640
have an advertisement, which is a root node. That advertisement has very simple fields


99
00:11:44,640 --> 00:11:52,080
like what is the peer ID, what is the address, information about how the data can be retrieved,


100
00:11:52,080 --> 00:11:59,040
which is a new thing that doesn't exist in DHT. And then it has a pointer or a link to


101
00:11:59,040 --> 00:12:07,360
a series of chunks or entry chunks, which actually contain the multi-hashes themselves.


102
00:12:07,360 --> 00:12:17,120
This entire structure is IPLD data. So we are using content address representation for


103
00:12:17,120 --> 00:12:24,080
advertising the content address information. And I think that is quite nice, because just


104
00:12:24,080 --> 00:12:28,720
having the advertisements as content address data just allows you to reuse the existing


105
00:12:28,720 --> 00:12:35,200
ecosystem for looking up other things, which we will talk about later a little bit. But


106
00:12:35,200 --> 00:12:44,240
I want you to imagine looking up other copies of an advertisement using the same content


107
00:12:44,240 --> 00:12:48,800
address network that enables looking up the information in the first place. I think there's


108
00:12:48,800 --> 00:13:02,080
something beautiful about that. The advertisements themselves never directly contain the entries


109
00:13:02,080 --> 00:13:07,680
that are removed from the chain. That is an important optimization. So if I have advertised


110
00:13:07,680 --> 00:13:14,480
millions of multi-hashes for me to say, I no longer have this, this is a very small


111
00:13:14,480 --> 00:13:19,360
data, very small advertisement just to say, you know, the things that I told you in the


112
00:13:19,360 --> 00:13:28,080
past with this ID, scratch them. Don't have them anymore. You never republish multi-hashes.


113
00:13:28,080 --> 00:13:33,360
You never republish any of these entries. If you want to change addresses from provider's


114
00:13:33,360 --> 00:13:38,960
perspective, again, this is a single advertisement that just changes the address. You literally


115
00:13:38,960 --> 00:13:43,600
publish a new thing, you say, I changed the address, that's it. Address gets changed for


116
00:13:43,600 --> 00:13:48,680
all the advertisements that happened in the past. There is another mechanism, which we


117
00:13:48,680 --> 00:13:56,280
call extended providers, which allows you to have a cluster of nodes that are providing


118
00:13:56,280 --> 00:14:03,200
the same information. This is typical of a large-scale content provider. And have a way


119
00:14:03,200 --> 00:14:07,640
of conveying that through advertisements to say, you know, all the advertisements that


120
00:14:07,640 --> 00:14:13,360
are published in the past, now I have three nodes in front of it that serve the same data.


121
00:14:13,360 --> 00:14:19,800
Again, you do not publish all the multi-hashes again. All you're doing is, please extend


122
00:14:19,800 --> 00:14:27,840
the addresses by which this can be retrieved by these extra nodes. And this is really nice.


123
00:14:27,840 --> 00:14:31,840
So far, I think this is probably as efficient as you can get when it comes to advertising


124
00:14:31,840 --> 00:14:42,320
content. The one thing I wanted to cover here... Yeah, we can get back to it. On the indexer


125
00:14:42,320 --> 00:14:51,120
side, this passive fetching allows the indexers to identify or think about what information


126
00:14:51,120 --> 00:14:56,000
they have before they reach out to the client. Because remember, all of this is content address


127
00:14:56,000 --> 00:15:04,960
data. What we're exchanging is CIDs and DAGs here. CIDs are pointing to immutable data.


128
00:15:04,960 --> 00:15:11,280
Network indexer can see whether a CID has been seen before or not. And therefore, it


129
00:15:11,280 --> 00:15:17,080
does not need to reach out and fetch data every time.


130
00:15:17,080 --> 00:15:25,160
So what does that really mean in practice? It means great things. An advertisement could


131
00:15:25,160 --> 00:15:33,640
be pointing to an entry that has been published weeks before. And there is a way for indexers


132
00:15:33,640 --> 00:15:42,880
to infer that. So you will never have to get the same data twice. That's pretty nice.


133
00:15:42,880 --> 00:15:48,120
On the indexer side, the data is structured in such a way that it is optimized for changes


134
00:15:48,120 --> 00:15:52,560
in the information that we store for each of these records. And by changes, I mean things


135
00:15:52,560 --> 00:15:59,640
like the address changes, the retrieval protocols, and so on that I touched on earlier. And that


136
00:15:59,640 --> 00:16:07,000
is done simply by having a foreign key-like mechanism, if you like, in RDBS relational


137
00:16:07,000 --> 00:16:13,240
database design. So what we have is a multi-hash that points to something that remains constant.


138
00:16:13,240 --> 00:16:18,140
And then that constant thing then points to other things that may change all the time.


139
00:16:18,140 --> 00:16:23,200
And that then allows us to efficiently handle things like change of protocol as well as


140
00:16:23,200 --> 00:16:28,040
change of addresses. The thing that I wanted to talk about that


141
00:16:28,040 --> 00:16:33,080
I couldn't remember earlier, advertisements are chained together and signed at every step


142
00:16:33,080 --> 00:16:43,840
of the way. That allows the whole ecosystem to prove that a peer indeed claimed to provide


143
00:16:43,840 --> 00:16:50,160
a multi-hash because everything is signed here. And this is a small window into a whole


144
00:16:50,160 --> 00:16:56,200
body of work that we haven't even started scratching the surface of, and that is reputation.


145
00:16:56,200 --> 00:17:03,400
Right there, you have a way by which you can prove that you told me you have this information.


146
00:17:03,400 --> 00:17:09,160
Where is it? Because you have an advertise that is removed, right? Imagine, like a scenario,


147
00:17:09,160 --> 00:17:13,840
some note that just publishes stuff, right? And this is the concept of accountability


148
00:17:13,840 --> 00:17:22,400
that is built into the advertisement chain, typical of a blockchain-like data structure.


149
00:17:22,400 --> 00:17:27,560
So the most recent extensions to the IP&I protocol, remember this talk is going to only


150
00:17:27,560 --> 00:17:32,400
talk about the protocol itself. We're going to dive into the implementation in a bit.


151
00:17:32,400 --> 00:17:37,400
Extended provider families. I touched on that a little bit earlier. If you're a large-scale


152
00:17:37,400 --> 00:17:43,480
provider and you're providing information across multiple nodes, typical setup in a


153
00:17:43,480 --> 00:17:51,320
large-scale provider, these nodes are probably elastic. They go small and they go big. More


154
00:17:51,320 --> 00:17:57,160
importantly, the protocols by which the data could be retrieved might change over time.


155
00:17:57,160 --> 00:18:03,280
Today you might be supporting BitSwap. Tomorrow you want to support GraphSync. The day after,


156
00:18:03,280 --> 00:18:08,520
tomorrow, day after tomorrow, insert your favorite protocol here, right? Should you


157
00:18:08,520 --> 00:18:12,720
really have to re-advertise all your content every time you change a protocol? Really not,


158
00:18:12,720 --> 00:18:20,800
right? So that's where IP&I has built-in levers to allow you to really express this. And these


159
00:18:20,800 --> 00:18:26,640
two levers are kind of combined together, both in terms of the node addresses as well


160
00:18:26,640 --> 00:18:32,600
as the protocols over which they can be retrieved. A really good example of this being used today


161
00:18:32,600 --> 00:18:42,520
is Boost. Boost is a new Filecoin market implementation which offers extensions in terms of retrieval


162
00:18:42,520 --> 00:18:49,920
of data as well as much, much simpler interface to interact with the data. Boost today supports


163
00:18:49,920 --> 00:18:56,800
BitSwap, supports HTTP, I think is in the making, almost there. From day one it didn't


164
00:18:56,800 --> 00:19:04,480
support BitSwap. It didn't support HTTP. And there are nodes out there in Filecoin network


165
00:19:04,480 --> 00:19:13,120
that are publishing, making deals all the time. So when the BitSwap and HTTP was added,


166
00:19:13,120 --> 00:19:18,080
for them to tell the world that, hey, all the millions of multi-ashes that I've published


167
00:19:18,080 --> 00:19:25,000
over the last few months now is retrievable over BitSwap, it was a single advertisement


168
00:19:25,000 --> 00:19:31,080
that was added to the chain and that is it. That is pretty nice.


169
00:19:31,080 --> 00:19:36,760
Reader privacy. We're going to cover that in depth by the talk by Ivan later on today,


170
00:19:36,760 --> 00:19:42,560
and we're going to cover what it means for the DHT itself. Reader privacy is something


171
00:19:42,560 --> 00:19:49,600
that is cross-cutting across multiple content routing systems, and it affects the way by


172
00:19:49,600 --> 00:19:54,700
which we exchange information fundamentally, but not necessarily the lookup algorithm itself.


173
00:19:54,700 --> 00:19:59,840
So it's quite a neat improvement, which is user-centric, and I think it's very welcome.


174
00:20:00,000 --> 00:20:30,000
we're going to cover that deeply today. Advertisement mirroring is another new extension in the IPNI. It is yet to be formalized in the specifications. I'll point you to the specifications later today, but the main idea there is that for other nodes to provide the same information, if we go back to the previous slide, for other nodes to provide the same information, indexer nodes, they need to have the ground truth. They need to have the


175
00:20:30,000 --> 00:21:00,000
advertisement chain. They need to have the entries, right? And it would be a real shame if every time they want that information, they have to go back to the original provider, because that means that every time IPNI network expands, providers get the punishment of, oh, I have to reserve this again, right? Also, it would be a real shame if we are talking about this content address network without CDNs, because data is immutable, right? This is highly


176
00:21:00,000 --> 00:21:29,940
cacheable information, right? So the idea of advertisement mirroring is to provide caches of this chain of advertisements across the network such that it enables, first of all, fast catch-up of new indexer nodes to the information, and second of all, lays the foundation for a world in which you can verify and assert that a node actually provides the information they claim, right? Because remember, at the end of


177
00:21:29,940 --> 00:21:41,100
the day, what we really, really, really want is for people to find their data and for them to get it, right? Really get what they asked for, okay?


178
00:21:41,100 --> 00:21:59,640
We have streaming lookup. So this is a work that started in the HTTP delegated routing by the stewards team. Very welcome. Basically, all the IPNI APIs now support NDJSON streaming responses back.


179
00:21:59,640 --> 00:22:11,400
The way that it works is extremely simple. Rather than having a giant body of JSON which has an array in it that has many, many records, you just write the records with new lines separated as soon as you find them.


180
00:22:11,400 --> 00:22:31,400
And this is quite powerful, yet simple. I'll go over the benefits of that in the SID.contact specifically. But the long story here is that it makes the chaining of this routing systems together much, much easier because you don't have to wait for...


181
00:22:31,400 --> 00:22:42,900
Because at the start of the chain, you can trickle the information up as you find them rather than having to wait for the aggregator latency of all these chain systems to get the results back.


182
00:22:42,900 --> 00:22:56,400
So it is pretty simple, a pretty fundamental advancement in terms of making this lookup faster while making it swappable. Yeah, so I'll cover the SID.contact stuff.


183
00:22:56,400 --> 00:23:11,400
The lookup that just works. So this is another advancement in the IPNI protocol. Originally, IPNI was only focused on the information that is explicitly published the way that the IPNI speaks.


184
00:23:11,400 --> 00:23:26,400
This meant that if there was an alternative routing system like BitSwap or the DHT, you need to go and run your own router, right? You go and run your own BitSwap client and find the information.


185
00:23:26,400 --> 00:23:45,400
Now we are moving towards a world... We have already delivered this extensions to the IPNI lookup protocol that allows you to specify cascading of lookup. So in the APIs, you can specify to which alternative routing system the lookup should be cascaded.


186
00:23:45,400 --> 00:23:58,400
And then the aggregated results are returned back to the client. There is this concept of cascade label, which right now supports two variations, IPFS, DHT, and what we call legacy.


187
00:23:58,400 --> 00:24:17,400
This is the mechanism by which LASI, for example, delegates the problem of content discovery and content routing to an external system and keeps its system boundaries crisp and firm to do one thing very, very well, and that is retrieval.


188
00:24:17,400 --> 00:24:30,400
So this is, I think, an interesting advancement because, again, we are keeping ourselves true to this promise of give me the set, I'll find it for you.


189
00:24:30,400 --> 00:24:47,400
Diving a little bit into cascading lookup, cascading lookup will always return the IPNI information, so it is just a complementary lookup over multiple routing systems. Like I mentioned, two ones are supported today.


190
00:24:47,400 --> 00:25:00,400
There's a label that you specify, so literally there is a query parameter called cascade. You add it to the end of the get request for the CID, and you insert IPFS-DHT.


191
00:25:00,400 --> 00:25:07,400
If you want the legacy, you add another one. It calls legacy, and that's all you have to do.


192
00:25:07,400 --> 00:25:18,400
We have built in a mechanism by which users can discover these labels as they appear because you really shouldn't have to watch the specification or read the specification every time you need to read the system.


193
00:25:18,400 --> 00:25:23,400
We also want to enable programmatic ways by which you can discover these cascading mechanisms.


194
00:25:23,400 --> 00:25:39,400
If you send a HTTP options request to an IPNI node, in the headers, you get back two extra headers, which is IPNI allow cascade, which contains a comma-separated list of those labels that you could specify.


195
00:25:39,400 --> 00:25:41,400
It's quite neat.


196
00:25:41,400 --> 00:25:49,400
In the case of legacy, it will tell you where you need to appear in order to make your content discoverable.


197
00:25:49,400 --> 00:25:53,400
Let me talk a little bit about this legacy thing.


198
00:25:53,400 --> 00:25:59,400
The context behind this was to enable Project Rhea, just make the Project Rhea work.


199
00:25:59,400 --> 00:26:04,400
We just want to delegate content discovery, so that's where all this came from.


200
00:26:04,400 --> 00:26:16,400
I have a whole bunch of numbers to present to you in the CID.contact presentation, but the main thing about the word legacy here is that right now it basically includes bits of gossip.


201
00:26:16,400 --> 00:26:28,400
We are very specific about how we are making the content that is only discoverable over bits of available, and that is by explicit peering.


202
00:26:28,400 --> 00:26:44,400
If you're a user, if you're a node out there that has this content only discoverable over bits of, for whatever reason, you could explicitly peer to an address in IPNI to then make that content discoverable.


203
00:26:44,400 --> 00:26:57,400
If you do not, then we won't be going out looking for it, because remember, IPNI is in the ballgame of mass discovery, large scale, buy the bucket sort of thing,


204
00:26:57,400 --> 00:27:09,400
and the bits of gossip is not something that scales in that type of environment, because it's sort of inefficient by design.


205
00:27:09,400 --> 00:27:20,400
Streaming responses. All you need to add to your responses is this accept header application xndjson. Just works. Please give it a try today, CID.contact.


206
00:27:20,400 --> 00:27:31,400
If you haven't switched to streaming responses, please do. There's numbers to specifically do some more in the CID.contact presentation.


207
00:27:31,400 --> 00:27:44,400
What's on the making in the IPNI roadmap? We plan to make the lookups private by default. Right now we store the information on encrypted on the back end.


208
00:27:44,400 --> 00:27:56,400
From observability perspective, an IPNI instance, for example, could collect analytics data on how popular CID is or what a specific user is looking at.


209
00:27:56,400 --> 00:28:01,400
That's not great. We really don't want to enable another Google Analytics when we are making Web3 World.


210
00:28:01,400 --> 00:28:09,400
What we want to do instead is we want to roll out reader privacy, the stuff that Ivan is going to cover.


211
00:28:09,400 --> 00:28:18,400
Then what we're going to do is, even for the unencrypted advertisements out there, we're going to do the encryption on client's behalf,


212
00:28:18,400 --> 00:28:26,400
and the value store on the back end will only contain encrypted information.


213
00:28:26,400 --> 00:28:33,400
We are making the lookup encrypted by default. As part of that, we're going to optimize the read path.


214
00:28:33,400 --> 00:28:39,400
So far we've done a lot of work in optimizing the write path, i.e. publication of information.


215
00:28:39,400 --> 00:28:48,400
We are going to make the read path even faster by the way of leveraging the double-hashed information,


216
00:28:48,400 --> 00:28:53,400
because double-hashed information opens the opportunity by which you get the data effectively pre-processed.


217
00:28:53,400 --> 00:28:56,400
I'll get into that a little bit more later.


218
00:28:56,400 --> 00:29:08,400
We want to enable more adoption of IPNI. What we are looking at is what does it look like if we want to enable that long tail of content providers to also publish to IPNI.


219
00:29:08,400 --> 00:29:13,400
That long tail is, remember, my laptop on the train going to work and back.


220
00:29:13,400 --> 00:29:17,400
How can we reduce barriers of adoption there?


221
00:29:17,400 --> 00:29:23,400
A whole body of work around federation, which has already started with the advertisement mirroring,


222
00:29:23,400 --> 00:29:27,400
but we're going to formalize that a little bit more.


223
00:29:27,400 --> 00:29:39,400
We're going to have protocols that reason about how consistency across multiple IPNI instances can be guaranteed as long as they follow the same protocol.


224
00:29:39,400 --> 00:29:48,400
This also includes things like caching, for example, as a natural way of reducing the need of trust for a single entry.


225
00:29:48,400 --> 00:29:51,400
So exciting stuff coming up.


226
00:29:51,400 --> 00:29:54,400
Who is working on all this awesome stuff?


227
00:29:54,400 --> 00:29:57,400
We have a team of five.


228
00:29:57,400 --> 00:30:03,400
Four people in this team are in IPFS thing today.


229
00:30:03,400 --> 00:30:05,400
Please reach out to us.


230
00:30:05,400 --> 00:30:09,400
Andrew, unfortunately, couldn't be with us today.


231
00:30:09,400 --> 00:30:12,400
He's based on West Coast, US.


232
00:30:12,400 --> 00:30:15,400
Ivan, sitting over there, is going to give us a talk.


233
00:30:15,400 --> 00:30:21,400
Yours truly, Torfin, who is the product manager in IPNI team.


234
00:30:21,400 --> 00:30:28,400
If you're interested or curious about running your own IPNI instance, please come talk to us.


235
00:30:28,400 --> 00:30:30,400
Torfin would love to give you a walkthrough.


236
00:30:30,400 --> 00:30:42,400
And we've got Will sitting right at the back that is basically shepherding all this work through integration with other systems across the interplanetary network.


237
00:30:42,400 --> 00:30:47,400
We have a brand new GitHub organization, IPNI.


238
00:30:47,400 --> 00:30:49,400
This is all things IPNI.


239
00:30:49,400 --> 00:30:51,400
Please go and visit it.


240
00:30:51,400 --> 00:30:56,400
We will have a shiny website coming up soon, so look out for that.


241
00:30:56,400 --> 00:31:01,400
If there's one repo you check out in the IPNI organization, it's specs.


242
00:31:01,400 --> 00:31:04,400
We already have three extensions merged.


243
00:31:04,400 --> 00:31:06,400
We've got three waiting.


244
00:31:06,400 --> 00:31:10,400
There is an open API, HTTP specification for all the APIs that I talked about.


245
00:31:10,400 --> 00:31:13,400
You can generate your own client and whatnot.


246
00:31:13,400 --> 00:31:19,400
If you're looking for changes on the IPNI protocol, that is the repo to watch.


247
00:31:19,400 --> 00:31:33,400
And I just wanted to point out the really nice work done by Andrew recently, which consolidated all these little improvements and functionalities into a single reusable Go library called GoLibIPNI.


248
00:31:33,400 --> 00:31:38,400
That allows you to kind of slice and dice functionalities and sort of build your own client the way you like it.


249
00:31:38,400 --> 00:31:40,400
We would love contributions there.


250
00:31:40,400 --> 00:31:45,400
We would love to understand how this looks from the developer experience perspective.


251
00:31:45,400 --> 00:31:47,400
Give it a try.


252
00:31:47,400 --> 00:31:49,400
Comments are more than welcome.


253
00:31:49,400 --> 00:31:50,400
Capture an issue.


254
00:31:50,400 --> 00:31:53,400
We are very quick in responding.


255
00:31:53,400 --> 00:31:59,400
There is a CLI to check out that allows you to verify interaction in IPNI network.


256
00:31:59,400 --> 00:32:02,400
For example, have I advertised the right thing as a provider?


257
00:32:02,400 --> 00:32:07,400
Has an indexer network seen the information that I've advertised?


258
00:32:07,400 --> 00:32:16,400
This CLI allows you to verify these things and actually check that you are an IPNI provider or consumable by an indexer.


259
00:32:16,400 --> 00:32:18,400
It is in that repo.


260
00:32:18,400 --> 00:32:19,400
Please check it out.


261
00:32:19,400 --> 00:32:23,400
Two main commands to remember, verifying just and list at.


262
00:32:23,400 --> 00:32:31,400
These are the commands that are heavily used inside Bedrock, if not the PL network, just for testing our own stuff.


263
00:32:31,400 --> 00:32:38,400
These are the tools we have built to make sure that systems work, such as, for example, Boost and other providers.


264
00:32:38,400 --> 00:32:41,400
There is a new CLI coming up, which looks much, much shinier.


265
00:32:41,400 --> 00:32:44,400
Watch out for that.


266
00:32:44,400 --> 00:32:48,400
We have four blog posts that talk about what is IPNI.


267
00:32:48,400 --> 00:32:53,400
That QR code takes you to the latest published by Ivan that walks you through extended providers.


268
00:32:53,400 --> 00:32:54,400
But there is more.


269
00:32:54,400 --> 00:32:56,400
Please check them out.


270
00:32:56,400 --> 00:32:59,400
And finally, Filecoin, Slack, IPNI.


271
00:32:59,400 --> 00:33:03,400
That's where you find all the community interaction.


272
00:33:03,400 --> 00:33:08,400
Please reach out to us if there is anything that we can help you with.


273
00:33:08,400 --> 00:33:14,400
I'm going to pause for questions and then walk you through SID.contact after that.


274
00:33:14,400 --> 00:33:15,400
How do you guys find that?


275
00:33:15,400 --> 00:33:17,400
Any questions I can answer?


276
00:33:17,400 --> 00:33:19,400
Great work, Massih.


277
00:33:19,400 --> 00:33:20,400
Just one question.


278
00:33:20,400 --> 00:33:30,400
In theory, if Kubo nodes start publishing these advertisements, there is nothing stopping from all that IPFS content getting indexed directly on IPNI, right?


279
00:33:30,400 --> 00:33:39,400
Could you integrate an IPNI client directly into a Kubo node and get them to start publishing advertising instead of using the DHT, for example?


280
00:33:39,400 --> 00:33:40,400
Absolutely.


281
00:33:40,400 --> 00:33:41,400
This exists today.


282
00:33:41,400 --> 00:33:46,400
It is actually integrated into Colab IPFS clusters.


283
00:33:46,400 --> 00:33:51,400
The data from Colab IPFS clusters is discoverable on SID.contact right now.


284
00:33:51,400 --> 00:33:57,400
And the way that that works is by integrating the index provider library that I pointed to earlier.


285
00:33:57,400 --> 00:34:08,400
The main thing to point out there is that the barrier for adoption right now means that you have to have a publicly accessible endpoint for advertisements to get fetched.


286
00:34:08,400 --> 00:34:11,400
And that is okay for long-running IPFS clusters.


287
00:34:11,400 --> 00:34:19,400
But if I'm on a train use case, running a publicly accessible thing is not really desirable.


288
00:34:19,400 --> 00:34:23,400
So what we want to do is make that a little bit better.


289
00:34:23,400 --> 00:34:28,400
Just talk about what we can do there to enable this long tail of publication.


290
00:34:28,400 --> 00:34:30,400
Sorry, just which instance do the Kubo nodes use?


291
00:34:30,400 --> 00:34:34,400
So they get to configure which IPNI instance to talk to?


292
00:34:34,400 --> 00:34:35,400
Right.


293
00:34:35,400 --> 00:34:39,400
So there are two ways by which you can announce the IPNI network.


294
00:34:39,400 --> 00:34:43,400
One is not directly configured.


295
00:34:43,400 --> 00:34:45,400
It works through GossipSub.


296
00:34:45,400 --> 00:34:48,400
So it gets propagated over the network.


297
00:34:48,400 --> 00:34:52,400
The other one is explicit HTTP announcements, which goes to SID.contact.


298
00:34:52,400 --> 00:34:56,400
I believe Colab clusters are using GossipSub.


299
00:34:56,400 --> 00:34:57,400
Are they?


300
00:34:57,400 --> 00:34:58,400
Or are they using direct connection?


301
00:34:58,400 --> 00:35:01,400
But anyway, so you have both options, right?


302
00:35:01,400 --> 00:35:03,400
So for example, Filecoin uses the GossipSub thing.


303
00:35:03,400 --> 00:35:10,400
And NFC.storage uses explicit HTTP, which you need an endpoint.


304
00:35:10,400 --> 00:35:11,400
Thanks. This is great.


305
00:35:11,400 --> 00:35:18,400
And this might be a question that sort of contextually should be in the CID.contact presentation.


306
00:35:18,400 --> 00:35:27,400
But what does it take for someone like the average user of Kubo to advertise onto this network right now?


307
00:35:27,400 --> 00:35:28,400
Does it happen by default?


308
00:35:28,400 --> 00:35:36,400
Are we actually filling the IPNI system with the existing CIDs out there?


309
00:35:36,400 --> 00:35:40,400
Or is there something that the average user has to flip a switch?


310
00:35:40,400 --> 00:35:48,400
So the lookup side right now, the reading content, is built into Kubo.


311
00:35:48,400 --> 00:35:50,400
You don't have to do anything about that.


312
00:35:50,400 --> 00:35:51,400
I think it's enabled by default.


313
00:35:51,400 --> 00:35:55,400
But publication, providing content, is not.


314
00:35:55,400 --> 00:36:06,400
What we do have, however, is extensions to the integration layers in Kubo, which is HTTP-delegated routing,


315
00:36:06,400 --> 00:36:09,400
for you to then use that interface to publish information.


316
00:36:09,400 --> 00:36:15,400
And that is the very same interface that is used by IPFS callout clusters to provide information.


317
00:36:15,400 --> 00:36:21,400
In that case, it would become a hands-on keyboard integration of a library.


318
00:36:21,400 --> 00:36:23,400
So you don't have to implement everything from scratch.


319
00:36:23,400 --> 00:36:26,400
It's just a bit of glue-making in terms of connecting things.


320
00:36:26,400 --> 00:36:37,400
But eventually, what we would like to look at is just reason about what the world would look like if providing was also made default behavior


321
00:36:37,400 --> 00:36:40,400
that was configurable without the need for implementation.


322
00:36:40,400 --> 00:36:47,400
But early days there, there is a need for some protocol changes just to reduce barriers for adoption, like I mentioned.


323
00:36:47,400 --> 00:36:50,400
But we are certainly thinking about it.


324
00:36:50,400 --> 00:37:00,400
One thing I hear from providers a lot is that they want to only provide the data that their customers are giving them, basically.


325
00:37:00,400 --> 00:37:05,400
Is that a use case that IP&I is considering?


326
00:37:05,400 --> 00:37:10,400
What sort of filtering are they exactly looking for? Would you mind repeating that for me?


327
00:37:10,400 --> 00:37:18,400
Imagine you're a provider and you have customers giving you data to provide.


328
00:37:18,400 --> 00:37:23,400
You only want to spend money providing that data and not any other data, right?


329
00:37:23,400 --> 00:37:27,400
Absolutely. Filtering is an interesting topic.


330
00:37:27,400 --> 00:37:32,400
This is something that came up in the Boost discussion as well.


331
00:37:32,400 --> 00:37:41,400
When the Boost started, it was all or nothing. By default, it always provides everything.


332
00:37:41,400 --> 00:37:45,400
And then there was a callout from community, like, I need to be able to configure this.


333
00:37:45,400 --> 00:37:52,400
So right now there's a specific flag. You can specify whether a deal information could be published to IP&I or not.


334
00:37:52,400 --> 00:37:57,400
Absolutely. It already exists, but it's on the Filecoin site.


335
00:37:57,400 --> 00:38:16,400
If this is a concern that's big enough, totally, we can certainly introduce it in the IPFS world as well.
